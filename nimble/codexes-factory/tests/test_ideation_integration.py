"""
Comprehensive tests for the ideation system integration.
"""

import pytest
import json
import tempfile
from pathlib import Path
from datetime import datetime
from unittest.mock import Mock, patch, MagicMock

# Import ideation modules
from src.codexes.modules.ideation import (
    BookIdea, IdeaSet, Tournament, TournamentManager,
    ContinuousIdeaGenerator, IntegratedIdeaGenerator,
    SyntheticReaderPanel, ReaderFeedback, SynthesizedInsights,
    IdeationPipelineBridge, FeedbackDrivenOptimizer
)
from src.codexes.modules.ideation.advanced_tournament import (
    AdvancedTournament, AdvancedTournamentManager, TournamentFormat,
    AdvancedTournamentConfig, JudgingCriteria
)
from src.codexes.modules.ideation.monitoring import (
    IdeationMonitoringSystem, AlertLevel, SystemMetrics
)


class TestBookIdea:
    """Test BookIdea class functionality."""
    
    def test_book_idea_creation(self):
        """Test basic BookIdea creation."""
        idea = BookIdea(
            title="Test Book",
            logline="A test book for testing purposes"
        )
        
        assert idea.title == "Test Book"
        assert idea.logline == "A test book for testing purposes"
        assert idea.status == "generated"
        assert isinstance(idea.created_at, datetime)

    def test_book_idea_validation(self):
        """Test BookIdea validation."""
        # Should work with title and logline
        idea = BookIdea(title="Test", logline="Test logline")
        assert idea.is_specific_enough_for_book()
        
        # Should fail with insufficient detail
        minimal_idea = BookIdea(title="Hi", logline="Short")
        assert not minimal_idea.is_specific_enough_for_book()

    def test_book_idea_serialization(self):
        """Test BookIdea to/from dict conversion."""
        original = BookIdea(
            title="Test Book",
            logline="Test logline",
            genre="Fiction",
            target_audience="Adults"
        )
        
        # Convert to dict and back
        idea_dict = original.to_dict()
        restored = BookIdea.from_dict(idea_dict)
        
        assert restored.title == original.title
        assert restored.logline == original.logline
        assert restored.genre == original.genre
        assert restored.target_audience == original.target_audience

    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_llm_idea_generation(self, mock_llm_caller):
        """Test LLM-based idea generation."""
        # Mock LLM response
        mock_response = {
            'content': '''Title: Generated Book
Logline: A book generated by AI
Description: This is a test description
Genre: Fiction
Target Audience: General'''
        }
        mock_llm_caller.call_llm.return_value = mock_response
        
        idea = BookIdea()
        idea.create_idea_with_llm(mock_llm_caller)
        
        assert idea.title == "Generated Book"
        assert idea.logline == "A book generated by AI"
        assert idea.generation_metadata['llm_generated'] is True


class TestIdeaSet:
    """Test IdeaSet collection functionality."""
    
    def test_idea_set_operations(self):
        """Test basic IdeaSet operations."""
        idea_set = IdeaSet()
        
        idea1 = BookIdea(title="Book 1", logline="First book")
        idea2 = BookIdea(title="Book 2", logline="Second book", genre="Fiction")
        
        idea_set.add_idea(idea1)
        idea_set.add_idea(idea2)
        
        assert len(idea_set) == 2
        assert idea1 in idea_set.get_ideas()
        assert idea2 in idea_set.get_ideas()
        
        # Test filtering
        fiction_ideas = idea_set.get_ideas_by_genre("Fiction")
        assert len(fiction_ideas) == 1
        assert fiction_ideas[0] == idea2

    def test_idea_set_persistence(self):
        """Test IdeaSet save/load functionality."""
        with tempfile.TemporaryDirectory() as temp_dir:
            idea_set = IdeaSet()
            idea_set.add_idea(BookIdea(title="Test", logline="Test logline"))
            
            file_path = Path(temp_dir) / "test_ideas.json"
            idea_set.save_to_json(str(file_path))
            
            # Load and verify
            new_idea_set = IdeaSet()
            new_idea_set.load_from_json(str(file_path))
            
            assert len(new_idea_set) == 1
            assert new_idea_set.get_ideas()[0].title == "Test"


class TestTournament:
    """Test Tournament functionality."""
    
    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_tournament_creation(self, mock_llm_caller):
        """Test tournament creation and setup."""
        ideas = [
            BookIdea(title="Book 1", logline="First book"),
            BookIdea(title="Book 2", logline="Second book"),
            BookIdea(title="Book 3", logline="Third book"),
            BookIdea(title="Book 4", logline="Fourth book")
        ]
        
        tournament = Tournament(ideas, mock_llm_caller)
        
        assert tournament.total_ideas == 4
        assert len(tournament.seeded_ideas) == 4
        assert all(hasattr(idea, 'seed') for idea in tournament.seeded_ideas)

    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_tournament_execution(self, mock_llm_caller):
        """Test tournament bracket creation and execution."""
        # Mock LLM responses
        mock_llm_caller.call_llm.return_value = {'content': 'A'}
        
        ideas = [
            BookIdea(title="Book 1", logline="First book"),
            BookIdea(title="Book 2", logline="Second book")
        ]
        
        tournament = Tournament(ideas, mock_llm_caller)
        tournament.create_brackets()
        
        assert len(tournament.rounds) > 0
        assert tournament.get_winner() is not None

    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_tournament_with_byes(self, mock_llm_caller):
        """Test tournament with odd number of participants."""
        mock_llm_caller.call_llm.return_value = {'content': 'A'}
        
        ideas = [
            BookIdea(title="Book 1", logline="First book"),
            BookIdea(title="Book 2", logline="Second book"),
            BookIdea(title="Book 3", logline="Third book")
        ]
        
        tournament = Tournament(ideas, mock_llm_caller)
        tournament.create_brackets()
        
        # Should have added a BYE to make it 4 participants
        assert tournament.total_teams == 4
        assert any(idea.title == "BYE" for idea in tournament.seeded_ideas)


class TestAdvancedTournament:
    """Test advanced tournament features."""
    
    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_advanced_tournament_config(self, mock_llm_caller):
        """Test advanced tournament configuration."""
        criteria = [
            JudgingCriteria(name="Market Appeal", weight=1.0),
            JudgingCriteria(name="Originality", weight=0.8)
        ]
        
        config = AdvancedTournamentConfig(
            format=TournamentFormat.ROUND_ROBIN,
            judging_criteria=criteria,
            seeding_strategy="rating_based"
        )
        
        ideas = [
            BookIdea(title="Book 1", logline="First book"),
            BookIdea(title="Book 2", logline="Second book")
        ]
        
        tournament = AdvancedTournament(ideas, mock_llm_caller, config)
        
        assert tournament.format == TournamentFormat.ROUND_ROBIN
        assert len(tournament.judging_criteria) == 2
        assert tournament.seeding_strategy == "rating_based"

    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_round_robin_tournament(self, mock_llm_caller):
        """Test round robin tournament format."""
        mock_llm_caller.call_llm.return_value = {'content': 'Idea A: 7\nIdea B: 5'}
        
        config = AdvancedTournamentConfig(format=TournamentFormat.ROUND_ROBIN)
        
        ideas = [
            BookIdea(title="Book 1", logline="First book"),
            BookIdea(title="Book 2", logline="Second book"),
            BookIdea(title="Book 3", logline="Third book")
        ]
        
        tournament = AdvancedTournament(ideas, mock_llm_caller, config)
        tournament.create_brackets()
        
        # Round robin should have 3 rounds for 3 participants
        assert len(tournament.rounds) == 3
        assert hasattr(tournament, 'standings')


class TestSyntheticReaderPanel:
    """Test synthetic reader feedback system."""
    
    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_reader_feedback_generation(self, mock_llm_caller):
        """Test synthetic reader feedback generation."""
        mock_response = {
            'content': json.dumps({
                'market_appeal_score': 7.5,
                'genre_fit_score': 8.0,
                'audience_alignment_score': 6.5,
                'detailed_feedback': 'This is a compelling idea with strong market potential.',
                'recommendations': ['Develop the characters more', 'Add more conflict'],
                'concerns': ['Plot may be too predictable']
            })
        }
        mock_llm_caller.call_llm.return_value = mock_response
        
        panel = SyntheticReaderPanel(mock_llm_caller)
        ideas = [BookIdea(title="Test Book", logline="A test book")]
        
        feedback = panel.evaluate_ideas(ideas)
        
        assert len(feedback) > 0
        assert isinstance(feedback[0], ReaderFeedback)
        assert feedback[0].market_appeal_score == 7.5
        assert feedback[0].overall_rating > 0

    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_feedback_synthesis(self, mock_llm_caller):
        """Test feedback synthesis into insights."""
        panel = SyntheticReaderPanel(mock_llm_caller)
        
        # Create mock feedback
        feedback_list = [
            ReaderFeedback(
                reader_persona="Test Reader 1",
                idea_id="test_idea",
                market_appeal_score=7.0,
                genre_fit_score=8.0,
                audience_alignment_score=6.0,
                detailed_feedback="Good idea",
                recommendations=["Improve pacing"],
                concerns=["Weak ending"]
            ),
            ReaderFeedback(
                reader_persona="Test Reader 2",
                idea_id="test_idea",
                market_appeal_score=8.0,
                genre_fit_score=7.5,
                audience_alignment_score=7.0,
                detailed_feedback="Strong concept",
                recommendations=["Add more detail"],
                concerns=["Character development"]
            )
        ]
        
        insights = panel.synthesize_feedback(feedback_list)
        
        assert "test_idea" in insights
        insight = insights["test_idea"]
        assert isinstance(insight, SynthesizedInsights)
        assert insight.market_potential > 0
        assert insight.reader_count == 2


class TestContinuousIdeaGenerator:
    """Test continuous idea generation system."""
    
    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_single_batch_generation(self, mock_llm_caller):
        """Test single batch idea generation."""
        mock_response = {
            'content': json.dumps({
                'title': 'Generated Book',
                'logline': 'A book generated for testing'
            })
        }
        mock_llm_caller.call_llm.return_value = mock_response
        
        with tempfile.TemporaryDirectory() as temp_dir:
            generator = ContinuousIdeaGenerator(
                llm_caller=mock_llm_caller,
                ideas_per_batch=2,
                base_dir=temp_dir
            )
            
            results = generator.generate_single_batch()
            
            assert results is not None
            assert len(results) <= 2  # May be less due to generation failures
            
            # Check that files were created
            resources_dir = Path(temp_dir) / "resources"
            assert resources_dir.exists()

    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_integrated_idea_generator(self, mock_llm_caller):
        """Test integrated idea generator with imprint customization."""
        generator = IntegratedIdeaGenerator(mock_llm_caller)
        
        # Test imprint-specific generator creation
        imprint_generator = generator.create_imprint_generator("test_imprint")
        
        assert isinstance(imprint_generator, ContinuousIdeaGenerator)
        assert "test_imprint" in generator.active_generators
        
        # Test status retrieval
        status = generator.get_generator_status("test_imprint")
        assert status['status'] in ['running', 'stopped', 'not_running']


class TestPipelineBridge:
    """Test pipeline integration bridge."""
    
    def test_metadata_conversion(self):
        """Test BookIdea to CodexMetadata conversion."""
        bridge = IdeationPipelineBridge()
        
        idea = BookIdea(
            title="Test Book",
            logline="A test book for conversion",
            genre="Fiction",
            target_audience="Adults"
        )
        
        metadata = bridge.convert_idea_to_metadata(idea, "test_imprint")
        
        assert metadata.title == "Test Book"
        assert metadata.summary_short == "A test book for conversion"
        assert hasattr(metadata, 'idea_metadata')
        assert metadata.idea_metadata['converted_at'] is not None

    def test_schedule_integration(self):
        """Test schedule integration functionality."""
        with tempfile.TemporaryDirectory() as temp_dir:
            bridge = IdeationPipelineBridge()
            
            idea = BookIdea(title="Test Book", logline="Test logline")
            metadata = bridge.convert_idea_to_metadata(idea, "test_imprint")
            
            schedule_file = Path(temp_dir) / "test_schedule.csv"
            bridge.add_to_schedule(metadata, "test_imprint", schedule_file=str(schedule_file))
            
            assert schedule_file.exists()
            
            # Verify schedule content
            import pandas as pd
            df = pd.read_csv(schedule_file)
            assert len(df) == 1
            assert df.iloc[0]['title'] == "Test Book"


class TestMonitoringSystem:
    """Test monitoring and alerting system."""
    
    def test_monitoring_system_initialization(self):
        """Test monitoring system setup."""
        monitoring = IdeationMonitoringSystem()
        
        assert monitoring.metrics_collector is not None
        assert monitoring.alert_manager is not None
        assert monitoring.health_checker is not None
        assert monitoring.dashboard is not None

    def test_alert_creation(self):
        """Test alert creation and management."""
        monitoring = IdeationMonitoringSystem()
        
        alert = monitoring.create_alert(
            AlertLevel.WARNING,
            "test_component",
            "Test alert message"
        )
        
        assert alert.level == AlertLevel.WARNING
        assert alert.component == "test_component"
        assert alert.message == "Test alert message"
        assert not alert.resolved

    def test_system_status(self):
        """Test system status reporting."""
        monitoring = IdeationMonitoringSystem()
        
        status = monitoring.get_system_status()
        
        assert 'timestamp' in status
        assert 'system_health' in status
        assert 'current_metrics' in status
        assert status['system_health'] in ['healthy', 'warning', 'degraded', 'critical']


class TestFeedbackOptimizer:
    """Test feedback-driven optimization system."""
    
    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_feedback_analysis(self, mock_llm_caller):
        """Test feedback pattern analysis."""
        optimizer = FeedbackDrivenOptimizer(mock_llm_caller)
        
        # Create mock feedback data
        feedback_data = {
            'all_feedback': [
                ReaderFeedback(
                    reader_persona="Test Reader",
                    idea_id="test_idea",
                    market_appeal_score=7.0,
                    genre_fit_score=8.0,
                    audience_alignment_score=6.0,
                    detailed_feedback="Good concept",
                    recommendations=["Improve pacing"],
                    concerns=["Weak ending"]
                )
            ],
            'current_prompts': {
                'idea_generation': 'Generate a book idea...'
            },
            'synthesized_insights': {},
            'feedback_by_imprint': {}
        }
        
        results = optimizer.run_comprehensive_optimization(feedback_data)
        
        assert 'timestamp' in results
        assert 'feedback_analysis' in results
        assert 'success_metrics' in results


@pytest.fixture
def mock_llm_caller():
    """Fixture providing a mock LLM caller."""
    mock = Mock()
    mock.call_llm.return_value = {
        'content': json.dumps({
            'title': 'Test Book',
            'logline': 'A test book for testing'
        })
    }
    return mock


@pytest.fixture
def sample_ideas():
    """Fixture providing sample BookIdea objects."""
    return [
        BookIdea(title="Book 1", logline="First test book"),
        BookIdea(title="Book 2", logline="Second test book"),
        BookIdea(title="Book 3", logline="Third test book"),
        BookIdea(title="Book 4", logline="Fourth test book")
    ]


class TestIntegrationWorkflow:
    """Test complete integration workflows."""
    
    @patch('src.codexes.core.llm_caller.LLMCaller')
    def test_complete_ideation_workflow(self, mock_llm_caller, sample_ideas):
        """Test complete workflow from generation to pipeline integration."""
        # Mock LLM responses
        mock_llm_caller.call_llm.return_value = {'content': 'A'}
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # 1. Generate ideas
            generator = ContinuousIdeaGenerator(
                llm_caller=mock_llm_caller,
                ideas_per_batch=2,
                base_dir=temp_dir
            )
            
            # 2. Run tournament
            tournament_manager = TournamentManager(mock_llm_caller)
            tournament = tournament_manager.create_tournament(sample_ideas)
            results = tournament_manager.run_tournament(tournament)
            
            assert results['winner'] is not None
            
            # 3. Get synthetic reader feedback
            reader_panel = SyntheticReaderPanel(mock_llm_caller)
            mock_llm_caller.call_llm.return_value = {
                'content': json.dumps({
                    'market_appeal_score': 8.0,
                    'genre_fit_score': 7.5,
                    'audience_alignment_score': 7.0,
                    'detailed_feedback': 'Excellent concept',
                    'recommendations': ['Polish the ending'],
                    'concerns': []
                })
            }
            
            feedback = reader_panel.evaluate_ideas([results['winner']])
            assert len(feedback) > 0
            
            # 4. Integrate with pipeline
            bridge = IdeationPipelineBridge()
            metadata = bridge.convert_idea_to_metadata(results['winner'], "test_imprint")
            
            assert metadata.title == results['winner'].title
            assert hasattr(metadata, 'idea_metadata')

    def test_monitoring_integration(self):
        """Test monitoring system integration."""
        monitoring = IdeationMonitoringSystem()
        
        # Test health checks
        health_results = monitoring.health_checker.run_all_health_checks()
        assert isinstance(health_results, dict)
        
        # Test metrics collection
        assert monitoring.metrics_collector is not None
        
        # Test alert system
        alert = monitoring.create_alert(
            AlertLevel.INFO,
            "integration_test",
            "Test integration alert"
        )
        
        assert alert.id is not None
        
        # Test system status
        status = monitoring.get_system_status()
        assert 'system_health' in status


if __name__ == "__main__":
    pytest.main([__file__, "-v"])