{
  "metadata": {
    "generated_at": "2025-08-07T01:28:57.981092",
    "total_specs": 24,
    "source_directory": "/Users/fred/my-organizations/nimble/repos/codexes-factory/.kiro/specs",
    "processing_order": "ascending_modification_time",
    "summary": {
      "total_tasks": 22,
      "total_design_items": 25,
      "total_requirements": 23,
      "directories_processed": [
        "lsi-field-enhancement-phase2",
        "lsi-field-enhancement-phase3",
        "lsi-field-enhancement",
        "lsi-field-enhancement-phase4",
        "xynapse-tranche-1",
        "textbf-line-detection-fix",
        "mnemonic-practice-layout",
        "cover-fixes",
        "llm-back-cover-text-processing",
        "lsi-csv-generator-project",
        "lsi-csv-bug-fixes",
        "lsi-pricing-fixes",
        "bisac-category-fixes",
        "lsi-json-formatting-fixes",
        "lsi-field-mapping-corrections",
        "streamlit-ui-config-enhancement",
        "streamlit-form-button-fix",
        "streamlit-ui-runaway-fixes",
        "config-sync-fix",
        "imprint_builder",
        "book-production-fixes",
        "isbn-schedule-assignment",
        "pipeline-configuration-fixes",
        "frontmatter-backmatter-fixes"
      ]
    }
  },
  "tasks": [
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Enhance LLM Field Completer for proper storage\n  - [x] 1.1 Improve directory discovery logic in _save_completions_to_disk method\n    - Update method to better locate appropriate output directory\n    - Add robust directory discovery that looks for existing book directories by publisher_reference_id or ISBN\n    - Ensure directory structure is created if it doesn't exist\n    - Improve logging to show where completions are being saved\n    - _Requirements: 1.1, 1.2, 1.",
      "full_content": "# Implementation Plan\n\n- [x] 1. Enhance LLM Field Completer for proper storage\n  - [x] 1.1 Improve directory discovery logic in _save_completions_to_disk method\n    - Update method to better locate appropriate output directory\n    - Add robust directory discovery that looks for existing book directories by publisher_reference_id or ISBN\n    - Ensure directory structure is created if it doesn't exist\n    - Improve logging to show where completions are being saved\n    - _Requirements: 1.1, 1.2, 1.3_\n\n  - [x] 1.2 Enhance file naming and metadata storage\n    - Use consistent file naming with timestamps and ISBN\n    - Save both timestamped and latest versions for easier access\n    - Include metadata information in saved files for context\n    - Add error handling to prevent failures from stopping the process\n    - _Requirements: 1.3, 1.4_\n\n- [x] 2. Enhance LLM Completion Strategy for CSV integration\n  - [x] 2.1 Update LLMCompletionStrategy to check for existing completions\n    - Modify map_field method to check metadata.llm_completions dictionary\n    - Add support for prompt_key parameter to help locate the right completion\n    - Implement priority order for field value sources\n    - Add detailed logging for field completion source\n    - _Requirements: 2.1, 2.2, 2.4_\n\n  - [x] 2.2 Improve field value extraction logic\n    - Handle different result formats (dictionaries, strings)\n    - Add key matching for dictionary results\n    - Implement fallback to direct field access if not in llm_completions\n    - Add error handling for malformed completion data\n    - _Requirements: 2.2, 2.3_\n\n- [x] 3. Implement Field Completion Reporter\n  - [x] 3.1 Create LSIFieldCompletionReporter class\n    - Implement basic class structure with registry dependency\n    - Add generate_field_strategy_report method\n    - Create _generate_report_data method for data collection\n    - Implement _determine_field_source method for source tracking\n    - _Requirements: 3.1, 3.2_\n\n  - [x] 3.2 Add multiple output format support\n    - Implement _generate_csv_report method\n    - Create _generate_html_report method with statistics and formatting\n    - Add _generate_json_report method for programmatic access\n    - Ensure consistent file naming and organization\n    - _Requirements: 3.3, 3.4_\n\n  - [x] 3.3 Add statistics and visualization\n    - Calculate field population rates and strategy usage\n    - Create visually appealing HTML reports with progress bars\n    - Add color coding for empty fields and potential issues\n    - Include book metadata information for context\n    - _Requirements: 3.4, 3.5_\n\n- [x] 4. Integrate with Book Pipeline\n  - [x] 4.1 Update run_book_pipeline.py to use the new reporter\n    - Add import for LSIFieldCompletionReporter\n    - Initialize reporter with field mapping registry\n    - Generate reports for each book in the batch\n    - Save reports alongside LSI CSV files\n    - _Requirements: 4.1, 4.2_\n\n  - [x] 4.2 Add backward compatibility and error handling\n    - Add fallback to existing report generator\n    - Ensure reports are generated even if some steps fail\n    - Add error handling to continue processing\n    - Improve logging for report generation\n    - _Requirements: 4.3, 4.4_\n\n- [ ] 5. Create comprehensive test suite\n  - [ ] 5.1 Write unit tests for enhanced components\n    - Test LLMFieldCompleter with various directory scenarios\n    - Test LLMCompletionStrategy with different completion sources\n    - Test LSIFieldCompletionReporter with various input data\n    - Achieve comprehensive coverage for new components\n    - _Requirements: 1.1, 2.1, 3.1_\n\n  - [ ] 5.2 Implement integration tests\n    - Test end-to-end flow from LLM completion to reporting\n    - Test with various metadata completeness levels\n    - Test with different output formats and configurations\n    - Test book pipeline integration with error scenarios\n    - _Requirements: 1.2, 2.2, 3.3, 4.1_\n\n- [ ] 6. Update documentation\n  - [ ] 6.1 Update LSI Field Enhancement Guide\n    - Document enhanced LLM field completion storage\n    - Explain LLM completion integration with CSV output\n    - Describe field completion reporting features\n    - Add examples and screenshots of reports\n    - _Requirements: 1.1, 2.1, 3.1, 4.1_\n\n  - [ ] 6.2 Create usage examples and troubleshooting guide\n    - Add examples of different report formats\n    - Create troubleshooting section for common issues\n    - Document directory structure and file naming conventions\n    - Add tips for optimizing field completion\n    - _Requirements: 1.4, 2.4, 3.5, 4.4_",
      "size": 4541,
      "modified": "2025-07-18T21:02:00.255804",
      "spec_directory": "lsi-field-enhancement-phase2",
      "directory_modified": "2025-07-18T21:02:00.255804"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Set up ISBN Database Management System\n  - Create database schema and models for ISBN tracking\n  - Implement core functionality for ISBN status management\n  - _Requirements: 1.1, 1.2, 1.6_\n\n- [x] 1.1 Implement ISBN Database Core Classes\n  - Create ISBNDatabase class with basic CRUD operations\n  - Implement ISBN status tracking (available, privately assigned, publicly assigned)\n  - Add validation for ISBN format and status transitions\n  - _Requirements: 1.2, 1.6_\n\n",
      "full_content": "# Implementation Plan\n\n- [x] 1. Set up ISBN Database Management System\n  - Create database schema and models for ISBN tracking\n  - Implement core functionality for ISBN status management\n  - _Requirements: 1.1, 1.2, 1.6_\n\n- [x] 1.1 Implement ISBN Database Core Classes\n  - Create ISBNDatabase class with basic CRUD operations\n  - Implement ISBN status tracking (available, privately assigned, publicly assigned)\n  - Add validation for ISBN format and status transitions\n  - _Requirements: 1.2, 1.6_\n\n- [x] 1.2 Develop Bowker Spreadsheet Importer\n  - Create ISBNImporter class to parse Bowker spreadsheet formats\n  - Implement detection of available ISBNs during import\n  - Add validation and error handling for import process\n  - _Requirements: 1.1, 1.2_\n\n- [x] 1.3 Implement ISBN Assignment System\n  - Create ISBNAssigner class for automatic ISBN assignment\n  - Implement logic to get next available ISBN for a publisher\n  - Add status transition from available to privately assigned\n  - _Requirements: 1.3, 1.4_\n\n- [x] 1.4 Add ISBN Publication Tracking\n  - Implement functionality to mark ISBNs as publicly assigned\n  - Add validation to prevent reassignment of published ISBNs\n  - Create ISBN release functionality for privately assigned ISBNs\n  - _Requirements: 1.5, 1.6, 1.7_\n\n- [x] 1.5 Create ISBN Database Storage Layer\n  - Implement persistent storage for ISBN database\n  - Add transaction support for concurrent operations\n  - Create backup and recovery mechanisms\n  - _Requirements: 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7_\n\n- [x] 2. Implement Series Metadata Management\n  - Create data models and storage for series information\n  - Implement core functionality for series tracking\n  - _Requirements: 2.1, 2.4, 2.5, 2.6, 2.7_\n\n- [x] 2.1 Create Series Registry Core Classes\n  - Implement SeriesRegistry class with CRUD operations\n  - Add validation for series creation and updates\n  - Implement publisher isolation for series management\n  - _Requirements: 2.1, 2.4, 2.5, 2.6, 2.7_\n\n- [x] 2.2 Develop Series Assignment System\n  - Create SeriesAssigner class for adding books to series\n  - Implement automatic sequence number assignment\n  - Add validation for sequence number integrity\n  - _Requirements: 2.2, 2.3_\n\n- [x] 2.3 Implement Multi-Publisher Series Support\n  - Add functionality to designate series as multi-publisher\n  - Implement access control for series based on publisher\n  - Create validation for multi-publisher operations\n  - _Requirements: 2.5, 2.6, 2.7_\n\n- [x] 2.4 Create Series UI Integration\n  - Implement UI components for series selection\n  - Add functionality to create new series from UI\n  - Integrate series selection with book pipeline\n  - _Requirements: 2.1, 2.3_\n\n- [x] 2.5 Implement Series CRUD Operations\n  - Add comprehensive CRUD operations for series management\n  - Implement integrity checks to prevent renumbering\n  - Create validation for series operations\n  - _Requirements: 2.8_\n\n- [-] 3. Enhance Field Completion System\n  - Extend existing LLMFieldCompleter with new capabilities\n  - Implement specialized field completion strategies\n  - _Requirements: 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9_\n\n- [x] 3.1 Implement Enhanced Annotation/Summary Generation\n  - Create specialized formatter for Annotation/Summary fields\n  - Implement HTML formatting with character limit (max 4000 characters)\n  - Add support for dramatic hook in bold italic as first paragraph\n  - Include back_cover_text and publisher dictionary strings\n  - _Requirements: 3.3_\n\n- [x] 3.2 Enhance BISAC Category Field Generation\n  - Extend LLM completion for BISAC code suggestion\n  - Implement support for user-specified category overrides\n  - Add validation for BISAC code format\n  - _Requirements: 3.4_\n\n- [x] 3.3 Implement Thema Subject Field Generation\n  - Create specialized completion for Thema subject codes\n  - Integrate with suggest_thema_codes function\n  - Add validation for Thema code format\n  - _Requirements: 3.5_\n\n- [x] 3.4 Enhance Contributor Information Extraction\n  - Extend contributor info extraction with comprehensive fields\n  - Implement parsing of contributor location, affiliations, etc.\n  - Add validation for contributor information format\n  - _Requirements: 3.6_\n\n- [x] 3.5 Implement Illustrations Field Generation\n  - Create specialized completion for illustration count and notes\n  - Integrate with gemini_get_basic_info function\n  - Add validation for illustration information\n  - _Requirements: 3.7_\n\n- [x] 3.6 Enhance Table of Contents Generation\n  - Implement improved TOC generation with formatting\n  - Integrate with create_simple_toc function\n  - Add validation for TOC format and length\n  - _Requirements: 3.8_\n\n- [x] 3.7 Update Field Mapping for Reserved Fields\n  - Implement logic to leave specified fields blank\n  - Add validation to ensure these fields remain empty\n  - Update field mapping registry with new strategies\n  - _Requirements: 3.9_\n\n- [ ] 4. Integrate Components with LSI ACS Generator\n  - Connect new components to existing LSI generation system\n  - Ensure seamless operation of the complete pipeline\n  - _Requirements: 1.3, 1.5, 2.2, 2.3, 3.1, 3.2_\n\n- [x] 4.1 Integrate ISBN Database with LSI Generator\n  - Connect ISBN assignment to book pipeline\n  - Implement automatic ISBN status updates during LSI generation\n  - Add validation to ensure ISBN integrity\n  - _Requirements: 1.3, 1.5_\n\n- [x] 4.2 Integrate Series Management with LSI Generator\n  - Connect series assignment to book pipeline\n  - Implement automatic series ID and number assignment\n  - Add validation for series metadata in LSI output\n  - _Requirements: 2.2, 2.3, 3.1, 3.2_\n\n- [x] 4.3 Integrate Enhanced Field Completion with LSI Generator\n  - Connect enhanced field completion to LSI generation\n  - Implement field validation before CSV generation\n  - Add reporting for field completion status\n  - _Requirements: 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9_\n\n- [ ] 5. Create Comprehensive Testing Suite\n  - Develop unit and integration tests for all components\n  - Implement test fixtures and mock data\n  - _Requirements: All_\n\n- [x] 5.1 Create ISBN Database Tests\n  - Implement unit tests for ISBN database operations\n  - Create integration tests for ISBN lifecycle\n  - Add performance tests for concurrent operations\n  - _Requirements: 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7_\n\n- [x] 5.2 Create Series Management Tests\n  - Implement unit tests for series registry operations\n  - Create integration tests for series assignment\n  - Add tests for multi-publisher scenarios\n  - _Requirements: 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8_\n\n- [x] 5.3 Create Enhanced Field Completion Tests\n  - Implement unit tests for each field completion strategy\n  - Create integration tests for the complete field completion pipeline\n  - Add validation tests for field format and content\n  - _Requirements: 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9_\n\n- [ ] 6. Update Documentation\n  - Create comprehensive documentation for all new features\n  - Update existing guides with new functionality\n  - _Requirements: All_\n\n- [x] 6.1 Update LSI Field Enhancement Guide\n  - Add sections for ISBN management\n  - Add sections for series management\n  - Update field completion documentation\n  - _Requirements: All_\n\n- [x] 6.2 Create API Reference Documentation\n  - Document all public APIs for new components\n  - Include examples and usage patterns\n  - Add error handling guidance\n  - _Requirements: All_\n\n- [x] 6.3 Update User Guides\n  - Create user-facing documentation for ISBN management\n  - Create user-facing documentation for series management\n  - Update field completion user guides\n  - _Requirements: All_\n\n- [x] 7. Create Field Completion Reporting System\n  - Implement reporting mechanism for field completion status\n  - Generate detailed reports on field completion quality\n  - _Requirements: 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9_\n\n- [x] 7.1 Implement Field Completion Reporter\n  - Create FieldCompletionReporter class to track completion status\n  - Implement methods to generate detailed reports\n  - Add support for exporting reports in multiple formats\n  - _Requirements: 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9_\n\n- [x] 7.2 Create Field Validation Framework\n  - Implement validation rules for each field type\n  - Create validation pipeline for field completion results\n  - Add support for custom validation rules\n  - _Requirements: 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9_\n\n- [x] 7.3 Integrate Reporting with LSI Generator\n  - Connect field completion reporting to LSI generation process\n  - Add reporting to the book pipeline\n  - Implement notification system for validation failures\n  - _Requirements: 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9_\n\n- [x] 8. Implement Error Recovery Manager\n  - Create system to handle and recover from field completion errors\n  - Implement fallback strategies for failed completions\n  - _Requirements: 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9_\n\n- [x] 8.1 Create Error Recovery Strategies\n  - Implement fallback strategies for different field types\n  - Create retry mechanism with exponential backoff\n  - Add support for manual intervention\n  - _Requirements: 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9_\n\n- [x] 8.2 Implement Error Logging System\n  - Create detailed error logging for field completion failures\n  - Implement error categorization and analysis\n  - Add support for error reporting and notifications\n  - _Requirements: 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9_",
      "size": 9340,
      "modified": "2025-07-18T21:02:00.256615",
      "spec_directory": "lsi-field-enhancement-phase3",
      "directory_modified": "2025-07-18T21:02:00.256615"
    },
    {
      "file": "Task: create and use database of unique .md",
      "content": "Task: create and use database of unique ISBNs owned by each publisher.\n- Each publisher owns one or more blocks of iSBNs.\n- Accept as \"sources of truth\" spreadsheet files downloaded from Bowker that list all the ISBNs that a publisher owns as well as all the ones that have been publicly assigned.\n- The publisher's available ISBNs are all those that have never been publicly assigned.\n- When a publisher uses LSI distribution, an ISBN is required. The system should pull the next available ISBN from",
      "full_content": "Task: create and use database of unique ISBNs owned by each publisher.\n- Each publisher owns one or more blocks of iSBNs.\n- Accept as \"sources of truth\" spreadsheet files downloaded from Bowker that list all the ISBNs that a publisher owns as well as all the ones that have been publicly assigned.\n- The publisher's available ISBNs are all those that have never been publicly assigned.\n- When a publisher uses LSI distribution, an ISBN is required. The system should pull the next available ISBN from the database, mark it as privately assigned, and use it in the book pipeline.\n- The privately assigned ISBN becomes publicly assigned whenever it is uploaded to LSI and can never again be used by the publisher for any other purpose.\n- Privately assigned ISBNs can be released by the publisher at any time before publication.\n\nTask: create and use minimal LSISeriesMetadata class that manages series names and contents for LSI metadata.\n- LSI Series metadata is three fields, name, series unique id, and number in series.\n- Other distributors such as KDP have more elaborate metadata with multiple descriptive fields and icons.\n- For our immediate purposes, we only need the LSI Series Metadata to:\n  - keep track of all unique series names\n  - keep track of all titles belonging to that series and their sequence number in the series\n- We need methods that:\n    - validate series names for consistency\n    - allow for CRUD of titles from series. No renumbering.\n- The user should be able to specify series name when starting the Book Pipeline and have the system automatically assign a series number.\n- The minimum initial requirement is that each publisher may have any number of series.\n- Different publishers may have series with the same name, but they are different series with different ids.\n- Multiple publishers may contribute to the same series.\n- A series is either a single-publisher series or a multi.  Default is single. \n\nTask: improve field completion for listed fields.\n- Series and Series #:\n   - Imprint has list of registered series and their members.\n   - In UI, user can select from registered series, or add a new one.\n   - In either case, title is assigned a series ID #.\n   - Delete the llm_completion option for identifying series, we are not using that at present.\n\n- Annotation/Summary:\n\n    - Constructed in simple HTML for LSI catalog display. Max 4000 characters. No outbound links.\n    - First paragraph: dramatic hook in bold italic.\n    - Then back_cover_text.\n    - Then concatenate new paragraphedlist of strings from a shared dictionary in configs/publisher, e.g. \"nimble_history\": \"Nimble Books was founded in 2006.\"\n\n- BISAC Category, BISAC Category 2, BISAC Category 3: \n    - each field contains one category/code pair that is the result of llm_competion call to suggest_bisac_codes.\n    - the user may specify a BISAC category override that automatically assigns a given code based on user information. This is job-specific. For example, all jobs to create pilsa books might be assigned Journalling:Self-Help for category 3.\n\n- Thema Subject 1, ... 2 and  ...3 and Regional Subjects: \n    - assign based on results of suggest_thema_codes\n\n- Contributor info fields:\n    - assign based on results of extract_lsi_contributor_info\n\n- Illustrations and Illustration Note:\n    - assign based on results of gemini_get_basic_info for Illustrations\n\n- Table of Contents:\n    - assign based on results of create_simple_toc\n\n- Ignore (leave cells blank but retain column headings):\n    - Reserved*\n    - SIBI*\n    - Stamped*\n    - LSI Flexfield*\n    - Review Quotes",
      "size": 3597,
      "modified": "2025-07-18T21:02:00.256138",
      "spec_directory": "lsi-field-enhancement-phase3",
      "directory_modified": "2025-07-18T21:02:00.256615"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Extend CodexMetadata model with missing LSI-specific fields\n  - Add Lightning Source account information fields (lightning_source_account, metadata_contact_dictionary)\n  - Add submission method fields (cover_submission_method, text_block_submission_method)\n  - Add enhanced contributor fields (contributor_one_bio, affiliations, professional_position, location, etc.)\n  - Add physical specification fields (weight_lbs, carton_pack_quantity)\n  - Add publication timing ",
      "full_content": "# Implementation Plan\n\n- [x] 1. Extend CodexMetadata model with missing LSI-specific fields\n  - Add Lightning Source account information fields (lightning_source_account, metadata_contact_dictionary)\n  - Add submission method fields (cover_submission_method, text_block_submission_method)\n  - Add enhanced contributor fields (contributor_one_bio, affiliations, professional_position, location, etc.)\n  - Add physical specification fields (weight_lbs, carton_pack_quantity)\n  - Add publication timing fields (street_date separate from pub_date)\n  - Add territorial rights and edition fields\n  - Add file path fields for submission (jacket_path_filename, interior_path_filename, cover_path_filename)\n  - Add LSI special fields (lsi_special_category, stamped_text fields, order_type_eligibility)\n  - Add LSI flex fields (lsi_flexfield1-5) and publisher_reference_id\n  - _Requirements: 4.1, 4.2, 4.3, 4.4_\n\n- [x] 2. Create field mapping strategy system\n- [x] 2.1 Implement base mapping strategy interface\n  - Create abstract MappingStrategy class with map_field method\n  - Define MappingContext dataclass for field mapping context\n  - Write unit tests for strategy interface\n  - _Requirements: 2.1, 2.2_\n\n- [x] 2.2 Implement concrete mapping strategies\n  - Create DirectMappingStrategy for simple field mappings\n  - Implement ComputedMappingStrategy for calculated fields (weight, pricing)\n  - Build DefaultMappingStrategy for fallback values\n  - Develop ConditionalMappingStrategy for dependent fields\n  - Create LLMCompletionStrategy using existing llm_caller and prompt_manager\n  - _Requirements: 2.1, 2.3_\n\n- [x] 2.3 Build field mapping registry\n  - Create FieldMappingRegistry class to manage strategies\n  - Implement strategy registration and retrieval methods\n  - Add method to apply all mappings to metadata\n  - Write comprehensive unit tests for registry\n  - _Requirements: 1.1, 2.1_\n\n- [x] 2.4 Implement LLM-based field completion system\n  - Create LLMFieldCompleter class using existing llm_caller and prompt_manager\n  - Design prompts for contributor bio generation, BISAC suggestions, and marketing copy\n  - Integrate with existing litellm infrastructure for API calls\n  - Add intelligent field completion for missing LSI metadata\n  - Write unit tests with mocked LLM responses\n  - _Requirements: 1.1, 2.1, 4.1_\n\n- [x] 3. Develop validation system\n- [x] 3.1 Create validation framework\n  - Implement abstract FieldValidator base class\n  - Create ValidationResult and FieldValidationResult dataclasses\n  - Build LSIValidationPipeline to orchestrate validators\n  - Write unit tests for validation framework\n  - _Requirements: 3.1, 3.2, 3.3, 3.4, 3.5_\n\n- [x] 3.2 Implement specific field validators\n  - Create ISBNValidator with format and check-digit validation\n  - Build PricingValidator for currency and discount validation\n  - Implement DateValidator for publication and street dates\n  - Create BISACValidator for BISAC code verification\n  - _Requirements: 3.1, 3.2, 3.3, 3.4_\n\n- [x] 3.3 Add PDF and file validation\n  - Implement PDFValidator for PDF X-1a format checking\n  - Create file existence validator for FTP staging area\n  - Add file naming convention validator (ISBN_interior.pdf format)\n  - Validate page count and trim size consistency\n  - _Requirements: 3.6, 3.7, 3.8_\n\n- [x] 4. Build configuration management system\n- [x] 4.1 Create LSI configuration class\n  - Implement LSIConfiguration class with JSON config loading\n  - Add methods for default values, overrides, and imprint configs\n  - Support territorial configuration management\n  - Create publishers/ and imprints/ directory structure for multiple configurations\n  - Write unit tests for configuration loading\n  - _Requirements: 2.2, 2.3, 2.4_\n\n- [x] 4.2 Design configuration file structure\n  - Create default LSI configuration JSON templates\n  - Define imprint-specific configuration sections\n  - Add territorial pricing and discount configurations\n  - Document configuration options and examples\n  - _Requirements: 2.2, 2.3_\n\n- [x] 5. Enhance existing LSI ACS Generator\n- [x] 5.1 Refactor existing LsiAcsGenerator class to use new mapping system\n  - Update existing _map_metadata_to_lsi_template method to use FieldMappingRegistry\n  - Replace hardcoded field mapping with strategy-based approach\n  - Integrate validation pipeline into generation process\n  - Add configuration support to generator initialization\n  - Maintain backward compatibility with existing interface\n  - _Requirements: 1.1, 1.2, 1.3_\n\n- [x] 5.2 Complete comprehensive field mapping for all 100+ LSI fields\n  - Analyze LSI template to identify all unmapped fields\n  - Create mapping strategies for remaining fields using strategy system\n  - Handle special cases for reserved fields (Reserved 1-12) and flex fields\n  - Implement proper field ordering and CSV formatting\n  - Add support for multiple BISAC categories and contributors\n  - Map all territorial pricing fields (USBR1, USDE1, USRU1, etc.)\n  - _Requirements: 1.1, 1.4_\n\n- [x] 5.3 Add generation result reporting\n  - Create GenerationResult dataclass for detailed reporting\n  - Implement field population statistics and validation summaries\n  - Add timestamp and metadata tracking to results\n  - Write unit tests for result generation\n  - _Requirements: 5.1, 5.2, 5.3, 5.4_\n\n- [x] 6. Implement error handling and recovery\n- [x] 6.1 Create error recovery manager\n  - Implement ErrorRecoveryManager class with correction methods\n  - Add ISBN format correction and validation\n  - Create BISAC code suggestion based on title and keywords\n  - Implement missing pricing calculation for territories\n  - _Requirements: 5.3, 5.4_\n\n- [x] 6.2 Add comprehensive logging system\n  - Integrate detailed logging throughout generation process\n  - Log all field mappings, transformations, and validations\n  - Create structured error and warning message formats\n  - Add performance metrics and timing information\n  - _Requirements: 5.1, 5.2, 5.4_\n\n- [x] 7. Create comprehensive test suite\n- [x] 7.1 Write unit tests for all components\n  - Test field mapping strategies independently\n  - Test validation system with valid and invalid inputs\n  - Test LLM field completion system with mocked responses\n  - Achieve comprehensive coverage for new components\n  - _Requirements: 1.1, 2.1, 3.1, 4.1_\n\n- [x] 7.2 Implement integration tests\n  - Test end-to-end metadata to CSV generation\n  - Test with various metadata completeness levels\n  - Test imprint and territorial configuration scenarios\n  - Validate generated CSV against LSI template requirements\n  - _Requirements: 1.2, 1.3, 2.2, 2.3_\n\n- [x] 7.3 Add file system integration tests\n  - Test PDF validation with sample files\n  - Test FTP staging area file checks\n  - Test file naming convention validation\n  - Mock file system operations for reliable testing\n  - _Requirements: 3.6, 3.7, 3.8_\n\n- [x] 8. Update existing codebase integration\n- [x] 8.1 Update metadata generation workflows\n  - Modify existing metadata processing to populate new LSI fields\n  - Update LLM prompts to extract LSI-specific information\n  - Ensure backward compatibility with existing metadata objects\n  - Test integration with existing book processing pipeline\n  - _Requirements: 4.1, 4.4_\n\n- [x] 8.2 Create migration utilities\n  - Build utility to migrate existing metadata to new format\n  - Add validation for migrated metadata completeness\n  - Create scripts to populate missing LSI fields from existing data\n  - Document migration process and requirements\n  - _Requirements: 4.1, 4.4_\n\n- [x] 9. Documentation and examples\n- [x] 9.1 Create comprehensive documentation\n  - Document all new classes, methods, and configuration options\n  - Create usage examples for different publishing scenarios\n  - Document LSI field mappings and validation rules\n  - Add troubleshooting guide for common issues\n  - _Requirements: 2.1, 3.1, 5.4_\n\n- [x] 9.2 Create sample configurations and test data\n  - Provide sample configuration files for different imprints\n  - Create test metadata objects with various completeness levels\n  - Add example CSV outputs for validation\n  - Document best practices for LSI submission preparation\n  - _Requirements: 2.2, 2.3, 5.4_\n\n## Punch List Fixes\n\n- [x] 10. Fix CSV output to include all pipeline job rows\n  - Ensure CSV generation includes complete row data from current pipeline\n  - Verify all existing metadata fields are properly mapped to CSV output\n  - Test CSV generation with multiple book records from pipeline\n  - _Requirements: 1.3, 6.2_\n\n- [x] 11. Remove specific discount mode cell values from output\n  - Ensure these columns are blank in CSV output: \"US-Ingram-Only* Suggested List Price (mode 2)\", \"US-Ingram-Only* Wholesale Discount % (Mode 2)\", \"US - Ingram - GAP * Suggested List Price (mode 2)\", \"US - Ingram - GAP * Wholesale Discount % (Mode 2)\", \"SIBI - EDUC - US * Suggested List Price (mode 2)\", \"SIBI - EDUC - US * Wholesale Discount % (Mode 2)\"\n  - Update field mapping strategies to leave these specific mode 2 fields empty\n  - Verify CSV template excludes these columns or maps them to empty values\n  - _Requirements: 1.2, 1.3_\n\n## Additional Enhancement Tasks\n\n\n- [x] 15. Add advanced territorial pricing strategies\n  - add a method that dynamically calculates pricing multiplier for each territory based on today's exchange rates\n    - include \"wiggle room\" parameter that allows an extra x% increase to allow for possible unexpected variations\n    - include \"market access fee\" that allows an extra y USD surcharge.\n  - Allow market-specific pricing strategies for different territories\n  - Create pricing validation against LSI territory requirements\n  - Add support for promotional pricing and special discounts\n  - _Requirements: 2.3, 2.4_\n\n\n- [x] 18. Optimize field completion prompts\n  - Review and refine LLM prompts for LSI-specific field completion\n  - Use the entire main content (body) of the book when creating information about the book as a whole, e.g. description, BISAC codes, etc.\n  - Enhance contributor information extraction prompts to extract information about up to 3 contributors total (but no more) if the body of the document contains it.\n  - Test prompt effectiveness with real-world metadata\n  - _Requirements: 1.1, 2.1, 4.1_\n\n- [x] 17. Enhance validation with LSI-specific business rules\n  - Add validation for LSI account-specific requirements\n  - Implement validation for territorial distribution restrictions\n  - Add checks for LSI special category eligibility\n  - Validate file naming conventions against LSI standards\n  - _Requirements: 3.1, 3.2, 3.5_",
      "size": 10555,
      "modified": "2025-07-18T21:02:00.257098",
      "spec_directory": "lsi-field-enhancement",
      "directory_modified": "2025-07-18T21:02:00.257098"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [ ] 1. Enhance LLM Field Completion\n  - Create improved prompts for all field types\n  - Add retry logic and better error handling\n  - Implement intelligent fallback values\n  - _Requirements: 1, 4_\n\n- [x] 1.1 Fix LLM completion storage issue\n  - Modify LLM field completer to save all completions to metadata\n  - Ensure completions are saved BEFORE filtering via field mapping strategies\n  - Add tracking for all 12 LLM completions\n  - _Requirements: 4_\n\n- [x] 1.2 Improve LLM",
      "full_content": "# Implementation Plan\n\n- [ ] 1. Enhance LLM Field Completion\n  - Create improved prompts for all field types\n  - Add retry logic and better error handling\n  - Implement intelligent fallback values\n  - _Requirements: 1, 4_\n\n- [x] 1.1 Fix LLM completion storage issue\n  - Modify LLM field completer to save all completions to metadata\n  - Ensure completions are saved BEFORE filtering via field mapping strategies\n  - Add tracking for all 12 LLM completions\n  - _Requirements: 4_\n\n- [x] 1.2 Improve LLM field completion prompts\n  - Update contributor bio prompt with more detailed instructions\n  - Enhance BISAC code suggestion prompt with better context handling\n  - Improve annotation/summary prompt with LSI-specific formatting\n  - Add fallback value templates to each prompt\n  - _Requirements: 1, 4_\n\n- [x] 1.3 Implement retry logic and error handling\n  - Add configurable retry attempts for LLM calls\n  - Implement exponential backoff for retries\n  - Add detailed error logging for failed completions\n  - Implement graceful degradation when LLM services are unavailable\n  - _Requirements: 1, 2_\n\n- [x] 1.4 Develop intelligent fallback values\n  - Create field-specific fallback generators\n  - Implement template-based fallback system\n  - Add context-aware fallback generation\n  - Ensure fallbacks meet minimum LSI requirements\n  - _Requirements: 1_\n\n- [ ] 2. Expand Computed Fields\n  - Implement new computed field strategies\n  - Add territorial pricing calculation\n  - Add physical specifications calculation\n  - Add date and file path computation\n  - _Requirements: 1_\n\n- [ ] 2.1 Implement territorial pricing calculation\n  - Create TerritorialPricingStrategy class\n  - Add exchange rate handling with configurable rates\n  - Implement price rounding and formatting\n  - Add support for all LSI territories\n  - _Requirements: 1_\n\n- [x] 2.2 Implement physical specifications calculation\n  - Create PhysicalSpecsStrategy class\n  - Add weight calculation based on page count and trim size\n  - Add spine width calculation\n  - Implement industry-standard formulas\n  - _Requirements: 1_\n\n- [x] 2.3 Implement date calculation\n  - Create DateComputationStrategy class\n  - Add publication date calculation\n  - Add street date calculation with configurable offset\n  - Implement date formatting for LSI requirements\n  - _Requirements: 1_\n\n- [x] 2.4 Implement file path generation\n  - Create FilePathStrategy class\n  - Add cover file path generation\n  - Add interior file path generation\n  - Add jacket file path generation\n  - Implement LSI naming conventions\n  - _Requirements: 1_\n\n- [ ] 3. Enhance Default Values System\n  - Implement multi-level configuration\n  - Add imprint-specific defaults\n  - Add publisher-specific defaults\n  - Enhance global defaults\n  - _Requirements: 1_\n\n- [x] 3.1 Implement multi-level configuration\n  - Create ConfigurationLevel class\n  - Add priority-based configuration resolution\n  - Implement configuration inheritance\n  - Add configuration validation\n  - _Requirements: 1_\n\n- [x] 3.2 Add imprint-specific default values\n  - Create imprint configuration template\n  - Add imprint-specific default values for common fields\n  - Implement imprint configuration loading\n  - Add imprint configuration validation\n  - _Requirements: 1_\n\n- [x] 3.3 Add publisher-specific default values\n  - Create publisher configuration template\n  - Add publisher-specific default values for common fields\n  - Implement publisher configuration loading\n  - Add publisher configuration validation\n  - _Requirements: 1_\n\n- [x] 3.4 Enhance global default values\n  - Update global default values for all common fields\n  - Add more comprehensive default values\n  - Implement default value validation\n  - Add default value documentation\n  - _Requirements: 1_\n\n- [ ] 4. Improve Field Mapping System\n  - Implement field name normalization\n  - Add support for field name variations\n  - Enhance field mapping registry\n  - Add detailed logging\n  - _Requirements: 1, 2_\n\n- [ ] 4.1 Implement field name normalization\n  - Create FieldNameNormalizer class\n  - Add normalization rules for common patterns\n  - Implement case-insensitive matching\n  - Add special character handling\n  - _Requirements: 1_\n\n- [ ] 4.2 Add support for field name variations\n  - Create field name variation generator\n  - Add common variation patterns\n  - Implement variation matching\n  - Add variation caching for performance\n  - _Requirements: 1_\n\n- [ ] 4.3 Enhance field mapping registry\n  - Update FieldMappingRegistry to use normalization\n  - Add variation-aware strategy registration\n  - Implement fuzzy matching for field names\n  - Add performance optimizations\n  - _Requirements: 1_\n\n- [ ] 4.4 Add detailed logging for field mapping\n  - Implement field mapping logging\n  - Add strategy selection logging\n  - Add field value transformation logging\n  - Create field mapping report generator\n  - _Requirements: 2, 3_\n\n- [ ] 5. Enhance Logging System\n  - Implement configurable verbosity levels\n  - Add severity-based filtering\n  - Create structured logging format\n  - Add performance monitoring\n  - _Requirements: 2, 3_\n\n- [ ] 5.1 Implement configurable verbosity levels\n  - Create EnhancedLSILoggingManager class\n  - Add verbosity level configuration\n  - Implement conditional logging based on verbosity\n  - Add verbosity level documentation\n  - _Requirements: 2, 3_\n\n- [ ] 5.2 Add severity-based filtering\n  - Implement severity levels for log entries\n  - Add filtering by severity\n  - Create filtered log views\n  - Add severity level documentation\n  - _Requirements: 3_\n\n- [ ] 5.3 Create structured logging format\n  - Implement JSON-based log format\n  - Add timestamp and unique identifiers\n  - Create log entry categories\n  - Add structured log documentation\n  - _Requirements: 2_\n\n- [ ] 5.4 Add performance monitoring\n  - Implement performance logging\n  - Add timing for key operations\n  - Create performance report\n  - Add performance monitoring documentation\n  - _Requirements: 2_\n\n- [ ] 6. Enhance Reporting System\n  - Implement HTML report generation\n  - Add field population statistics\n  - Create visualization components\n  - Add recommendation generation\n  - _Requirements: 4_\n\n- [ ] 6.1 Implement HTML report generation\n  - Create EnhancedLSIFieldReportGenerator class\n  - Add HTML report template\n  - Implement report data generation\n  - Add HTML report styling\n  - _Requirements: 4_\n\n- [ ] 6.2 Add field population statistics\n  - Implement field population rate calculation\n  - Add field-level population tracking\n  - Create field population summary\n  - Add field population visualization\n  - _Requirements: 4_\n\n- [ ] 6.3 Create visualization components\n  - Implement field population chart\n  - Add validation error visualization\n  - Create LLM completion success chart\n  - Add interactive report elements\n  - _Requirements: 4_\n\n- [ ] 6.4 Add recommendation generation\n  - Implement recommendation engine\n  - Add field-specific recommendations\n  - Create actionable suggestions\n  - Add recommendation prioritization\n  - _Requirements: 4_\n\n- [ ] 7. Integration and Testing\n  - Integrate all components\n  - Add comprehensive tests\n  - Optimize performance\n  - Create documentation\n  - _Requirements: All_\n\n- [ ] 7.1 Integrate all components\n  - Update LsiAcsGenerator to use enhanced components\n  - Integrate enhanced LLM field completer\n  - Add expanded computed fields\n  - Implement multi-level configuration\n  - Add field name normalization\n  - _Requirements: All_\n\n- [x] 7.2 Test with xynapse_traces_schedule.json\n  - Set up test environment for rows 1-12\n  - Run pipeline against test data\n  - Verify 100% field population\n  - Fix any issues found during testing\n  - _Requirements: 5_\n\n- [ ] 7.3 Add comprehensive tests\n  - Create unit tests for all new components\n  - Add integration tests for end-to-end workflow\n  - Implement validation tests for LSI compliance\n  - Add performance tests\n  - _Requirements: All_\n\n- [ ] 7.4 Create documentation\n  - Update LSI Field Enhancement Guide\n  - Add configuration documentation\n  - Create troubleshooting guide\n  - Add examples and best practices\n  - _Requirements: All_",
      "size": 8094,
      "modified": "2025-07-19T04:48:35.438525",
      "spec_directory": "lsi-field-enhancement-phase4",
      "directory_modified": "2025-07-19T17:23:37.888292"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Set Lightning Source Account # to 6024045\n  - Lightning Source Account is already set to \"6024045\" in configs/default_lsi_config.json\n  - Account number appears correctly in generated CSV files\n  - _Punch List Item: 1_\n\n- [x] 2. Exclude Metadata Contact Dictionary from CSV output\n  - Modify LSI CSV generator to skip \"Metadata Contact Dictionary\" field in output\n  - Test that field does not appear in final CSV files\n  - _Punch List Item: 2_\n\n- [x] 3. Ensure Parent ",
      "full_content": "# Implementation Plan\n\n- [x] 1. Set Lightning Source Account # to 6024045\n  - Lightning Source Account is already set to \"6024045\" in configs/default_lsi_config.json\n  - Account number appears correctly in generated CSV files\n  - _Punch List Item: 1_\n\n- [x] 2. Exclude Metadata Contact Dictionary from CSV output\n  - Modify LSI CSV generator to skip \"Metadata Contact Dictionary\" field in output\n  - Test that field does not appear in final CSV files\n  - _Punch List Item: 2_\n\n- [x] 3. Ensure Parent ISBN field is empty\n  - Verify Parent ISBN field mapping returns empty string for this tranche\n  - Test that Parent ISBN is empty in all generated CSV files\n  - _Punch List Item: 3_\n\n- [x] 4. Load ISBN database with real data\n  - Imported authoritative ISBN data from /Users/fred/Downloads/prefix-978160888.csv using corrected logic\n  - Database contains exactly 1,000 ISBNs: 680 available, 320 publicly assigned (with complete metadata)\n  - Status correctly determined: ISBNs with Title, Format, and Status non-empty are publicly assigned\n  - Ready to import additional Bowker files as they become available\n  - _Punch List Item: 4_\n\n- [x] 5. Assign unique ISBNs from database\n  - Implement ISBN assignment functionality to assign unique ISBNs to each book\n  - Test that each book gets a unique, valid ISBN from the database\n  - _Punch List Item: 5_\n\n- [x] 6. Create tranche configuration system\n  - Create configs/tranches/ directory structure\n  - Create configs/tranches/xynapse_tranche_1.json with batch-level settings\n  - Modify pipeline to load and apply tranche config to all books in batch\n  - Test that tranche settings are applied consistently to all 12 books\n  - _Punch List Item: 6_\n\n- [x] 7. Add rendition booktype validation\n  - Implement validation check in field mapping using existing lsi_valid_rendition_booktypes.txt\n  - Add validation to ensure booktype is in valid list before CSV generation\n  - Test validation with both valid and invalid booktype values\n  - _Punch List Item: 7_\n\n- [x] 8. Add contributor role validation\n  - Created contributor_role_validator.py with validation against LSI codes\n  - Implemented ContributorRoleMappingStrategy for field mapping\n  - Updated enhanced_field_mappings.py to use the new mapping strategy\n  - Added comprehensive tests for validation and correction\n  - _Punch List Item: 8_\n\n- [x] 9. Implement Tuesday publication date distribution\n  - Create date calculation utility to find all Tuesdays in target month\n  - Read month/year from existing xynapse_traces_schedule.json\n  - Implement date assignment logic to spread 12 books across available Tuesdays\n  - Test that dates are assigned correctly and evenly distributed\n  - _Punch List Item: 9_\n\n- [x] 10. Combine LLM results with boilerplate for annotations\n  - Created AnnotationProcessor class to handle boilerplate application\n  - Modified LLM field completer to prepare annotations for boilerplate\n  - Updated generate_lsi_csv.py to use AnnotationProcessor\n  - Added comprehensive tests for annotation processing\n  - _Punch List Item: 10_\n\n- [x] 11. Strip codes from BISAC Subject fields\n  - Created bisac_utils.py with strip_bisac_code and get_bisac_code functions\n  - Created BisacCategoryMappingStrategy to handle BISAC field mapping\n  - Updated enhanced_field_mappings.py to use the new mapping strategy\n  - Added comprehensive tests for all components\n  - _Punch List Item: 11_\n\n- [x] 12. Add tranche BISAC Subject override\n  - Added get_tranche_bisac_subject method to TrancheConfigLoader\n  - Modified BisacCategoryMappingStrategy to check for tranche override\n  - Updated FieldMappingRegistry to include context_config for overrides\n  - Added comprehensive tests for tranche BISAC override functionality\n  - _Punch List Item: 12_\n\n- [x] 13. Add short description length validation\n  - Created text_length_validator.py with validation and truncation functions\n  - Implemented ShortDescriptionMappingStrategy for field mapping\n  - Updated enhanced_field_mappings.py to use the new mapping strategy\n  - Added comprehensive tests for validation and truncation\n  - _Punch List Item: 13_\n\n- [x] 14. Fix truncated Thema subjects\n  - Created thema_subject_mapping.py with comprehensive Thema code mappings\n  - Implemented ThemaSubjectMappingStrategy for field mapping\n  - Updated enhanced_field_mappings.py to use the new mapping strategy\n  - Added tests to verify proper expansion of Thema codes\n  - _Punch List Item: 14_\n\n- [x] 15. Set GC market prices equal to US price\n  - Modified territorial_pricing.py to return base price for GC territory\n  - Created test_territorial_pricing.py to verify the implementation\n  - Confirmed that GC market prices are now equal to US price\n  - _Punch List Item: 15_",
      "size": 4719,
      "modified": "2025-07-20T15:23:30.266950",
      "spec_directory": "xynapse-tranche-1",
      "directory_modified": "2025-07-20T15:23:30.266950"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Create unit tests for the regex pattern\n  - Write test cases for various `\\textbf` positioning scenarios\n  - Test edge cases like empty strings and strings without `\\textbf` commands\n  - Verify the regex pattern correctly identifies line-beginning `\\textbf` commands\n  - _Requirements: 1.1, 1.2, 1.3, 2.3_\n\n- [ ] 2. Implement the regex-based fix in prepress.py\n  - Add import for `re` module if not already present\n  - Replace the string counting logic with regex patt",
      "full_content": "# Implementation Plan\n\n- [x] 1. Create unit tests for the regex pattern\n  - Write test cases for various `\\textbf` positioning scenarios\n  - Test edge cases like empty strings and strings without `\\textbf` commands\n  - Verify the regex pattern correctly identifies line-beginning `\\textbf` commands\n  - _Requirements: 1.1, 1.2, 1.3, 2.3_\n\n- [ ] 2. Implement the regex-based fix in prepress.py\n  - Add import for `re` module if not already present\n  - Replace the string counting logic with regex pattern matching\n  - Add clear comments explaining the regex pattern and its purpose\n  - Ensure the fix maintains the same variable name and integration points\n  - _Requirements: 1.1, 1.2, 1.3, 2.1, 2.2_\n\n- [ ] 3. Test the implementation with actual mnemonics content\n  - Run the modified code with real mnemonics LaTeX content\n  - Verify that the count is accurate and practice pages are generated correctly\n  - Check that logging output shows the correct count\n  - Ensure no regressions in the surrounding functionality\n  - _Requirements: 1.4, 2.3_",
      "size": 1046,
      "modified": "2025-07-20T19:51:45.228220",
      "spec_directory": "textbf-line-detection-fix",
      "directory_modified": "2025-07-20T19:51:45.228220"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Create LaTeX template commands for mnemonic practice layout\n  - Add new `\\mnemonicwithpractice` command to handle verso/recto page pairs\n  - Add enhanced `\\fullpagedotgridwithinstruction` command for practice pages with bottom instructions\n  - Add counter system for practice page numbering\n  - Test commands compile correctly in LaTeX\n  - _Requirements: 1.1, 1.2, 1.3, 3.2_\n\n- [x] 2. Implement mnemonic extraction logic in prepress system\n  - Create function to parse",
      "full_content": "# Implementation Plan\n\n- [x] 1. Create LaTeX template commands for mnemonic practice layout\n  - Add new `\\mnemonicwithpractice` command to handle verso/recto page pairs\n  - Add enhanced `\\fullpagedotgridwithinstruction` command for practice pages with bottom instructions\n  - Add counter system for practice page numbering\n  - Test commands compile correctly in LaTeX\n  - _Requirements: 1.1, 1.2, 1.3, 3.2_\n\n- [x] 2. Implement mnemonic extraction logic in prepress system\n  - Create function to parse individual mnemonics from `mnemonics_tex` content\n  - Use regex to split on line-beginning `\\textbf` commands\n  - Handle edge cases like empty content and malformed LaTeX\n  - Add unit tests for extraction logic\n  - _Requirements: 1.1, 4.1, 4.2_\n\n- [x] 3. Modify prepress processing to generate alternating layout\n  - Update `imprints/xynapse_traces/prepress.py` to use new layout system\n  - Replace current batch processing with individual mnemonic processing\n  - Generate verso/recto pairs for each mnemonic with sequential numbering\n  - Maintain backward compatibility with existing mnemonic processing\n  - _Requirements: 1.1, 1.2, 1.3, 3.1, 4.3_\n\n- [x] 4. Enhance dot grid system for instruction placement\n  - Modify dot grid generation to support bottom instruction text\n  - Ensure proper spacing between dot grid and instruction text\n  - Test instruction text positioning and formatting\n  - Verify dot grid scaling works with instruction area\n  - _Requirements: 1.3, 2.1, 2.2, 2.3_\n\n- [x] 5. Add comprehensive error handling and logging\n  - Handle missing dot grid files gracefully\n  - Add logging for mnemonic count and processing steps\n  - Implement fallback behavior for malformed content\n  - Add validation for page sequencing\n  - _Requirements: 3.1, 4.1, 4.2, 4.3_\n\n- [x] 6. Create integration tests for end-to-end processing\n  - Test complete pipeline from JSON input to LaTeX output\n  - Verify correct page sequencing and numbering\n  - Test with various mnemonic counts (1, 5, 10+ mnemonics)\n  - Validate generated LaTeX compiles without errors\n  - _Requirements: 1.1, 1.2, 1.3, 2.1, 2.2, 2.3, 4.1, 4.2, 4.3_\n\n- [x] 7. Evaluate and potentially refine mnemonic formatting\n  - Test current `\\formattedquote` formatting with actual mnemonic content\n  - Assess readability and layout effectiveness\n  - Create alternative formatting options if needed\n  - Document formatting recommendations\n  - _Requirements: 1.1, 3.2_\n\n- [x] 8. Update documentation and add configuration options\n  - Document new LaTeX commands and their usage\n  - Add configuration examples for different layouts\n  - Create troubleshooting guide for common issues\n  - Update existing documentation to reference new layout system\n  - _Requirements: 3.1, 3.2_",
      "size": 2734,
      "modified": "2025-07-20T20:17:00.627804",
      "spec_directory": "mnemonic-practice-layout",
      "directory_modified": "2025-07-22T02:31:54.566794"
    },
    {
      "file": "tasks.md",
      "content": "# Cover Fixes Implementation Plan\n\n## Task 1: Implement Template Variable Substitution System\n\n- [ ] 1.1 Create substitute_template_variables function\n  - Write function to handle {stream}, {title}, {description}, {quotes_per_book} substitution\n  - Implement fallback values for missing variables\n  - Add circular reference detection\n  - _Requirements: 1.1, 1.2, 1.3, 1.4_\n\n- [ ] 1.2 Integrate variable substitution into cover generation pipeline\n  - Modify create_cover_latex function to call substi",
      "full_content": "# Cover Fixes Implementation Plan\n\n## Task 1: Implement Template Variable Substitution System\n\n- [ ] 1.1 Create substitute_template_variables function\n  - Write function to handle {stream}, {title}, {description}, {quotes_per_book} substitution\n  - Implement fallback values for missing variables\n  - Add circular reference detection\n  - _Requirements: 1.1, 1.2, 1.3, 1.4_\n\n- [ ] 1.2 Integrate variable substitution into cover generation pipeline\n  - Modify create_cover_latex function to call substitute_template_variables\n  - Apply substitution to back_cover_text before LaTeX escaping\n  - Add logging for substitution operations\n  - _Requirements: 1.1, 1.2, 1.3, 1.4, 1.5_\n\n## Task 2: Enhance Korean Text Processing\n\n- [ ] 2.1 Create Korean-aware LaTeX escaping system\n  - Implement placeholder system for \\korean{} commands\n  - Modify _escape_latex to preserve Korean commands\n  - Add pattern matching for Korean LaTeX commands\n  - _Requirements: 2.1, 2.2, 2.3, 2.4_\n\n- [ ] 2.2 Update cover template Korean text handling\n  - Ensure coverPilsaLineOne uses proper Korean command format\n  - Verify Korean font definition is correct\n  - Test Korean text rendering in LaTeX compilation\n  - _Requirements: 2.2, 2.3, 2.4, 2.5_\n\n## Task 3: Create Comprehensive Test Suite\n\n- [ ] 3.1 Write unit tests for variable substitution\n  - Test individual variable substitution\n  - Test multiple variables in same text\n  - Test missing data scenarios\n  - Test circular reference prevention\n  - _Requirements: 1.1, 1.2, 1.3, 1.4, 1.5_\n\n- [ ] 3.2 Write unit tests for Korean text processing\n  - Test \\korean{} command preservation during escaping\n  - Test mixed Korean/English text processing\n  - Test malformed command handling\n  - _Requirements: 2.1, 2.2, 2.3, 2.4, 2.5_\n\n- [ ] 3.3 Create integration test for complete cover generation\n  - Test end-to-end cover generation with both fixes\n  - Verify LaTeX compilation succeeds\n  - Check final PNG output quality\n  - Validate both front and back cover content\n  - _Requirements: All requirements_\n\n## Task 4: Validate and Deploy Fixes\n\n- [ ] 4.1 Test with real book data\n  - Run cover generation with actual book from CSV\n  - Verify back cover text substitution works correctly\n  - Confirm Korean text displays properly on front cover\n  - Check for any remaining issues\n  - _Requirements: All requirements_\n\n- [ ] 4.2 Update documentation and commit changes\n  - Document new variable substitution system\n  - Update Korean text handling documentation\n  - Commit all changes with comprehensive tests\n  - _Requirements: All requirements_",
      "size": 2569,
      "modified": "2025-07-28T00:27:39.502680",
      "spec_directory": "cover-fixes",
      "directory_modified": "2025-07-28T00:27:39.502680"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Enhance Stage 1 content generation to include back cover text generation\n  - Add back cover text generation function to `llm_get_book_data.py`\n  - Use existing LLM infrastructure to generate clean, final text\n  - Store result in book data for later use during cover generation\n  - _Requirements: 1.1, 1.2, 1.3, 3.1, 3.2_\n\n- [ ] 2. Update back cover text prompt configuration\n  - Modify existing `back_cover_text` prompt in `prompts.json`\n  - Change prompt to generate ",
      "full_content": "# Implementation Plan\n\n- [x] 1. Enhance Stage 1 content generation to include back cover text generation\n  - Add back cover text generation function to `llm_get_book_data.py`\n  - Use existing LLM infrastructure to generate clean, final text\n  - Store result in book data for later use during cover generation\n  - _Requirements: 1.1, 1.2, 1.3, 3.1, 3.2_\n\n- [ ] 2. Update back cover text prompt configuration\n  - Modify existing `back_cover_text` prompt in `prompts.json`\n  - Change prompt to generate final text instead of template with variables\n  - Ensure prompt produces clean, publication-ready text\n  - _Requirements: 1.1, 1.2, 3.3_\n\n- [ ] 3. Implement back cover text generation in Stage 1 pipeline\n  - Integrate back cover text generation into existing content generation workflow\n  - Call LLM after quote generation when full context is available\n  - Add error handling and fallback logic for failed generation\n  - _Requirements: 1.1, 1.2, 1.4, 2.2, 4.3_\n\n- [ ] 4. Add validation and fallback mechanisms\n  - Implement response validation for generated back cover text\n  - Create fallback text generation using book metadata\n  - Add comprehensive logging for debugging and monitoring\n  - _Requirements: 2.2, 2.4, 4.1, 4.4_\n\n- [ ] 5. Simplify cover generator to use pre-generated text\n  - Remove variable substitution logic from `cover_generator.py`\n  - Update to use `back_cover_text` field directly from book data\n  - Remove unused `substitute_template_variables()` function\n  - _Requirements: 2.1, 2.3_\n\n- [ ] 6. Update cover generation to handle pre-processed text\n  - Modify cover generation to expect clean, final back cover text\n  - Ensure LaTeX escaping still works correctly with generated text\n  - Test cover compilation with LLM-generated back cover text\n  - _Requirements: 1.4, 2.1, 2.3_\n\n- [ ] 7. Create comprehensive tests for back cover text generation\n  - Write unit tests for back cover text generation function\n  - Test LLM integration with various book metadata scenarios\n  - Test fallback behavior when LLM generation fails\n  - _Requirements: 2.2, 4.3_\n\n- [ ] 8. Test full pipeline integration\n  - Run complete book pipeline from Stage 1 through cover generation\n  - Verify back cover text is generated correctly in Stage 1\n  - Confirm cover generation uses pre-generated text without issues\n  - _Requirements: 2.1, 2.3, 2.4_\n\n- [ ] 9. Add monitoring and configuration options\n  - Add LLM usage tracking for back cover text generation\n  - Implement configurable settings for text generation parameters\n  - Add success/failure metrics for monitoring\n  - _Requirements: 4.1, 4.2, 4.4_\n\n- [ ] 10. Validate and optimize text quality\n  - Test generated back cover text quality across different book types\n  - Fine-tune prompt parameters for optimal results\n  - Ensure generated text meets length and style requirements\n  - _Requirements: 1.3, 3.3, 3.4_",
      "size": 2872,
      "modified": "2025-07-28T04:08:55.856371",
      "spec_directory": "llm-back-cover-text-processing",
      "directory_modified": "2025-07-28T07:17:57.229868"
    },
    {
      "file": "tasks.md",
      "content": "# LSI CSV Bug Fixes - Implementation Tasks\n\n## Task Overview\n\nThis implementation plan focuses on systematically fixing bugs in the existing LSI CSV generation system. Tasks are organized by priority and dependency, with each task building on previous fixes.\n\n## Implementation Tasks\n\n### Phase 1: Critical Prompt and JSON Fixes\n\n- [x] 1. Fix remaining old-style prompts causing JSON parsing errors\n  - Audit all prompt files for old \"prompt\" format usage\n  - Convert remaining prompts to messages fo",
      "full_content": "# LSI CSV Bug Fixes - Implementation Tasks\n\n## Task Overview\n\nThis implementation plan focuses on systematically fixing bugs in the existing LSI CSV generation system. Tasks are organized by priority and dependency, with each task building on previous fixes.\n\n## Implementation Tasks\n\n### Phase 1: Critical Prompt and JSON Fixes\n\n- [x] 1. Fix remaining old-style prompts causing JSON parsing errors\n  - Audit all prompt files for old \"prompt\" format usage\n  - Convert remaining prompts to messages format with JSON enforcement\n  - Test all prompts to ensure valid JSON responses\n  - _Requirements: 1.1, 1.2, 1.3, 1.4, 1.5_\n\n- [x] 2. Implement robust JSON response validation\n  - Add JSON validation wrapper for all LLM responses\n  - Implement fallback mechanisms for malformed responses\n  - Add detailed logging for JSON parsing failures\n  - _Requirements: 1.1, 1.2, 1.3, 6.1, 6.4_\n\n- [x] 3. Fix bibliography prompt and similar field completion issues\n  - Update bibliography prompt to use book content context properly\n  - Fix other prompts that return conversational text instead of JSON\n  - Add response format validation for all field completion prompts\n  - _Requirements: 1.1, 1.4, 2.1, 2.2_\n\n### Phase 2: Field Completion and Validation Fixes\n\n- [x] 4. Improve BISAC code generation and validation\n  - Update BISAC code validation with current 2024 standards\n  - Fix BISAC code generation prompts to return valid codes\n  - Add fallback BISAC codes for common topics\n  - _Requirements: 2.1, 3.1, 3.4_\n\n- [x] 5. Fix description length and formatting issues\n  - Implement intelligent text truncation at sentence boundaries\n  - Add character limit validation for all description fields\n  - Fix HTML formatting issues in long descriptions\n  - _Requirements: 2.2, 3.1, 3.4_\n\n- [x] 6. Enhance contributor information generation\n  - Improve contributor bio generation with book context\n  - Fix contributor role validation and mapping\n  - Add fallback contributor information for missing data\n  - _Requirements: 2.5, 7.1, 7.2_\n\n- [x] 7. Fix age range and audience determination\n  - Update age range logic to provide valid numeric ranges\n  - Fix audience classification based on content analysis\n  - Add validation for age range consistency\n  - _Requirements: 2.3, 3.1, 3.4_\n\n### Phase 3: Configuration and System Fixes\n\n- [x] 8. Fix multi-level configuration inheritance\n  - Debug and fix configuration loading order issues\n  - Implement proper default value handling\n  - Add configuration validation and error reporting\n  - _Requirements: 4.1, 4.2, 4.3, 4.5_\n\n- [ ] 9. Resolve tranche configuration application issues\n  - Fix tranche settings not being applied correctly\n  - Debug annotation boilerplate processing\n  - Add tranche configuration debugging tools\n  - _Requirements: 4.4, 7.3, 7.4_\n\n- [ ] 10. Implement robust error handling for configuration\n  - Add graceful handling of missing configuration files\n  - Implement configuration syntax validation\n  - Add detailed error messages for configuration issues\n  - _Requirements: 4.2, 4.3, 6.1, 6.4_\n\n### Phase 4: Batch Processing and Performance Fixes\n\n- [x] 11. Fix batch processing failure cascades\n  - Implement error isolation for individual book processing\n  - Add batch processing resume capability\n  - Fix memory leaks in large batch processing\n  - _Requirements: 5.1, 5.3, 5.4, 8.2_\n\n- [ ] 12. Improve batch processing error reporting\n  - Add comprehensive batch processing reports\n  - Implement detailed error logging for failed books\n  - Add progress tracking and status updates\n  - _Requirements: 5.2, 5.5, 6.2, 6.4_\n\n- [ ] 13. Optimize memory usage and performance\n  - Fix memory leaks in LLM field completion\n  - Implement efficient caching for repeated operations\n  - Add streaming processing for large datasets\n  - _Requirements: 8.1, 8.2, 8.3, 8.5_\n\n### Phase 5: Validation and Quality Assurance\n\n- [x] 14. Implement comprehensive LSI field validation\n  - Create complete LSI field rules database\n  - Add field-by-field validation with specific error messages\n  - Implement validation report generation\n  - _Requirements: 3.1, 3.2, 3.4, 6.4_\n\n- [x] 15. Fix field mapping and transformation issues\n  - Debug and fix metadata field mapping problems\n  - Implement proper data type conversion and validation\n  - Add field dependency handling and validation\n  - _Requirements: 7.1, 7.2, 7.3, 7.5_\n\n- [x] 16. Add comprehensive validation reporting\n  - Implement detailed validation reports with suggestions\n  - Add field completeness analysis and scoring\n  - Create validation dashboard for monitoring\n  - _Requirements: 3.4, 6.1, 6.4_\n\n### Phase 6: Enhanced Error Handling and Monitoring\n\n- [x] 17. Implement retry logic for LLM failures\n  - Add exponential backoff for rate limiting\n  - Implement intelligent retry strategies for different error types\n  - Add circuit breaker pattern for persistent failures\n  - _Requirements: 6.3, 8.3, 8.4_\n\n- [x] 18. Enhance logging and debugging capabilities\n  - Add structured logging with context information\n  - Implement debug mode with verbose output\n  - Add performance monitoring and metrics collection\n  - _Requirements: 6.1, 6.2, 6.5_\n\n- [x] 19. Add comprehensive error recovery mechanisms\n  - Implement graceful degradation for partial failures\n  - Add automatic fallback value generation\n  - Create error recovery workflows for common issues\n  - _Requirements: 6.1, 6.3, 8.5_\n\n### Phase 7: Additional Issues from Known Issues List\n\n- [x] 20. Fix BISAC field validation (ISSUE-015)\n  - Ensure each BISAC field contains exactly one BISAC category\n  - Validate BISAC Category, BISAC Category 2, and BISAC Category 3 fields\n  - Add validation to prevent multiple categories in single field\n  - _Requirements: 3.1, 3.4_\n\n- [x] 21. Implement price calculations (ISSUE-016, ISSUE-017, ISSUE-018, ISSUE-019)\n  - Add missing price calculation logic\n  - Ensure all prices are two-decimal floats without currency symbols\n  - Calculate EU, CA, and AU prices with exchange rates and fees\n  - Replicate US List Price to specified international fields\n  - _Requirements: 7.1, 7.2, 7.5_\n\n- [x] 22. Fix calculated spine width override (ISSUE-012)\n  - Implement spine width calculation based on page count and paper type\n  - Override any configured spine width with calculated value\n  - Add validation to ensure spine calculation is always used\n  - _Requirements: 7.1, 7.3_\n\n- [x] 23. Fix contributor role validation (ISSUE-013)\n  - Ensure blank contributor names have blank contributor roles\n  - Validate contributor roles match actual book contributors\n  - Add validation for contributor role codes\n  - _Requirements: 2.5, 3.1, 7.2_\n\n- [x] 24. Implement file path generation (ISSUE-014)\n  - Generate correct interior and cover file paths\n  - Match file paths to actual deliverable artifact names\n  - Add validation for file path accuracy\n  - _Requirements: 7.1, 7.5_\n\n- [x] 25. Set reserved fields to blank (ISSUE-020)\n  - Ensure all reserved and unused fields are consistently blank\n  - Add validation to prevent accidental population of reserved fields\n  - Document which fields should always remain blank\n  - _Requirements: 3.1, 7.1_\n\n### Phase 8: Testing and Quality Assurance\n\n- [ ] 26. Create comprehensive test suite for bug fixes\n  - Add regression tests for all fixed bugs\n  - Implement integration tests for complete LSI generation\n  - Add performance tests for batch processing\n  - _Requirements: All requirements - testing coverage_\n\n- [ ] 27. Validate fixes against real LSI submissions\n  - Test generated CSV files with IngramSpark validation tools\n  - Compare output with successful historical submissions\n  - Validate field completeness and accuracy\n  - _Requirements: 3.1, 3.2, 3.3, 3.4_\n\n- [ ] 28. Performance testing and optimization\n  - Benchmark processing times for single books and batches\n  - Test memory usage under various load conditions\n  - Validate system behavior under resource constraints\n  - _Requirements: 8.1, 8.2, 8.3, 8.4, 8.5_\n\n### Phase 9: Documentation and Deployment\n\n- [ ] 29. Update documentation for fixed components\n  - Document all bug fixes and their solutions\n  - Update configuration documentation\n  - Create troubleshooting guide for common issues\n  - _Requirements: Supporting documentation_\n\n- [ ] 30. Create deployment and monitoring tools\n  - Add health check endpoints for system monitoring\n  - Create deployment scripts with validation\n  - Add monitoring dashboards for production use\n  - _Requirements: Production readiness_\n\n- [ ] 31. Final integration testing and validation\n  - Run complete end-to-end testing with real data\n  - Validate all bug fixes work together correctly\n  - Perform final performance and reliability testing\n  - _Requirements: All requirements - final validation_\n\n## Task Dependencies\n\n### Critical Path\n1  2  3  4  5  8  11  14  20  21  25\n\n### Parallel Development Tracks\n- **Prompt Fixes**: Tasks 1, 2, 3\n- **Field Completion**: Tasks 4, 5, 6, 7\n- **Configuration**: Tasks 8, 9, 10\n- **Batch Processing**: Tasks 11, 12, 13\n- **Validation**: Tasks 14, 15, 16\n- **Error Handling**: Tasks 17, 18, 19\n- **Testing**: Tasks 20, 21, 22\n- **Deployment**: Tasks 23, 24, 25\n\n## Success Criteria\n\n### Phase Completion Criteria\n- **Phase 1**:  All JSON parsing errors resolved, prompts return valid JSON\n- **Phase 2**:  Field completion accuracy >90%, validation catches all format issues\n- **Phase 3**:  Configuration loading works reliably, tranche settings apply correctly\n- **Phase 4**:  Batch processing handles 100+ books without failures\n- **Phase 5**:  Validation system implemented, field mapping enhanced\n- **Phase 6**:  System recovers gracefully from all error conditions\n- **Phase 7**:  Additional known issues addressed (pricing, BISAC, contributor roles)\n- **Phase 8**: Comprehensive test coverage with no regression failures\n- **Phase 9**: Production-ready deployment with monitoring\n\n### Overall Success Metrics\n- LSI CSV generation success rate >95% \n- Field completion accuracy >90% \n- Batch processing reliability >99% \n- Zero critical bugs in production \n- Processing time <30 seconds per book \n- Memory usage stable during large batches \n\n### Current Status: 25/31 Tasks Complete (81%)\n- **Critical Issues**:  All resolved\n- **High Priority Issues**:  Most resolved, some new issues identified\n- **Medium Priority Issues**:  In progress\n- **System Reliability**:  Significantly improved",
      "size": 10407,
      "modified": "2025-07-28T22:26:09.548904",
      "spec_directory": "lsi-csv-bug-fixes",
      "directory_modified": "2025-07-28T22:26:09.548904"
    },
    {
      "file": "tasks.md",
      "content": "# LSI Pricing System Fix Implementation Tasks\n\n## Task Overview\nFix critical pricing regressions in LSI CSV generation system to ensure Lightning Source compatibility.\n\n## Implementation Tasks\n\n- [x] 1. Create Currency Formatter Utility\n  - Create `src/codexes/modules/distribution/currency_formatter.py`\n  - Implement `format_decimal_price()` method to remove currency symbols\n  - Implement `extract_numeric_value()` method to parse price strings\n  - Add comprehensive unit tests for all currency sy",
      "full_content": "# LSI Pricing System Fix Implementation Tasks\n\n## Task Overview\nFix critical pricing regressions in LSI CSV generation system to ensure Lightning Source compatibility.\n\n## Implementation Tasks\n\n- [x] 1. Create Currency Formatter Utility\n  - Create `src/codexes/modules/distribution/currency_formatter.py`\n  - Implement `format_decimal_price()` method to remove currency symbols\n  - Implement `extract_numeric_value()` method to parse price strings\n  - Add comprehensive unit tests for all currency symbol removal\n  - _Requirements: 1.1, 1.2, 1.3, 1.4, 1.5_\n\n- [x] 2. Create Enhanced Pricing Strategy\n  - Create `src/codexes/modules/distribution/enhanced_pricing_strategy.py`\n  - Implement unified pricing strategy for all price fields\n  - Replace multiple pricing strategies with single optimized strategy\n  - Ensure all prices output as decimal numbers without symbols\n  - _Requirements: 1.1, 1.2, 5.4, 5.5_\n\n- [ ] 3. Implement Territorial Price Calculator\n  - Create `src/codexes/modules/distribution/territorial_price_calculator.py`\n  - Implement automatic EU price calculation from US base price\n  - Implement automatic CA price calculation from US base price\n  - Implement automatic AU price calculation from US base price\n  - Add exchange rate caching and fallback mechanisms\n  - _Requirements: 2.1, 2.2, 2.3, 2.4, 2.5_\n\n- [ ] 4. Fix Specialty Market Pricing\n  - Update pricing strategy to set GC prices equal to US prices\n  - Update pricing strategy to set USBR1 prices equal to US prices\n  - Update pricing strategy to set USDE1 prices equal to US prices\n  - Ensure all specialty market prices default to US price when not configured\n  - Format all specialty market prices as decimal numbers\n  - _Requirements: 3.1, 3.2, 3.3, 3.4, 3.5_\n\n- [ ] 5. Standardize Wholesale Discount Formatting\n  - Remove percentage symbols from wholesale discount output\n  - Ensure consistent wholesale discount application across all markets\n  - Set default wholesale discount to 40 for all markets\n  - Format wholesale discounts as integer strings without % symbol\n  - _Requirements: 4.1, 4.2, 4.3, 4.4, 4.5_\n\n- [ ] 6. Optimize Field Mapping Strategy Registration\n  - Audit current field mapping registrations to identify duplicates\n  - Eliminate duplicate strategy registrations\n  - Consolidate pricing strategies into single enhanced strategy\n  - Reduce total strategy count from 190 to approximately 119\n  - Add validation to prevent duplicate strategy registration\n  - _Requirements: 5.1, 5.2, 5.3, 5.4, 5.5_\n\n- [ ] 7. Update Price Field Mappings\n  - Update US Suggested List Price mapping to use enhanced strategy\n  - Update UK Suggested List Price mapping to use enhanced strategy\n  - Update EU Suggested List Price mapping to use enhanced strategy\n  - Update CA Suggested List Price mapping to use enhanced strategy\n  - Update AU Suggested List Price mapping to use enhanced strategy\n  - Update all specialty market price mappings\n  - _Requirements: 1.3, 1.4, 2.1, 2.2, 2.3, 3.1, 3.2, 3.3_\n\n- [ ] 8. Create Comprehensive Pricing Tests\n  - Write unit tests for currency formatter\n  - Write unit tests for territorial price calculator\n  - Write unit tests for enhanced pricing strategy\n  - Write integration tests for complete pricing pipeline\n  - Write validation tests for LSI compliance\n  - _Requirements: All requirements validation_\n\n- [ ] 9. Update Configuration System\n  - Update default pricing configuration with proper decimal formats\n  - Remove currency symbols from configuration files\n  - Add exchange rate configuration for territorial pricing\n  - Update wholesale discount configuration format\n  - _Requirements: 1.1, 1.2, 2.4, 4.2_\n\n- [ ] 10. Integration and Testing\n  - Integrate all pricing components into LSI generator\n  - Run comprehensive pipeline tests with pricing fixes\n  - Validate CSV output format compliance\n  - Verify field coverage improvement\n  - Test performance with optimized strategy count\n  - _Requirements: All requirements integration_\n\n## Success Criteria\n\n### Pricing Format Compliance\n- All price fields output as decimal numbers (e.g., \"19.95\")\n- No currency symbols in any price field\n- All wholesale discounts as integers (e.g., \"40\")\n\n### Territorial Price Coverage\n- EU, CA, AU prices automatically calculated and populated\n- All specialty market prices equal to US price when not configured\n- Consistent wholesale discounts across all markets\n\n### System Optimization\n- Field mapping strategy count reduced to ~119 (one per field)\n- No duplicate strategy registrations\n- Improved field coverage percentage\n\n### LSI Compliance\n- CSV passes Lightning Source validation\n- All required price fields properly formatted\n- No pricing-related validation errors",
      "size": 4693,
      "modified": "2025-07-29T00:53:34.598584",
      "spec_directory": "lsi-pricing-fixes",
      "directory_modified": "2025-07-29T00:53:34.598584"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Create enhanced BISAC category generator\n  - Create `BISACCategoryGenerator` class that uses LLM to generate multiple relevant categories\n  - Implement tranche config override support for primary category (e.g., PILSA  \"Self-Help / Journaling\")\n  - Add category diversity logic to prefer different top-level categories (BUS, SEL, COM, etc.)\n  - Implement category validation and formatting to return full names instead of codes\n  - Add fallback strategies for when LL",
      "full_content": "# Implementation Plan\n\n- [x] 1. Create enhanced BISAC category generator\n  - Create `BISACCategoryGenerator` class that uses LLM to generate multiple relevant categories\n  - Implement tranche config override support for primary category (e.g., PILSA  \"Self-Help / Journaling\")\n  - Add category diversity logic to prefer different top-level categories (BUS, SEL, COM, etc.)\n  - Implement category validation and formatting to return full names instead of codes\n  - Add fallback strategies for when LLM generation fails\n  - _Requirements: 1.1, 1.2, 1.3, 2.1, 2.2, 3.1, 3.2, 4.1, 4.2, 4.3, 5.1, 5.2_\n\n- [x] 2. Enhance BISAC validator for category names\n  - Extend `BISACValidator` to validate category names (not just codes)\n  - Add method to convert BISAC codes to full category names\n  - Implement category name lookup and similarity matching\n  - _Requirements: 2.1, 2.2, 2.3, 2.4_\n\n- [x] 3. Create specialized LLM prompts for BISAC generation\n  - Design prompts that analyze book metadata to suggest relevant BISAC categories\n  - Add instructions to prefer categories from different top-level categories for diversity\n  - Ensure prompts return full category names in proper format\n  - Add ranking by relevance to book content while maintaining diversity\n  - Handle cases where tranche override is already specified\n  - _Requirements: 3.1, 3.2, 3.3, 5.1, 5.2, 5.3_\n\n- [x] 4. Implement unified BISAC category mapping strategy\n  - Create `EnhancedBISACCategoryStrategy` that generates all three categories\n  - Add tranche config integration to check for category overrides\n  - Implement caching to avoid regenerating categories for each field\n  - Add proper error handling and logging for debugging\n  - _Requirements: 1.1, 1.2, 1.3, 4.1, 4.2, 4.3, 6.1, 6.2, 6.3_\n\n- [x] 5. Fix field mapping registrations\n  - Remove conflicting BISAC category field registrations in enhanced_field_mappings.py\n  - Register new enhanced strategies for all three BISAC category fields\n  - Ensure proper strategy precedence and no overrides\n  - _Requirements: 1.1, 1.4_\n\n- [ ] 6. Test BISAC category generation with live pipeline\n  - Run live pipeline test to verify all three BISAC fields are populated\n  - Verify categories are full names without codes\n  - Test tranche override functionality (e.g., PILSA books  \"Self-Help / Journaling\")\n  - Verify at least 2 categories come from different top-level categories when possible\n  - Test with different book types to ensure relevance and diversity\n  - _Requirements: 1.1, 1.2, 1.3, 2.1, 3.1, 4.1, 4.2, 4.3, 5.1, 5.2_\n\n- [ ] 7. Add comprehensive error handling and logging\n  - Implement detailed logging for category generation process\n  - Add logging for tranche override application\n  - Add logging for category diversity analysis\n  - Add fallback logging when validation fails\n  - Create monitoring for category generation quality\n  - _Requirements: 4.4, 5.4, 6.1, 6.2, 6.3, 6.4_\n\n- [ ] 8. Create unit tests for BISAC category system\n  - Test category generation with various book metadata\n  - Test tranche override functionality with different configurations\n  - Test category diversity logic with different book types\n  - Test validation against BISAC standards\n  - Test fallback strategies and error handling\n  - _Requirements: 2.1, 2.2, 2.3, 2.4, 4.1, 4.2, 4.3, 4.4, 5.1, 5.2, 5.3, 5.4, 6.1, 6.2_",
      "size": 3331,
      "modified": "2025-07-29T02:25:21.321841",
      "spec_directory": "bisac-category-fixes",
      "directory_modified": "2025-07-29T02:25:21.321841"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Create tranche override management system\n  - Implement TrancheOverrideManager class with replace/append logic\n  - Add methods to determine override precedence and field types\n  - Create unit tests for override behavior with various field types\n  - _Requirements: 1.1, 1.2, 1.3, 1.4_\n\n- [x] 2. Implement JSON metadata extraction utilities\n  - Create JSONMetadataExtractor class for thema and age data extraction\n  - Add thema subject extraction with array handling and",
      "full_content": "# Implementation Plan\n\n- [x] 1. Create tranche override management system\n  - Implement TrancheOverrideManager class with replace/append logic\n  - Add methods to determine override precedence and field types\n  - Create unit tests for override behavior with various field types\n  - _Requirements: 1.1, 1.2, 1.3, 1.4_\n\n- [x] 2. Implement JSON metadata extraction utilities\n  - Create JSONMetadataExtractor class for thema and age data extraction\n  - Add thema subject extraction with array handling and validation\n  - Implement age range extraction with integer conversion and bounds checking\n  - Write comprehensive unit tests for extraction edge cases\n  - _Requirements: 2.1, 2.2, 2.3, 2.4, 2.5, 3.1, 3.2, 3.3, 3.4, 3.5_\n\n- [x] 3. Build series-aware description processor\n  - Create SeriesDescriptionProcessor class for \"This book\" replacement logic\n  - Implement series name validation and context checking\n  - Add string replacement logic with series name interpolation\n  - Write unit tests for various description and series combinations\n  - _Requirements: 4.1, 4.2, 4.3, 4.4_\n\n- [x] 4. Create enhanced field mapping strategies\n  - Implement ThemaSubjectStrategy for mapping extracted thema codes to LSI columns\n  - Create AgeRangeStrategy for mapping min/max age values to integer columns\n  - Build SeriesAwareDescriptionStrategy integrating description processor\n  - Implement BlankIngramPricingStrategy to enforce blank values for specific pricing fields\n  - Add TrancheFilePathStrategy for file path generation from tranche templates\n  - _Requirements: 2.1, 2.2, 3.1, 3.2, 4.1, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 6.1, 6.2_\n\n- [x] 5. Integrate tranche override system with field mapping\n  - Modify existing field mapping strategies to use TrancheOverrideManager\n  - Update field mapping registry to apply overrides after LLM generation\n  - Ensure append-type fields (like annotation_boilerplate) work correctly\n  - Add integration tests for override precedence across different field types\n  - _Requirements: 1.1, 1.2, 1.3, 1.4_\n\n- [x] 6. Update tranche configuration schema and loading\n  - Extend tranche configuration to support field_overrides and append_fields\n  - Add file_path_templates and blank_fields configuration options\n  - Update configuration validation to handle new schema elements\n  - Modify tranche configuration loader to parse new fields\n  - _Requirements: 6.1, 6.2, 6.3_\n\n- [x] 7. Implement comprehensive field validation\n  - Add validation for thema subject codes against known formats\n  - Implement age range bounds checking (0-150) with error logging\n  - Create file path sanitization for LSI naming convention compliance\n  - Add validation error collection and reporting mechanisms\n  - _Requirements: 2.5, 3.3, 3.4, 6.4, 6.5_\n\n- [x] 8. Wire new strategies into LSI generation pipeline\n  - Register new field mapping strategies in the field mapping registry\n  - Update LSI CSV generator to use enhanced strategies\n  - Ensure proper order of operations (extraction  mapping  override  validation)\n  - Add logging for each correction step for debugging purposes\n  - _Requirements: All requirements integration_\n\n- [x] 9. Create comprehensive test suite\n  - Write integration tests for complete LSI generation with all corrections\n  - Test with sample book metadata containing thema, age, and series data\n  - Verify blank Ingram pricing fields in generated CSV output\n  - Test tranche override precedence with real configuration files\n  - Add performance tests to ensure no significant regression\n  - _Requirements: All requirements validation_\n\n- [ ] 10. Update existing tranche configuration files\n  - Add field_overrides for Series Name in xynapse_tranche_1.json\n  - Configure blank_fields for Ingram pricing columns\n  - Add file_path_templates for interior and cover file generation\n  - Test updated configuration with live pipeline\n  - _Requirements: 1.1, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 6.1, 6.2_",
      "size": 3925,
      "modified": "2025-07-31T18:33:35.171851",
      "spec_directory": "lsi-field-mapping-corrections",
      "directory_modified": "2025-07-31T18:33:35.171851"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Create Enhanced Configuration Management Infrastructure\n  - Create src/codexes/modules/ui/configuration_manager.py with EnhancedConfigurationManager class\n  - Implement dynamic configuration loading from configs/ directories\n  - Add configuration merging logic for multi-level inheritance (default  publisher  imprint  tranche)\n  - Create configuration validation framework with parameter-specific validators\n  - _Requirements: 1.1, 1.2, 1.3, 3.1, 3.2_\n\n- [x] 2. Im",
      "full_content": "# Implementation Plan\n\n- [x] 1. Create Enhanced Configuration Management Infrastructure\n  - Create src/codexes/modules/ui/configuration_manager.py with EnhancedConfigurationManager class\n  - Implement dynamic configuration loading from configs/ directories\n  - Add configuration merging logic for multi-level inheritance (default  publisher  imprint  tranche)\n  - Create configuration validation framework with parameter-specific validators\n  - _Requirements: 1.1, 1.2, 1.3, 3.1, 3.2_\n\n- [x] 2. Implement Dynamic Configuration Discovery\n  - Create src/codexes/modules/ui/dynamic_config_loader.py with DynamicConfigurationLoader class\n  - Implement directory scanning for publishers, imprints, and tranches\n  - Add JSON configuration file validation and error handling\n  - Create fallback mechanisms for missing or invalid configurations\n  - _Requirements: 3.1, 3.2, 3.3, 3.4_\n\n- [x] 3. Create Parameter Organization System\n  - Create src/codexes/modules/ui/parameter_groups.py with ParameterGroupManager class\n  - Define parameter groups and their metadata (Core Settings, LSI Configuration, etc.)\n  - Implement parameter dependency tracking and validation\n  - Add parameter help text and validation rule definitions\n  - _Requirements: 4.1, 4.2, 4.3, 4.4_\n\n- [x] 4. Build Configuration Validation Framework\n  - Create src/codexes/modules/ui/config_validator.py with ConfigurationValidator class\n  - Implement real-time parameter validation with specific error messages\n  - Add LSI-specific validation rules (territorial pricing, physical specs, etc.)\n  - Create cross-parameter dependency validation\n  - _Requirements: 5.1, 5.2, 5.3, 5.4_\n\n- [x] 5. Create Enhanced UI Components\n  - Create src/codexes/modules/ui/streamlit_components.py with ConfigurationUI class\n  - Implement configuration selection dropdowns with dynamic loading\n  - Create expandable parameter group widgets with display modes (Simple/Advanced/Expert)\n  - Add parameter input widgets with real-time validation feedback\n  - _Requirements: 4.1, 4.2, 8.1, 8.2_\n\n- [x] 6. Implement Complete Configuration Preview System\n  - Add mandatory configuration preview section to UI components\n  - Create JSON viewer for final merged configuration display\n  - Implement command-line preview showing exact parameters being passed to pipeline\n  - Add configuration statistics and verification hash display\n  - _Requirements: 5.3, 10.1, 10.4_\n\n- [x] 7. Build Command Builder and Serialization\n  - Create src/codexes/modules/ui/command_builder.py with CommandBuilder class\n  - Implement conversion from UI configuration to command-line arguments\n  - Add complex parameter serialization for nested configurations\n  - Create temporary file management for uploaded configurations\n  - _Requirements: 7.1, 7.2, 7.3_\n\n- [x] 8. Enhance Book Pipeline Page with Multi-Level Configuration\n  - Update src/codexes/pages/10_Book_Pipeline.py to use new configuration system\n  - Add publisher/imprint/tranche selection dropdowns at top of page\n  - Implement configuration inheritance display and override indicators\n  - Add display mode selector (Simple/Advanced/Expert) for parameter visibility\n  - _Requirements: 1.1, 1.2, 1.4, 8.1_\n\n- [x] 9. Implement LSI Configuration UI Components\n  - Add comprehensive LSI parameter sections to Book Pipeline page\n  - Create territorial pricing table with multi-currency support\n  - Implement physical specifications form with validation\n  - Add field overrides editor with key-value pairs\n  - _Requirements: 2.1, 2.2, 2.3, 2.4_\n\n- [x] 10. Add LLM and AI Configuration Interface\n  - Create LLM configuration section with model selection and parameters\n  - Implement retry configuration with exponential backoff settings\n  - Add field completion rules editor for AI-powered metadata generation\n  - Create monitoring settings interface for LLM usage tracking\n  - _Requirements: 10.1, 10.2, 10.3_\n\n- [x] 11. Build Debug and Monitoring Dashboard\n  - Create debug settings section with comprehensive logging options\n  - Implement validation rule editor for custom validation logic\n  - Add performance monitoring toggles and real-time metrics display\n  - Create field mapping strategy configuration interface\n  - _Requirements: 10.2, 10.4_\n\n- [x] 12. Implement Configuration File Management\n  - Add configuration file upload functionality with validation\n  - Create configuration template download for each type (publisher/imprint/tranche)\n  - Implement configuration export and import capabilities\n  - Add configuration comparison and diff viewing\n  - _Requirements: 6.1, 6.2, 6.3, 6.4_\n\n- [x] 13. Add Pre-Submission Validation and Inspection\n  - Create comprehensive validation status grid showing all parameter checks\n  - Implement parameter dependency tree visualization\n  - Add LSI compliance checker with detailed compliance report\n  - Create execution readiness indicator with go/no-go status\n  - _Requirements: 5.1, 5.2, 5.4_\n\n- [x] 14. Implement Configuration History and Audit Trail\n  - Create configuration snapshot system for audit purposes\n  - Add configuration history viewer with timestamp and change tracking\n  - Implement configuration comparison between different executions\n  - Create audit log generation for compliance and troubleshooting\n  - _Requirements: 7.4, 9.1, 9.2, 9.3, 9.4_\n\n- [x] 15. Add Advanced User Experience Features\n  - Implement responsive design for different screen sizes\n  - Add search and filter capabilities for parameters\n  - Create batch configuration support for multiple books\n  - Add keyboard shortcuts and accessibility features\n  - _Requirements: 8.1, 8.2, 8.3, 8.4_\n\n- [x] 16. Create Configuration Management Page\n  - Create new Streamlit page src/codexes/pages/Configuration_Management.py\n  - Implement configuration file browser with tree view\n  - Add configuration editor with JSON syntax highlighting and validation\n  - Create configuration template management system\n  - _Requirements: 6.1, 6.2, 6.3, 6.4_\n\n- [x] 17. Integrate Enhanced UI with Pipeline Execution\n  - Update pipeline execution logic to use new configuration system\n  - Implement configuration serialization for command-line argument generation\n  - Add execution monitoring with real-time status updates\n  - Create post-execution configuration archival and reporting\n  - _Requirements: 7.1, 7.2, 7.3, 7.4_\n\n- [x] 18. Add Comprehensive Testing and Validation\n  - Create unit tests for all configuration management components\n  - Implement integration tests for UI components and pipeline integration\n  - Add end-to-end tests for complete configuration workflows\n  - Create performance tests for large configuration handling\n  - _Requirements: All requirements validation_\n\n- [x] 19. Create Documentation and Help System\n  - Add inline help text and tooltips for all parameters\n  - Create configuration guide documentation\n  - Implement contextual help system within the UI\n  - Add troubleshooting guide for common configuration issues\n  - _Requirements: 4.4, 8.4_\n\n- [x] 20. Final Integration and Polish\n  - Integrate all components into cohesive user experience\n  - Add error handling and graceful degradation for edge cases\n  - Implement performance optimizations for large configurations\n  - Create final validation and testing of complete system\n  - _Requirements: All requirements final validation_",
      "size": 7299,
      "modified": "2025-08-02T15:42:18.959595",
      "spec_directory": "streamlit-ui-config-enhancement",
      "directory_modified": "2025-08-02T15:42:18.959595"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Remove problematic button from ConfigurationUI\n  - Remove `st.button(\"Load Config\")` from `render_configuration_selector` method\n  - Update column layout to remove button column\n  - Test that form no longer throws Streamlit API exception\n  - _Requirements: 3.1, 3.2_\n\n- [x] 2. Implement automatic configuration loading detection\n  - Add `_has_selection_changed()` method to detect selection changes\n  - Compare current selections with session state values\n  - Implemen",
      "full_content": "# Implementation Plan\n\n- [x] 1. Remove problematic button from ConfigurationUI\n  - Remove `st.button(\"Load Config\")` from `render_configuration_selector` method\n  - Update column layout to remove button column\n  - Test that form no longer throws Streamlit API exception\n  - _Requirements: 3.1, 3.2_\n\n- [x] 2. Implement automatic configuration loading detection\n  - Add `_has_selection_changed()` method to detect selection changes\n  - Compare current selections with session state values\n  - Implement change detection logic for publisher/imprint/tranche\n  - _Requirements: 1.1, 2.1_\n\n- [x] 3. Add automatic configuration loading logic\n  - Modify `render_configuration_selector()` to trigger loading on changes\n  - Call `_load_configuration()` automatically when selections change\n  - Update session state with new selections immediately\n  - _Requirements: 1.1, 1.2_\n\n- [x] 4. Enhance loading feedback and error handling\n  - Add loading state management to session state\n  - Implement `_show_loading_feedback()` method for visual indicators\n  - Enhance `_load_configuration()` with better error handling\n  - Add success/error messages for configuration loading\n  - _Requirements: 1.3, 4.1, 4.2_\n\n- [x] 5. Implement configuration change preservation\n  - Add logic to preserve manual parameter overrides during config changes\n  - Implement smart merging of existing form data with new config\n  - Add validation to ensure parameter compatibility\n  - _Requirements: 2.3_\n\n- [x] 6. Add real-time validation feedback\n  - Integrate automatic validation when configurations load\n  - Display validation status in configuration selector\n  - Show validation warnings for invalid configurations\n  - _Requirements: 2.2, 4.4_\n\n- [x] 7. Test form compliance and functionality\n  - Verify no `st.button()` calls remain in form context\n  - Test that form submits correctly with all button types\n  - Verify automatic loading works in both form and non-form contexts\n  - Test configuration switching and error handling\n  - _Requirements: 3.3, 3.4_\n\n- [x] 8. Add performance optimizations\n  - Implement configuration caching to avoid redundant loads\n  - Add debouncing for rapid selection changes\n  - Optimize validation calls to reduce overhead\n  - Test loading performance with various configuration sizes\n  - _Requirements: 4.3_",
      "size": 2310,
      "modified": "2025-08-02T20:13:10.265817",
      "spec_directory": "streamlit-form-button-fix",
      "directory_modified": "2025-08-02T20:13:10.265817"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Create core infrastructure classes for controlled state management\n  - Create DropdownManager class with publisher change handling and debouncing\n  - Create ValidationManager class with safe validation and loop prevention\n  - Create StateManager class with atomic session state updates\n  - _Requirements: 1.3, 2.3, 3.1, 4.1_\n\n- [x] 2. Implement session state structure updates\n  - Add control flags for dropdown updates and validation state\n  - Add caching structures ",
      "full_content": "# Implementation Plan\n\n- [x] 1. Create core infrastructure classes for controlled state management\n  - Create DropdownManager class with publisher change handling and debouncing\n  - Create ValidationManager class with safe validation and loop prevention\n  - Create StateManager class with atomic session state updates\n  - _Requirements: 1.3, 2.3, 3.1, 4.1_\n\n- [x] 2. Implement session state structure updates\n  - Add control flags for dropdown updates and validation state\n  - Add caching structures for publisher-imprint and imprint-tranche mappings\n  - Add timestamp tracking for debouncing and update control\n  - _Requirements: 3.5, 4.2, 4.5_\n\n- [x] 3. Fix imprint dropdown refresh without rerun loops\n  - Replace direct st.rerun() calls in publisher change detection\n  - Implement controlled dropdown refresh using session state flags\n  - Add publisher-to-imprints mapping cache with automatic refresh\n  - _Requirements: 1.1, 1.2, 1.3, 1.4_\n\n- [ ] 4. Implement debouncing mechanisms for state updates\n  - Add timestamp-based debouncing for rapid successive changes\n  - Implement update event queuing to prevent cascading refreshes\n  - Add controlled refresh triggers that respect debounce timing\n  - _Requirements: 3.1, 3.2, 3.5_\n\n- [x] 5. Fix validation button runaway loop\n  - Implement validation state flags to prevent multiple simultaneous validations\n  - Create safe validation method that doesn't trigger st.rerun()\n  - Add validation result display that maintains page stability\n  - _Requirements: 2.1, 2.2, 2.3, 2.4_\n\n- [ ] 6. Create atomic session state management\n  - Implement atomic_update method for multiple session state changes\n  - Add state consistency validation and correction mechanisms\n  - Create selection preservation logic for valid dependent selections\n  - _Requirements: 4.1, 4.2, 4.3, 4.4_\n\n- [x] 7. Update ConfigurationUI to use new managers\n  - Integrate DropdownManager into render_configuration_selector method\n  - Replace existing validation logic with ValidationManager\n  - Update all session state modifications to use StateManager\n  - _Requirements: 1.5, 2.5, 3.3, 3.4_\n\n- [ ] 8. Update Book Pipeline page to prevent validation loops\n  - Modify validation button handler to use ValidationManager\n  - Add validation state protection in form submission logic\n  - Implement stable validation result display without reruns\n  - _Requirements: 2.1, 2.2, 2.3_\n\n- [ ] 9. Add comprehensive error handling and recovery\n  - Implement fallback mechanisms for corrupted session state\n  - Add error recovery for failed dropdown refreshes\n  - Create robust validation error handling without UI instability\n  - _Requirements: 4.5, 2.4, 3.2_\n\n- [ ] 10. Create unit tests for all new manager classes\n  - Write tests for DropdownManager publisher change handling\n  - Write tests for ValidationManager loop prevention\n  - Write tests for StateManager atomic updates and consistency\n  - _Requirements: 1.1, 1.2, 2.1, 2.2, 4.1_\n\n- [ ] 11. Create integration tests for UI interaction flows\n  - Test complete publisher  imprint  tranche selection workflow\n  - Test validation button behavior under various conditions\n  - Test configuration loading with dropdown dependency updates\n  - _Requirements: 1.1, 1.4, 2.1, 2.3_\n\n- [ ] 12. Implement performance optimizations and monitoring\n  - Add caching for publisher-imprint mappings with LRU eviction\n  - Implement performance monitoring for dropdown refresh times\n  - Add logging for rerun loop detection and prevention\n  - _Requirements: 3.5, 4.2, 4.3_",
      "size": 3518,
      "modified": "2025-08-02T22:40:42.397820",
      "spec_directory": "streamlit-ui-runaway-fixes",
      "directory_modified": "2025-08-02T22:40:42.397820"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Implement core configuration synchronization mechanism\n  - Create ConfigurationSynchronizer class with session state management\n  - Implement sync_config_to_form method to merge configuration values into form defaults\n  - Add tracking for user overrides to distinguish between config-derived and user-entered values\n  - _Requirements: 1.1, 1.2, 5.1, 5.2, 5.3_\n\n- [x] 2. Enhance Book Pipeline page with configuration synchronization\n  - Modify Book Pipeline page to use",
      "full_content": "# Implementation Plan\n\n- [x] 1. Implement core configuration synchronization mechanism\n  - Create ConfigurationSynchronizer class with session state management\n  - Implement sync_config_to_form method to merge configuration values into form defaults\n  - Add tracking for user overrides to distinguish between config-derived and user-entered values\n  - _Requirements: 1.1, 1.2, 5.1, 5.2, 5.3_\n\n- [x] 2. Enhance Book Pipeline page with configuration synchronization\n  - Modify Book Pipeline page to use ConfigurationSynchronizer for form data building\n  - Update form data initialization to pull defaults from configuration selection\n  - Implement real-time synchronization when configuration selection changes\n  - _Requirements: 1.1, 1.2, 1.3, 2.2_\n\n- [x] 3. Create configuration-aware validation system\n  - Implement ConfigurationAwareValidator that considers both form and configuration values\n  - Update validation logic to treat configuration-derived values as valid for required fields\n  - Enhance validation error messages to provide context about configuration vs form values\n  - _Requirements: 3.1, 3.2, 3.3, 3.4, 3.5_\n\n- [x] 4. Add visual indicators for synchronized fields\n  - Implement SynchronizedFormRenderer to show which fields are auto-populated from configuration\n  - Add helper text and tooltips indicating when values come from configuration selection\n  - Create visual distinction between configuration-derived and user-entered values\n  - _Requirements: 2.4, 4.1, 4.2, 4.4_\n\n- [x] 5. Implement user override functionality\n  - Add ability for users to manually override configuration-derived values in core settings\n  - Track override state and provide visual feedback when values are overridden\n  - Implement clear indication when core settings values are independent of configuration\n  - _Requirements: 1.5, 4.3, 5.4_\n\n- [x] 6. Add comprehensive error handling and fallbacks\n  - Implement safe synchronization with graceful degradation when sync fails\n  - Add error handling for malformed configuration data or missing values\n  - Ensure backward compatibility with existing workflows when configuration is not selected\n  - _Requirements: 2.1, 2.3, 5.5_\n\n- [x] 7. Create integration tests for configuration synchronization\n  - Write tests for end-to-end synchronization workflow from configuration selection to form validation\n  - Test user override scenarios and state management across page interactions\n  - Verify validation behavior with various combinations of configuration and form values\n  - _Requirements: All requirements validation_\n\n- [x] 8. Polish user experience and add real-time feedback\n  - Implement immediate visual feedback when configuration selection changes affect core settings\n  - Add smooth transitions and animations for synchronized field updates\n  - Ensure consistent behavior across different browsers and screen sizes\n  - _Requirements: 4.2, 4.5_",
      "size": 2896,
      "modified": "2025-08-02T23:36:25.671807",
      "spec_directory": "config-sync-fix",
      "directory_modified": "2025-08-02T23:36:25.671807"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Fix bibliography formatting with memoir citation fields and hanging indent\n  - Implement BibliographyFormatter class with memoir citation field integration\n  - Add 0.15 hanging indent formatting for second and subsequent lines\n  - Create LaTeX template modifications for proper bibliography display\n  - Test bibliography formatting with multiple entries and long citations\n  - _Requirements: 1.1, 1.2, 1.3, 1.4, 1.5_\n\n- [x] 2. Implement ISBN lookup caching system to p",
      "full_content": "# Implementation Plan\n\n- [x] 1. Fix bibliography formatting with memoir citation fields and hanging indent\n  - Implement BibliographyFormatter class with memoir citation field integration\n  - Add 0.15 hanging indent formatting for second and subsequent lines\n  - Create LaTeX template modifications for proper bibliography display\n  - Test bibliography formatting with multiple entries and long citations\n  - _Requirements: 1.1, 1.2, 1.3, 1.4, 1.5_\n\n- [x] 2. Implement ISBN lookup caching system to prevent duplicate API calls\n  - Create ISBNLookupCache class with persistent JSON storage\n  - Add document scanning tracking to avoid duplicate ISBN scans\n  - Implement cache validation and expiration mechanisms\n  - Add error handling for cache corruption and API failures\n  - _Requirements: 2.1, 2.2, 2.3, 2.4, 2.5_\n\n- [x] 3. Fix reporting accuracy for prompt success and quote retrieval statistics\n  - Implement AccurateReportingSystem to track real-time statistics\n  - Fix quote count reporting to show actual retrieved counts (not 0)\n  - Add detailed prompt execution tracking with success/failure rates\n  - Align reported statistics with actual pipeline execution results\n  - _Requirements: 3.1, 3.2, 3.3, 3.4, 3.5_\n\n- [x] 4. Enhance error handling for quote verification and field completion failures\n  - Create EnhancedErrorHandler with detailed logging and context\n  - Fix quote verification error handling for invalid verifier model responses\n  - Add graceful fallbacks for field completion method errors\n  - Implement comprehensive error logging with debugging context\n  - _Requirements: 4.1, 4.2, 4.3, 4.4, 4.5_\n\n- [x] 5. Implement Book Pipeline UI tranche configuration dropdown\n  - Create TrancheConfigUIManager to load and display available tranche configurations\n  - Update Book Pipeline UI to populate tranche dropdown with available options\n  - Add tranche selection validation and configuration loading\n  - Implement dropdown refresh functionality when configurations change\n  - Test tranche selection and configuration passing to pipeline\n  - _Requirements: 5.1, 5.2, 5.3, 5.4, 5.5_\n\n- [x] 6. Implement typography enhancements for professional formatting\n  - Create TypographyManager for consistent font and layout management\n  - Format mnemonics pages with Adobe Caslon font matching quotations style\n  - Add Apple Myungjo font support for Korean characters on title pages\n  - Implement instruction placement on every 8th recto page bottom\n  - Adjust chapter heading leading to approximately 36 points\n  - Ensure LaTeX commands are properly escaped and not visible in final PDF\n  - _Requirements: 6.1, 6.2, 6.3, 6.4, 6.5, 6.6_\n\n- [x] 7. Implement 2-column glossary layout with Korean/English term stacking\n  - Create GlossaryLayoutManager for proper column formatting\n  - Ensure glossary fits within page text area constraints\n  - Stack Korean and English terms vertically in left-hand cells\n  - Distribute glossary entries evenly across both columns\n  - _Requirements: 7.1, 7.2, 7.3, 7.4, 7.5_\n\n- [x] 8. Enhance Publisher's Note generation with structured formatting\n  - Create PublishersNoteGenerator with 3-paragraph structure requirement\n  - Implement 600-character maximum limit per paragraph validation\n  - Ensure pilsa book explanation is included exactly once\n  - Add current events references without date-specific content\n  - Focus content on motivating both publishers and readers\n  - _Requirements: 8.1, 8.2, 8.3, 8.4, 8.5_\n\n- [x] 9. Fix mnemonics JSON generation to include required keys\n  - Create MnemonicsJSONProcessor with proper JSON structure validation\n  - Update mnemonics prompt to require 'mnemonics_data' key in response\n  - Implement validation for expected JSON keys before processing\n  - Add error handling for missing keys with fallback behavior\n  - Test mnemonics.tex creation with validated JSON data\n  - _Requirements: 9.1, 9.2, 9.3, 9.4, 9.5_\n\n- [x] 10. Implement pilsa book identification in all content types\n  - Create PilsaBookContentProcessor to add pilsa identification\n  - Update back cover text generation to include pilsa book description\n  - Ensure all descriptive content mentions 90 quotations and journaling pages\n  - Add pilsa book designation to marketing copy and metadata\n  - Validate pilsa description consistency across all content types\n  - _Requirements: 10.1, 10.2, 10.3, 10.4, 10.5_\n\n- [x] 11. Implement specific BISAC category generation\n  - Create BISACCategoryAnalyzer to analyze book content for relevant categories\n  - Replace generic categories like \"Business>General\" with specific technical categories\n  - Implement content analysis to determine appropriate categories like \"Science > Planetary Exploration\"\n  - Validate category relevance to actual book content\n  - Test category generation with various book topics\n  - _Requirements: 11.1, 11.2, 11.3, 11.4, 11.5_\n\n- [x] 12. Create hierarchical writing style configuration system\n  - Implement WritingStyleManager with tranche/imprint/publisher hierarchy\n  - Create writing_style.json file format and validation\n  - Build prompt construction system for multiple text values\n  - Add style configuration loading with proper precedence rules\n  - Test style configuration application across different levels\n  - _Requirements: 12.1, 12.2, 12.3, 12.4, 12.5_\n\n- [x] 13. Optimize quote assembly to prevent excessive author repetition\n  - Create QuoteAssemblyOptimizer to limit consecutive author quotes\n  - Implement quote reordering algorithm to improve author distribution\n  - Ensure no author appears more than 3 times consecutively\n  - Maintain thematic coherence while improving variety\n  - Test quote assembly with various author distributions\n  - _Requirements: 13.1, 13.2, 13.3, 13.4, 13.5_\n\n- [x] 14. Implement Notes heading for blank last verso pages\n  - Create LastVersoPageManager to detect blank last verso pages\n  - Add \"Notes\" chapter heading to blank last verso pages\n  - Ensure Notes heading follows same formatting as other chapter headings\n  - Validate Notes page positioning as final verso page\n  - Test Notes page creation with various book structures\n  - _Requirements: 14.1, 14.2, 14.3, 14.4, 14.5_\n\n- [x] 15. Implement ISBN barcode generation with UPC-A format\n  - Create ISBNBarcodeGenerator for automatic barcode creation\n  - Generate UPC-A barcodes with ISBN-13 and bar-code-reader numerals\n  - Integrate barcode positioning into back cover design\n  - Ensure barcode meets industry standards for retail scanning\n  - Test barcode generation and integration with various ISBN formats\n  - _Requirements: 15.1, 15.2, 15.3, 15.4, 15.5_\n\n- [x] 16. Fix storefront metadata to use accurate author information\n  - Create StorefrontMetadataManager to extract author from tranche configuration\n  - Ensure storefront_author_en and _ko fields use Contributor One name from tranche config\n  - Prevent model interpolation of author names\n  - Validate author consistency between LSI CSV and storefront data\n  - Add error handling for missing author data in tranche configuration\n  - _Requirements: 16.1, 16.2, 16.3, 16.4, 16.5_\n\n- [x] 17. Create comprehensive testing suite for all production fixes\n  - Write unit tests for bibliography formatting, ISBN caching, and reporting accuracy\n  - Create integration tests for typography enhancements and layout systems\n  - Test error handling scenarios and recovery mechanisms\n  - Test new UI components and configuration systems\n  - Validate all fixes work together in complete pipeline execution\n  - _Requirements: All requirements validation_\n\n- [x] 18. Update documentation and user guides for new features\n  - Document writing style configuration system usage\n  - Create troubleshooting guide for enhanced error handling\n  - Update typography and formatting guidelines\n  - Document tranche configuration UI usage\n  - Provide examples of proper bibliography and glossary formatting\n  - _Requirements: All requirements documentation_",
      "size": 7917,
      "modified": "2025-08-04T02:16:41.765212",
      "spec_directory": "book-production-fixes",
      "directory_modified": "2025-08-04T02:16:41.765212"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Create core ISBN scheduler module with data models\n  - Implement `ISBNScheduler` class with basic initialization and file handling\n  - Create `ISBNAssignment` and `ISBNBlock` dataclasses with proper type hints\n  - Implement `ISBNStatus` enumeration for assignment states\n  - Add JSON serialization/deserialization methods for data persistence\n  - _Requirements: 1.1, 1.2, 9.1, 9.2_\n\n- [x] 2. Implement ISBN block management functionality\n  - Code `add_isbn_block()` me",
      "full_content": "# Implementation Plan\n\n- [x] 1. Create core ISBN scheduler module with data models\n  - Implement `ISBNScheduler` class with basic initialization and file handling\n  - Create `ISBNAssignment` and `ISBNBlock` dataclasses with proper type hints\n  - Implement `ISBNStatus` enumeration for assignment states\n  - Add JSON serialization/deserialization methods for data persistence\n  - _Requirements: 1.1, 1.2, 9.1, 9.2_\n\n- [x] 2. Implement ISBN block management functionality\n  - Code `add_isbn_block()` method with validation and conflict detection\n  - Implement block utilization tracking and statistics calculation\n  - Add methods for retrieving and displaying block information\n  - Create ISBN formatting utilities with check digit calculation\n  - _Requirements: 1.1, 1.2, 1.3, 1.4, 11.2_\n\n- [x] 3. Develop ISBN assignment scheduling system\n  - Implement `schedule_isbn_assignment()` method with automatic ISBN selection\n  - Add logic to find next available ISBN from appropriate blocks\n  - Create assignment validation and business rule enforcement\n  - Implement priority-based assignment handling\n  - _Requirements: 2.1, 2.2, 2.3, 2.4, 2.5_\n\n- [x] 4. Create assignment management and status operations\n  - Implement `assign_isbn_now()` method for immediate assignment\n  - Add `reserve_isbn()` functionality with reason tracking\n  - Create `update_assignment()` method with field validation\n  - Implement status transition logic and audit trail maintenance\n  - _Requirements: 4.1, 4.2, 4.3, 4.4, 5.1, 5.2, 5.3, 5.4, 6.1, 6.2, 6.3, 6.4_\n\n- [x] 5. Build assignment querying and filtering system\n  - Implement `get_scheduled_assignments()` with date range filtering\n  - Add `get_assignments_by_status()` for status-based filtering\n  - Create `get_upcoming_assignments()` for time-based queries\n  - Implement search functionality across title, ISBN, and book ID fields\n  - _Requirements: 3.1, 3.2, 3.3, 3.4_\n\n- [x] 6. Develop comprehensive reporting engine\n  - Implement `get_isbn_availability_report()` with detailed statistics\n  - Create block utilization analysis and trending\n  - Add assignment analytics by status, date, and imprint\n  - Implement export functionality for JSON and CSV formats\n  - _Requirements: 7.1, 7.2, 7.3, 7.4, 7.5_\n\n- [x] 7. Create bulk operations and CSV processing\n  - Implement `bulk_schedule_from_csv()` with validation and error handling\n  - Add bulk status update operations for scheduled assignments\n  - Create progress tracking and detailed error reporting for bulk operations\n  - Implement CSV format validation and data sanitization\n  - _Requirements: 8.1, 8.2, 8.3, 8.4, 8.5_\n\n- [ ] 8. Build comprehensive error handling and validation\n  - Create custom exception hierarchy for ISBN scheduler operations\n  - Implement input validation for all public methods\n  - Add data integrity checks and corruption detection\n  - Create graceful error recovery mechanisms with user feedback\n  - _Requirements: 11.1, 11.2, 11.3, 11.4_\n\n- [ ] 9. Implement data persistence and session management\n  - Create atomic file operations with backup and rollback capability\n  - Implement automatic data loading and saving with error handling\n  - Add data migration support for future schema changes\n  - Create file locking mechanisms to prevent concurrent access issues\n  - _Requirements: 9.1, 9.2, 9.3, 9.4_\n\n- [x] 10. Develop Streamlit user interface components\n  - Create main dashboard page with metrics and upcoming assignments display\n  - Implement assignment scheduling form with validation and user feedback\n  - Build assignment management interface with filtering and search capabilities\n  - Add ISBN block management interface with creation and monitoring features\n  - _Requirements: 3.1, 3.4, 3.5_\n\n- [ ] 11. Build Streamlit reporting and analytics interface\n  - Implement comprehensive reports page with visual charts and statistics\n  - Create export functionality for reports and assignment data\n  - Add interactive filtering and date range selection for reports\n  - Implement real-time data updates and session state management\n  - _Requirements: 7.1, 7.2, 7.3, 7.4, 7.5_\n\n- [ ] 12. Create Streamlit bulk operations interface\n  - Implement CSV upload interface with format validation and preview\n  - Add bulk assignment processing with progress tracking\n  - Create bulk status update operations with confirmation dialogs\n  - Implement error reporting and recovery options for failed bulk operations\n  - _Requirements: 8.1, 8.2, 8.3, 8.4, 8.5_\n\n- [x] 13. Develop command-line interface tool\n  - Create main CLI parser with subcommands for all major operations\n  - Implement `add-block` command with parameter validation and confirmation\n  - Add `schedule` command for individual assignment creation\n  - Create `list` command with filtering options and multiple output formats\n  - _Requirements: 10.1, 10.2, 10.3, 10.4_\n\n- [ ] 14. Implement CLI assignment management commands\n  - Add `assign` command for immediate ISBN assignment\n  - Create `reserve` command with reason requirement and validation\n  - Implement `update` command for modifying existing assignments\n  - Add `report` command with comprehensive statistics and export options\n  - _Requirements: 10.1, 10.2, 10.3, 10.4_\n\n- [ ] 15. Create CLI bulk operations and advanced features\n  - Implement `bulk` command for CSV-based assignment scheduling\n  - Add output formatting options (table, JSON, CSV) for all commands\n  - Create comprehensive help documentation and usage examples\n  - Implement proper exit codes and error handling for automation integration\n  - _Requirements: 10.1, 10.2, 10.3, 10.4_\n\n- [x] 16. Write comprehensive unit tests for core scheduler\n  - Create test fixtures for ISBN blocks, assignments, and various scenarios\n  - Test all public methods of `ISBNScheduler` class with edge cases\n  - Implement tests for data persistence, loading, and error recovery\n  - Add performance tests for large datasets and concurrent operations\n  - _Requirements: All core functionality requirements_\n\n- [ ] 17. Develop integration tests for user interfaces\n  - Create tests for Streamlit interface components and user workflows\n  - Test CLI tool with various command combinations and error scenarios\n  - Implement end-to-end tests for complete assignment lifecycles\n  - Add tests for bulk operations and data export/import functionality\n  - _Requirements: All interface and bulk operation requirements_\n\n- [ ] 18. Create comprehensive test data and validation scenarios\n  - Generate test datasets with various ISBN blocks and assignment patterns\n  - Create tests for data corruption scenarios and recovery mechanisms\n  - Implement validation tests for all input formats and edge cases\n  - Add stress tests for system limits and performance boundaries\n  - _Requirements: 11.1, 11.2, 11.3, 11.4_\n\n- [ ] 19. Implement system integration and configuration\n  - Integrate ISBN scheduler with existing Codexes Factory configuration system\n  - Add logging integration with existing infrastructure and monitoring\n  - Create configuration options for file paths, backup settings, and behavior\n  - Implement integration points with book pipeline and metadata systems\n  - _Requirements: 9.1, 9.2, 9.3, 9.4_\n\n- [ ] 20. Finalize documentation and deployment preparation\n  - Create comprehensive user documentation for all interfaces\n  - Write developer documentation for API integration and extension\n  - Implement deployment scripts and configuration templates\n  - Create monitoring and maintenance procedures for production use\n  - _Requirements: All requirements for production readiness_",
      "size": 7564,
      "modified": "2025-08-05T02:05:32.467808",
      "spec_directory": "isbn-schedule-assignment",
      "directory_modified": "2025-08-05T02:05:32.467808"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n- [x] 1. Fix default configuration values in Book Pipeline UI\n  - Update default values for lightning_source_account, language_code, and field_reports\n  - Ensure defaults are visible in the UI form fields\n  - Add validation for required fields\n  - _Requirements: 1.1, 1.2, 1.3, 1.4_\n\n- [x] 2. Fix LLM configuration propagation in BackmatterProcessor\n  - Update BackmatterProcessor to use provided LLM configuration instead of hardcoded values\n  - Change default model from gemi",
      "full_content": "# Implementation Plan\n\n- [x] 1. Fix default configuration values in Book Pipeline UI\n  - Update default values for lightning_source_account, language_code, and field_reports\n  - Ensure defaults are visible in the UI form fields\n  - Add validation for required fields\n  - _Requirements: 1.1, 1.2, 1.3, 1.4_\n\n- [x] 2. Fix LLM configuration propagation in BackmatterProcessor\n  - Update BackmatterProcessor to use provided LLM configuration instead of hardcoded values\n  - Change default model from gemini-2.5-flash to gpt-4o-mini\n  - Add logging to show which LLM model is being used\n  - _Requirements: 2.1, 2.2, 2.5, 2.6_\n\n- [x] 3. Update XynapseTracesPrepress to pass LLM configuration\n  - Modify XynapseTracesPrepress to extract LLM config from pipeline configuration\n  - Pass LLM configuration to BackmatterProcessor during initialization\n  - Ensure configuration flows properly from pipeline to processors\n  - _Requirements: 2.1, 2.2_\n\n- [x] 4. Fix foreword generation LLM configuration\n  - Update foreword generation to use pipeline LLM configuration instead of hardcoded gemini-2.5-pro\n  - Ensure foreword generation receives LLM config from prepress workflow\n  - Add logging for foreword generation model usage\n  - _Requirements: 2.3, 2.6, 4.1_\n\n- [x] 5. Fix publisher's note generation LLM configuration\n  - Update PublishersNoteGenerator to accept and use pipeline LLM configuration\n  - Ensure publisher's note generation uses configured model instead of hardcoded values\n  - Add logging for publisher's note generation model usage\n  - _Requirements: 2.4, 2.6, 4.2_\n\n- [x] 6. Implement font configuration system in template processing\n  - Update _process_template method to inject font variables from configuration\n  - Add font configuration extraction from tranche/imprint settings\n  - Ensure Korean font and other fonts are properly substituted in templates\n  - _Requirements: 3.1, 3.2, 3.3_\n\n- [x] 7. Update LaTeX template to use font variables\n  - Replace hardcoded \"Apple Myungjo\" with {korean_font} template variable\n  - Ensure template can handle font substitution gracefully\n  - Add fallback handling for missing font configurations\n  - _Requirements: 3.1, 3.2, 3.4_\n\n- [x] 8. Validate glossary generation and formatting\n  - Ensure glossary is generated in proper 2-column layout within page margins\n  - Verify Korean terms appear at top of left-hand cells with English equivalents below\n  - Test glossary layout with various term counts and lengths\n  - Add validation for glossary formatting requirements\n  - _Requirements: 4.3, 4.4, 4.5_\n\n- [x] 9. Fix LaTeX escaping and command formatting issues\n  - Fix broken LaTeX commands like \"extit{\" in foreword and other generated content\n  - Implement robust LaTeX escaping for all text processing\n  - Ensure no stray LaTeX commands appear in final output\n  - Add validation to detect and fix malformed LaTeX commands\n  - _Requirements: 5.1, 5.2, 5.3, 5.4_\n\n- [x] 10. Implement proper bibliography formatting with hanging indents\n  - Ensure bibliography citations have first line flush left\n  - Implement 0.15 inch indentation for second and following lines\n  - Verify hanging indent formatting appears correctly in compiled PDF\n  - Add validation for bibliography formatting requirements\n  - _Requirements: 6.1, 6.2, 6.3, 6.4_\n\n- [ ] 11. Add configuration validation and error handling\n  - Implement validation for required fields (lightning_source_account, language_code)\n  - Add validation for LLM configuration parameters\n  - Add validation for font configuration and provide helpful error messages\n  - Create clear error messages for configuration issues\n  - _Requirements: 4.1, 4.2, 4.3, 4.4_\n\n- [ ] 12. Add comprehensive logging and transparency\n  - Log LLM model usage in BackmatterProcessor, foreword, and publisher's note generation\n  - Log configuration values at pipeline startup\n  - Log template substitution details for debugging\n  - Log backmatter generation success/failure for all components\n  - Add monitoring for configuration validation success/failure\n  - Add logging for LaTeX processing errors with context\n  - _Requirements: TR6_\n\n- [ ] 13. Create integration tests for configuration fixes\n  - Test that default values are properly applied in pipeline\n  - Test that LLM configuration flows correctly to BackmatterProcessor, foreword, and publisher's note generation\n  - Test that font configuration is properly substituted in templates\n  - Test that glossary is generated with proper 2-column Korean/English layout\n  - Test that LaTeX escaping prevents broken commands in output\n  - Test that bibliography formatting includes proper hanging indents\n  - Test error handling for invalid configurations\n  - _Requirements: All requirements validation_\n\n- [ ] 14. Update documentation and create migration guide\n  - Document the new default values and their rationale\n  - Create guide for users to update existing configurations\n  - Document the font configuration system\n  - Document LLM configuration propagation to all backmatter components\n  - Document glossary generation requirements and formatting\n  - Document LaTeX escaping improvements and bibliography formatting\n  - _Requirements: User guidance and transparency_",
      "size": 5204,
      "modified": "2025-08-05T20:20:01.076231",
      "spec_directory": "pipeline-configuration-fixes",
      "directory_modified": "2025-08-05T20:20:01.076231"
    },
    {
      "file": "tasks.md",
      "content": "# Implementation Plan\n\n## Phase 1: Lock Stable Components and Prevent Regressions\n\n- [ ] 1. Mark bibliography formatting as LOCKED (DO NOT CHANGE)\n  - Add clear documentation that bibliography uses memoir class hangparas\n  - Add validation to ensure format is not changed\n  - Create regression test for bibliography hanging indents\n  - _Requirements: 2.1, 2.2, 2.3_\n\n- [ ] 2. Implement configuration hierarchy enforcement\n  - Create ConfigurationHierarchyEnforcer class\n  - Implement strict hierarchy",
      "full_content": "# Implementation Plan\n\n## Phase 1: Lock Stable Components and Prevent Regressions\n\n- [ ] 1. Mark bibliography formatting as LOCKED (DO NOT CHANGE)\n  - Add clear documentation that bibliography uses memoir class hangparas\n  - Add validation to ensure format is not changed\n  - Create regression test for bibliography hanging indents\n  - _Requirements: 2.1, 2.2, 2.3_\n\n- [ ] 2. Implement configuration hierarchy enforcement\n  - Create ConfigurationHierarchyEnforcer class\n  - Implement strict hierarchy: default < publisher < imprint < tranche\n  - Ensure schedule.json subtitle always trumps machine-generated alternatives\n  - Ensure tranche author and imprint always trump LLM generated values\n  - _Requirements: 1.1, 1.2, 1.3_\n\n- [ ] 3. Fix ISBN display on copyright page\n  - Ensure assigned ISBN appears on copyright page\n  - Validate ISBN is pulled from correct configuration level\n  - Test ISBN display in compiled PDF\n  - _Requirements: 1.4_\n\n- [ ] 4. Fix logo font configuration\n  - Ensure Zapfino font is used for xynapse_traces (not Berkshire)\n  - Implement proper font hierarchy from imprint config\n  - Validate logo font in compiled PDF\n  - _Requirements: 1.5_\n\n## Phase 2: Fix Content Generation Issues\n\n- [ ] 5. Fix glossary formatting issues\n  - Remove numeral \"2\" from glossary by using `\\chapter*{Glossary}`\n  - Fix text overprinting by restoring proper leading\n  - Implement proper typographic spacing\n  - Test glossary display in compiled PDF\n  - _Requirements: 5.1, 5.2, 5.3, 5.4_\n\n- [ ] 6. Clean up publisher's note generation\n  - Remove boilerplate paragraph attachment\n  - Ensure 100% LLM generated content\n  - Use only `storefront_get_en_motivation` prompt output\n  - Test publisher's note in compiled PDF\n  - _Requirements: 3.1, 3.2, 3.3_\n\n- [ ] 7. Fix foreword generation Korean formatting\n  - Update foreword prompt to eliminate visible markdown\n  - Fix Korean character presentation using proper LaTeX commands\n  - Remove `*pilsa*` markdown syntax from output\n  - Use `\\textit{pilsa}` and `\\korean{}` properly\n  - _Requirements: 4.1, 4.2, 4.3_\n\n- [ ] 8. Ensure mnemonics section appears\n  - Debug why mnemonics section is not being created\n  - Implement proper fallback mechanisms\n  - Ensure mnemonics.tex file is generated\n  - Validate mnemonics appear in final document\n  - _Requirements: 6.1, 6.2, 6.3, 6.4_\n\n## Phase 3: Implement Reprompting System for Frontmatter\n\n- [ ] 9. Classify sections as frontmatter vs backmatter\n  - Mark foreword, publisher's note, glossary as frontmatter\n  - Mark mnemonics, bibliography as backmatter\n  - Document section classification clearly\n  - _Requirements: 7.1, 7.2, 7.3, 7.4, 7.5_\n\n- [ ] 10. Implement foreword reprompting\n  - Add foreword to reprompt_keys in prompts.json\n  - Create foreword prompt with proper Korean formatting\n  - Integrate foreword generation with reprompting system\n  - Follow same sequence as main model calls\n  - _Requirements: 4.4, 7.6_\n\n- [ ] 11. Integrate publisher's note with reprompting\n  - Ensure publisher's note uses existing `storefront_get_en_motivation`\n  - Remove any direct LLM calls in prepress for publisher's note\n  - Use reprompting system consistently\n  - _Requirements: 3.3, 7.6_\n\n## Phase 4: Comprehensive Validation and Testing\n\n- [ ] 12. Create regression prevention system\n  - Implement validation for all fixed components\n  - Add automated tests for configuration hierarchy\n  - Create content presence validation\n  - Add formatting quality checks\n  - _Requirements: All requirements validation_\n\n- [ ] 13. Add comprehensive logging and monitoring\n  - Log configuration hierarchy application\n  - Log all section generation success/failure\n  - Monitor for formatting regressions\n  - Add clear error messages for debugging\n  - _Requirements: TR5_\n\n- [ ] 14. Create integration tests\n  - Test complete pipeline with configuration hash 4889ffa3373907e7\n  - Validate all sections appear in final document\n  - Test configuration hierarchy enforcement\n  - Verify no regressions in working components\n  - _Requirements: All requirements validation_\n\n- [ ] 15. Document locked components and anti-regression measures\n  - Create clear documentation of DO NOT CHANGE components\n  - Document configuration hierarchy requirements\n  - Create troubleshooting guide for common issues\n  - Document best practices for preventing regressions\n  - _Requirements: TR5_\n\n## Critical Success Criteria\n\nEach task must ensure:\n1.  No regressions in working components (especially bibliography)\n2.  Configuration hierarchy strictly enforced\n3.  All sections appear in final document\n4.  Clean, professional formatting output\n5.  Proper Korean character handling\n6.  No visible markdown or formatting errors\n\n## Anti-Regression Checklist\n\nBefore marking any task complete:\n- [ ] Bibliography hanging indents still work correctly\n- [ ] Configuration hierarchy is enforced\n- [ ] All required sections appear in final document\n- [ ] No new formatting errors introduced\n- [ ] Korean characters display properly\n- [ ] No visible markdown syntax in output",
      "size": 5043,
      "modified": "2025-08-06T00:48:02.840139",
      "spec_directory": "frontmatter-backmatter-fixes",
      "directory_modified": "2025-08-06T00:48:02.840139"
    }
  ],
  "design": [
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThe LSI Field Enhancement Phase 2 addresses specific issues identified during acceptance testing of the initial implementation. This phase focuses on three key areas:\n\n1. **LLM Completion Storage**: Ensuring LLM-generated field completions are properly stored in a consistent directory structure.\n2. **LSI CSV Output Integration**: Ensuring LLM-generated field completions are properly included in the final LSI CSV output.\n3. **Field Completion Visibility**: Providin",
      "full_content": "# Design Document\n\n## Overview\n\nThe LSI Field Enhancement Phase 2 addresses specific issues identified during acceptance testing of the initial implementation. This phase focuses on three key areas:\n\n1. **LLM Completion Storage**: Ensuring LLM-generated field completions are properly stored in a consistent directory structure.\n2. **LSI CSV Output Integration**: Ensuring LLM-generated field completions are properly included in the final LSI CSV output.\n3. **Field Completion Visibility**: Providing detailed reports on field completion strategies and values.\n\nThe design builds upon the existing architecture while adding new components and enhancing existing ones to address these specific issues.\n\n## Architecture\n\n### Enhanced Components\n\n```\n\n                    LSI ACS Generator                        \n\n       \n     Field Mapper       Validator         Config      \n     - Strategy         - Rules           - Defaults  \n     - Transform        - Formats         - Overrides \n       \n\n       \n   Enhanced            File Manager       Reporter    \n   Metadata Model      - PDF Check        - Logging   \n   - LSI Fields        - FTP Stage        - Errors    \n       \n\n                              \n                              \n\n                Phase 2 Enhancements                         \n\n       \n   Enhanced LLM       Enhanced LLM       Field        \n   Field Completer    Completion         Completion   \n   - Storage          Strategy           Reporter     \n       \n\n```\n\n### Enhanced LLM Field Completer\n\nThe existing `LLMFieldCompleter` class will be enhanced to improve storage of LLM completions:\n\n```python\nclass LLMFieldCompleter:\n    # ... existing methods ...\n    \n    def _save_completions_to_disk(self, metadata: CodexMetadata, output_dir: Optional[str] = None) -> None:\n        \"\"\"\n        Save LLM completions to disk.\n        \n        Args:\n            metadata: CodexMetadata object with completions\n            output_dir: Directory to save completions (defaults to metadata/ parallel to covers/ and interiors/)\n            \n        Returns:\n            Path to the saved file or None if saving failed\n        \"\"\"\n        # Enhanced directory discovery logic\n        # Look for existing book directories by publisher_reference_id or ISBN\n        # Create directory structure if it doesn't exist\n        # Save completions with consistent file naming\n```\n\n### Enhanced LLM Completion Strategy\n\nThe `LLMCompletionStrategy` class will be enhanced to properly include LLM completions in the CSV output:\n\n```python\nclass LLMCompletionStrategy(MappingStrategy):\n    \"\"\"\n    Strategy for LLM-based field completion.\n    Uses the LLMFieldCompleter to generate field values intelligently.\n    \n    This strategy first checks if the field has already been completed in the\n    metadata.llm_completions dictionary, then checks if the field has a direct value,\n    and finally attempts to complete the field using the LLMFieldCompleter.\n    \"\"\"\n    \n    def __init__(self, field_completer, metadata_field: str, prompt_key: str = None, fallback_value: str = \"\"):\n        # ... initialization ...\n    \n    def map_field(self, metadata: CodexMetadata, context: MappingContext) -> str:\n        \"\"\"Generate field value using LLM field completer with fallback.\"\"\"\n        # Check for existing completions in metadata.llm_completions\n        # Check for direct field value\n        # Use LLM to complete the field if needed\n        # Log the source of the field value\n```\n\n### Field Completion Reporter\n\nA new `LSIFieldCompletionReporter` class will be implemented to provide detailed reports on field completion strategies and values:\n\n```python\nclass LSIFieldCompletionReporter:\n    \"\"\"\n    Generates reports on LSI field completion strategies and values.\n    \n    This class provides visibility into which fields are being completed by which\n    strategies and their actual values. It can generate reports in CSV and HTML formats.\n    \"\"\"\n    \n    def __init__(self, registry: FieldMappingRegistry):\n        \"\"\"Initialize with field mapping registry.\"\"\"\n        \n    def generate_field_strategy_report(self, metadata: CodexMetadata, \n                                      lsi_headers: List[str], \n                                      output_dir: Optional[str] = None,\n                                      formats: List[str] = [\"csv\", \"html\"]) -> Dict[str, str]:\n        \"\"\"Generate report on field completion strategies and values.\"\"\"\n        \n    def _generate_report_data(self, metadata: CodexMetadata, lsi_headers: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Generate report data for field completion strategies and values.\"\"\"\n        \n    def _determine_field_source(self, metadata: CodexMetadata, field_name: str, \n                              strategy: MappingStrategy, value: str) -> str:\n        \"\"\"Determine the source of a field value.\"\"\"\n        \n    def _generate_csv_report(self, report_data: List[Dict[str, Any]], output_path: str) -> None:\n        \"\"\"Generate a CSV report.\"\"\"\n        \n    def _generate_html_report(self, report_data: List[Dict[str, Any]], \n                            metadata: CodexMetadata, output_path: str) -> None:\n        \"\"\"Generate an HTML report with statistics and formatting.\"\"\"\n```\n\n### Book Pipeline Integration\n\nThe existing `run_book_pipeline.py` script will be enhanced to integrate the field completion reporter:\n\n```python\n# In run_book_pipeline.py\n\n# After generating batch LSI CSV\nif result.success:\n    # ... existing code ...\n    \n    # Generate field completion reports\n    try:\n        # Initialize LSIFieldCompletionReporter\n        reporter = LSIFieldCompletionReporter(registry)\n        \n        # Generate reports for each metadata object\n        for metadata in lsi_metadata_list:\n            # Generate report\n            report_files = reporter.generate_field_strategy_report(\n                metadata=metadata,\n                lsi_headers=headers,\n                output_dir=config['lsi_output_dir'],\n                formats=[\"html\", \"csv\"]\n            )\n            \n        # Also generate combined report for backward compatibility\n        generate_field_report(\n            csv_path=str(batch_lsi_csv_path),\n            output_path=str(report_path),\n            config_path=config['lsi_config_path'],\n            format=\"markdown\"\n        )\n    except Exception as e:\n        logger.warning(f\"Failed to generate field completion report: {e}\")\n```\n\n## Data Models\n\n### Field Completion Report Data\n\n```python\n@dataclass\nclass FieldCompletionData:\n    field_name: str\n    strategy_type: str\n    value: str\n    source: str\n    is_empty: bool\n    llm_value: Optional[str] = None\n```\n\n### Report Statistics\n\n```python\n@dataclass\nclass ReportStatistics:\n    total_fields: int\n    populated_fields: int\n    empty_fields: int\n    completion_percentage: float\n    strategy_counts: Dict[str, int]\n    source_counts: Dict[str, int]\n```\n\n## Implementation Approach\n\n### Phase 2.1: Enhanced LLM Field Completer\n\n1. Enhance the `_save_completions_to_disk` method to improve directory discovery\n2. Add robust error handling and logging\n3. Ensure consistent file naming with timestamps and ISBN\n\n### Phase 2.2: Enhanced LLM Completion Strategy\n\n1. Update the `LLMCompletionStrategy` class to check for existing completions\n2. Add support for prompt_key parameter to help locate the right completion\n3. Improve field value extraction logic to handle different result formats\n4. Add logging for field completion source\n\n### Phase 2.3: Field Completion Reporter\n\n1. Implement the `LSIFieldCompletionReporter` class\n2. Add support for multiple output formats (CSV, HTML, JSON)\n3. Include statistics on field population rates and strategy usage\n4. Create visually appealing HTML reports with progress bars and color coding\n\n### Phase 2.4: Book Pipeline Integration\n\n1. Update `run_book_pipeline.py` to use the new reporter\n2. Add fallback to existing report generator for backward compatibility\n3. Ensure reports are generated for each book in the batch\n4. Add error handling to continue processing even if reporting fails\n\n## Testing Strategy\n\n### Unit Testing\n\n1. **LLM Field Completer Tests**\n   - Test directory discovery logic with various scenarios\n   - Test file naming and path construction\n   - Test error handling and recovery\n\n2. **LLM Completion Strategy Tests**\n   - Test completion source priority (direct field, llm_completions, new completion)\n   - Test field value extraction from different result formats\n   - Test fallback behavior\n\n3. **Field Completion Reporter Tests**\n   - Test report data generation\n   - Test field source determination\n   - Test CSV and HTML report generation\n   - Test statistics calculation\n\n### Integration Testing\n\n1. **End-to-End Tests**\n   - Test complete flow from LLM completion to CSV generation to reporting\n   - Test with various metadata completeness levels\n   - Test with different output formats and configurations\n\n2. **Book Pipeline Integration Tests**\n   - Test report generation during batch processing\n   - Test error handling and recovery\n   - Test backward compatibility with existing reporting\n\n### Validation Testing\n\n1. **Real-world Data Tests**\n   - Test with existing book metadata\n   - Test with edge cases and unusual data\n   - Verify report accuracy and completeness\n\n## Implementation Phases\n\n### Phase 2.1: Core Enhancements\n- Enhanced LLM Field Completer with improved storage\n- Enhanced LLM Completion Strategy with better integration\n\n### Phase 2.2: Reporting System\n- Field Completion Reporter implementation\n- Multiple output format support\n- Statistics and visualization\n\n### Phase 2.3: Pipeline Integration\n- Book Pipeline integration\n- Backward compatibility\n- Error handling and recovery",
      "size": 10588,
      "modified": "2025-07-18T21:02:00.255514",
      "spec_directory": "lsi-field-enhancement-phase2",
      "directory_modified": "2025-07-18T21:02:00.255804"
    },
    {
      "file": "Field Name | Strategy | Value | Source |",
      "content": "Field Name | Strategy | Value | Source | Empty |\n|------------|----------|-------|--------|-------|\n| # in Series | DirectMappingStrategy |  | Default | Yes |\n| Annotation / Summary | DirectMappingStrategy |  | Default | Yes |\n| BISAC Category 2 | DirectMappingStrategy |  | Default | Yes |\n| BISAC Category 3 | DirectMappingStrategy |  | Default | Yes |\n| Contributor One Affiliations | DirectMappingStrategy |  | Default | Yes |\n| Contributor One BIO | DirectMappingStrategy |  | Default | Yes |\n| ",
      "full_content": "Field Name | Strategy | Value | Source | Empty |\n|------------|----------|-------|--------|-------|\n| # in Series | DirectMappingStrategy |  | Default | Yes |\n| Annotation / Summary | DirectMappingStrategy |  | Default | Yes |\n| BISAC Category 2 | DirectMappingStrategy |  | Default | Yes |\n| BISAC Category 3 | DirectMappingStrategy |  | Default | Yes |\n| Contributor One Affiliations | DirectMappingStrategy |  | Default | Yes |\n| Contributor One BIO | DirectMappingStrategy |  | Default | Yes |\n| Contributor One Location | DirectMappingStrategy |  | Default | Yes |\n| Contributor One Location Type Code | DirectMappingStrategy |  | Default | Yes |\n| Contributor One Prior Work | DirectMappingStrategy |  | Default | Yes |\n| Contributor One Professional Position | DirectMappingStrategy |  | Default | Yes |\n| Contributor Three | DirectMappingStrategy |  | Default | Yes |\n| Contributor Three Role | DirectMappingStrategy |  | Default | Yes |\n| Contributor Two | DirectMappingStrategy |  | Default | Yes |\n| Contributor Two Role | DirectMappingStrategy |  | Default | Yes |\n| Cover Path / Filename | DirectMappingStrategy |  | Default | Yes |\n| ISBN or SKU | DirectMappingStrategy |  | Default | Yes |\n| Illustration Notes | DirectMappingStrategy |  | Default | Yes |\n| Interior Path / Filename | DirectMappingStrategy |  | Default | Yes |\n| Jacket Path / Filename | DirectMappingStrategy |  | Default | Yes |\n| LSI FlexField1 (please consult LSI before using) | DirectMappingStrategy |  | Default | Yes |\n| LSI FlexField2 (please consult LSI before using) | DirectMappingStrategy |  | Default | Yes |\n| LSI FlexField3 (please consult LSI before using) | DirectMappingStrategy |  | Default | Yes |\n| LSI FlexField4 (please consult LSI before using) | DirectMappingStrategy |  | Default | Yes |\n| LSI FlexField5 (please consult LSI before using) | DirectMappingStrategy |  | Default | Yes |\n| LSI Special Category  (please consult LSI before using | DirectMappingStrategy |  | Default | Yes |\n| Marketing Image | DirectMappingStrategy |  | Default | Yes |\n| Max Age | DirectMappingStrategy |  | Default | Yes |\n| Max Grade | DirectMappingStrategy |  | Default | Yes |\n| Min Age | DirectMappingStrategy |  | Default | Yes |\n| Min Grade | DirectMappingStrategy |  | Default | Yes |\n| Parent ISBN | DirectMappingStrategy |  | Default | Yes |\n| Publisher Reference ID | DirectMappingStrategy |  | Default | Yes |\n| Regional Subjects | DirectMappingStrategy |  | Default | Yes |\n| Reserved (Special Instructions) | DefaultMappingStrategy |  | Default | Yes |\n| Reserved 1 | DefaultMappingStrategy |  | Default | Yes |\n| Reserved 2 | DefaultMappingStrategy |  | Default | Yes |\n| Reserved 3 | DefaultMappingStrategy |  | Default | Yes |\n| Reserved 4 | DefaultMappingStrategy |  | Default | Yes |\n| Reserved10 | DefaultMappingStrategy |  | Default | Yes |\n| Reserved11 | DefaultMappingStrategy |  | Default | Yes |\n| Reserved12 | DefaultMappingStrategy |  | Default | Yes |\n| Reserved5 | DefaultMappingStrategy |  | Default | Yes |\n| Reserved6 | DefaultMappingStrategy |  | Default | Yes |\n| Reserved7 | DefaultMappingStrategy |  | Default | Yes |\n| Reserved8 | DefaultMappingStrategy |  | Default | Yes |\n| Reserved9 | DefaultMappingStrategy |  | Default | Yes |\n| Review Quote(s) | DefaultMappingStrategy |  | Default | Yes |\n| SIBI - EDUC - US * Suggested List Price (mode 2) | DefaultMappingStrategy |  | Default | Yes |\n| SIBI - EDUC - US * Wholesale Discount % (Mode 2) | DefaultMappingStrategy |  | Default | Yes |\n| Series Name | DirectMappingStrategy |  | Default | Yes |\n| Short Description | DirectMappingStrategy |  | Default | Yes |\n| Stamped Text CENTER | DirectMappingStrategy |  | Default | Yes |\n| Stamped Text LEFT | DirectMappingStrategy |  | Default | Yes |\n| Stamped Text RIGHT | DirectMappingStrategy |  | Default | Yes |\n| Street Date | DirectMappingStrategy |  | Default | Yes |\n| Table of Contents | DirectMappingStrategy |  | Default | Yes |\n| Thema Subject 1 | DirectMappingStrategy |  | Default | Yes |\n| Thema Subject 2 | DirectMappingStrategy |  | Default | Yes |\n| Thema Subject 3 | DirectMappingStrategy |  | Default | Yes |",
      "size": 4151,
      "modified": "2025-07-18T21:02:00.256047",
      "spec_directory": "lsi-field-enhancement-phase3",
      "directory_modified": "2025-07-18T21:02:00.256615"
    },
    {
      "file": "design.md",
      "content": "# Design Document: LSI Field Enhancement Phase 3\n\n## Overview\n\nLSI Field Enhancement Phase 3 builds upon the existing field mapping and completion infrastructure to implement three key features:\n\n1. **ISBN Database Management**: A system to track, manage, and automatically assign ISBNs from publisher-owned inventory.\n2. **Series Metadata Management**: A framework for managing book series metadata with proper sequencing and publisher isolation.\n3. **Enhanced Field Completion**: Improvements to th",
      "full_content": "# Design Document: LSI Field Enhancement Phase 3\n\n## Overview\n\nLSI Field Enhancement Phase 3 builds upon the existing field mapping and completion infrastructure to implement three key features:\n\n1. **ISBN Database Management**: A system to track, manage, and automatically assign ISBNs from publisher-owned inventory.\n2. **Series Metadata Management**: A framework for managing book series metadata with proper sequencing and publisher isolation.\n3. **Enhanced Field Completion**: Improvements to the existing LLM-based field completion system for LSI metadata fields.\n\nThis design document outlines the architecture, components, data models, and implementation approach for each of these features.\n\n## Architecture\n\nThe Phase 3 enhancements will integrate with the existing LSI field mapping and completion system while adding new components for ISBN and series management. The overall architecture follows the established pattern of the Codexes Factory platform:\n\n```\n          \n                                                                         \n  ISBN Database             Series Metadata           Enhanced Field     \n  Manager                   Manager                   Completion         \n                                                                         \n          \n                                                                \n                                                                \n                                                                \n\n                                                                             \n                       LSI Field Mapping Registry                            \n                                                                             \n\n                                      \n                                      \n                                      \n\n                                                                             \n                       LSI ACS Generator                                     \n                                                                             \n\n```\n\n### Key Design Principles\n\n1. **Modularity**: Each component is designed to be independent but interoperable.\n2. **Extensibility**: The system can be extended to support additional features in future phases.\n3. **Configurability**: All components are configurable through JSON configuration files.\n4. **Validation**: Robust validation ensures data integrity throughout the process.\n5. **Logging**: Comprehensive logging provides visibility into the system's operation.\n\n## Components and Interfaces\n\n### 1. ISBN Database Manager\n\nThe ISBN Database Manager will handle the storage, tracking, and assignment of ISBNs.\n\n#### Key Components:\n\n- **ISBNDatabase**: Core class for managing the ISBN database\n- **ISBNImporter**: Handles importing ISBNs from Bowker spreadsheets\n- **ISBNAssigner**: Assigns ISBNs to books based on availability\n- **ISBNStatusTracker**: Tracks the status of ISBNs (available, privately assigned, publicly assigned)\n\n#### Interface:\n\n```python\nclass ISBNDatabase:\n    def import_from_bowker(self, file_path: str) -> Dict[str, Any]:\n        \"\"\"Import ISBNs from a Bowker spreadsheet.\"\"\"\n        pass\n        \n    def get_next_available_isbn(self, publisher_id: str = None) -> str:\n        \"\"\"Get the next available ISBN for a publisher.\"\"\"\n        pass\n        \n    def assign_isbn(self, isbn: str, book_id: str) -> bool:\n        \"\"\"Assign an ISBN to a book (private assignment).\"\"\"\n        pass\n        \n    def mark_as_published(self, isbn: str) -> bool:\n        \"\"\"Mark an ISBN as publicly assigned (published).\"\"\"\n        pass\n        \n    def release_isbn(self, isbn: str) -> bool:\n        \"\"\"Release a privately assigned ISBN back to the available pool.\"\"\"\n        pass\n        \n    def get_isbn_status(self, isbn: str) -> str:\n        \"\"\"Get the status of an ISBN (available, privately assigned, publicly assigned).\"\"\"\n        pass\n```\n\n### 2. Series Metadata Manager\n\nThe Series Metadata Manager will handle the creation, tracking, and assignment of series metadata.\n\n#### Key Components:\n\n- **SeriesRegistry**: Core class for managing series metadata\n- **SeriesAssigner**: Assigns books to series and manages sequence numbers\n- **SeriesValidator**: Validates series metadata for consistency\n- **SeriesUIIntegration**: Integrates with the UI for series selection and creation\n\n#### Interface:\n\n```python\nclass SeriesRegistry:\n    def create_series(self, name: str, publisher_id: str, multi_publisher: bool = False) -> str:\n        \"\"\"Create a new series and return its ID.\"\"\"\n        pass\n        \n    def add_book_to_series(self, series_id: str, book_id: str, sequence_number: int = None) -> bool:\n        \"\"\"Add a book to a series with an optional sequence number.\"\"\"\n        pass\n        \n    def get_next_sequence_number(self, series_id: str) -> int:\n        \"\"\"Get the next available sequence number for a series.\"\"\"\n        pass\n        \n    def get_series_by_name(self, name: str, publisher_id: str = None) -> List[Dict[str, Any]]:\n        \"\"\"Get series by name, optionally filtered by publisher.\"\"\"\n        pass\n        \n    def get_series_by_id(self, series_id: str) -> Dict[str, Any]:\n        \"\"\"Get series metadata by ID.\"\"\"\n        pass\n        \n    def update_series(self, series_id: str, updates: Dict[str, Any]) -> bool:\n        \"\"\"Update series metadata.\"\"\"\n        pass\n        \n    def delete_series(self, series_id: str) -> bool:\n        \"\"\"Delete a series (if no books are assigned to it).\"\"\"\n        pass\n```\n\n### 3. Enhanced Field Completion\n\nThe Enhanced Field Completion system will build upon the existing LLMFieldCompleter to improve field completion for various LSI metadata fields.\n\n#### Key Components:\n\n- **EnhancedLLMFieldCompleter**: Extended version of the existing LLMFieldCompleter\n- **FieldCompletionStrategies**: Specialized strategies for different field types\n- **FieldCompletionReporter**: Generates reports on field completion status\n- **ValidationFramework**: Validates completed fields against LSI requirements\n\n#### Interface:\n\n```python\nclass EnhancedLLMFieldCompleter(LLMFieldCompleter):\n    def complete_annotation_summary(self, metadata: CodexMetadata) -> str:\n        \"\"\"Complete the Annotation/Summary field with enhanced formatting.\"\"\"\n        pass\n        \n    def suggest_bisac_codes(self, metadata: CodexMetadata) -> List[str]:\n        \"\"\"Suggest BISAC codes with category overrides support.\"\"\"\n        pass\n        \n    def suggest_thema_codes(self, metadata: CodexMetadata) -> List[str]:\n        \"\"\"Suggest Thema subject codes.\"\"\"\n        pass\n        \n    def extract_lsi_contributor_info(self, metadata: CodexMetadata) -> Dict[str, Any]:\n        \"\"\"Extract comprehensive contributor information.\"\"\"\n        pass\n        \n    def get_illustration_info(self, metadata: CodexMetadata) -> Dict[str, Any]:\n        \"\"\"Get illustration count and notes.\"\"\"\n        pass\n        \n    def create_simple_toc(self, metadata: CodexMetadata, book_content: str = None) -> str:\n        \"\"\"Create a simple table of contents.\"\"\"\n        pass\n```\n\n## Data Models\n\n### 1. ISBN Database Model\n\n```python\n@dataclass\nclass ISBN:\n    isbn: str\n    publisher_id: str\n    status: str  # \"available\", \"privately_assigned\", \"publicly_assigned\"\n    assigned_to: Optional[str] = None  # book_id if assigned\n    assignment_date: Optional[datetime] = None\n    publication_date: Optional[datetime] = None\n    notes: Optional[str] = None\n```\n\n### 2. Series Metadata Model\n\n```python\n@dataclass\nclass Series:\n    id: str\n    name: str\n    publisher_id: str\n    multi_publisher: bool = False\n    creation_date: datetime = field(default_factory=datetime.now)\n    last_updated: datetime = field(default_factory=datetime.now)\n    \n@dataclass\nclass SeriesBook:\n    series_id: str\n    book_id: str\n    sequence_number: int\n    addition_date: datetime = field(default_factory=datetime.now)\n```\n\n### 3. Enhanced Field Completion Models\n\n```python\n@dataclass\nclass FieldCompletionResult:\n    field_name: str\n    value: Any\n    completion_method: str  # \"llm\", \"direct\", \"computed\", etc.\n    confidence_score: float = 1.0\n    completion_time: datetime = field(default_factory=datetime.now)\n    \n@dataclass\nclass FieldCompletionReport:\n    metadata_id: str\n    completed_fields: List[FieldCompletionResult]\n    missing_fields: List[str]\n    error_fields: Dict[str, str]  # field_name -> error_message\n    completion_time: datetime = field(default_factory=datetime.now)\n```\n\n## Error Handling\n\n### ISBN Database Errors\n\n- **ISBNNotFoundError**: Raised when an ISBN is not found in the database\n- **ISBNAlreadyAssignedError**: Raised when attempting to assign an already assigned ISBN\n- **ISBNPublishedError**: Raised when attempting to release a publicly assigned ISBN\n- **BowkerImportError**: Raised when there's an error importing ISBNs from a Bowker spreadsheet\n\n### Series Metadata Errors\n\n- **SeriesNotFoundError**: Raised when a series is not found\n- **SeriesAccessDeniedError**: Raised when a publisher attempts to access another publisher's series\n- **SequenceNumberConflictError**: Raised when there's a conflict in sequence numbers\n- **SeriesDeleteError**: Raised when attempting to delete a series with assigned books\n\n### Field Completion Errors\n\n- **FieldCompletionError**: Base error for field completion issues\n- **ValidationError**: Raised when a completed field fails validation\n- **LLMCallError**: Raised when there's an error calling the LLM\n- **FieldFormatError**: Raised when a field cannot be formatted correctly\n\n## Testing Strategy\n\n### Unit Tests\n\n1. **ISBN Database Tests**:\n   - Test importing ISBNs from various Bowker spreadsheet formats\n   - Test ISBN assignment and status tracking\n   - Test edge cases like releasing already published ISBNs\n\n2. **Series Metadata Tests**:\n   - Test series creation and book assignment\n   - Test sequence number management\n   - Test multi-publisher series functionality\n   - Test CRUD operations on series\n\n3. **Field Completion Tests**:\n   - Test each field completion strategy\n   - Test validation of completed fields\n   - Test integration with the existing LLMFieldCompleter\n\n### Integration Tests\n\n1. **End-to-End ISBN Management**:\n   - Test the full ISBN lifecycle from import to assignment to publication\n\n2. **Series Management Integration**:\n   - Test series creation and assignment through the UI\n   - Test series metadata in LSI CSV generation\n\n3. **Field Completion Pipeline**:\n   - Test the complete field completion pipeline with real book data\n   - Test integration with the LSI ACS Generator\n\n### Mock Tests\n\n1. **LLM Mock Tests**:\n   - Test field completion with mocked LLM responses\n   - Test error handling with simulated LLM failures\n\n2. **Database Mock Tests**:\n   - Test ISBN and series operations with a mocked database\n   - Test concurrent operations with simulated race conditions\n\n## Implementation Plan\n\nThe implementation will follow a phased approach, with each component being developed and tested independently before integration.\n\n### Phase 3.1: ISBN Database Management\n\n1. Implement the core ISBNDatabase class\n2. Develop the Bowker spreadsheet importer\n3. Implement ISBN assignment and status tracking\n4. Create the ISBN database schema and storage mechanism\n5. Develop the ISBN management UI\n6. Integrate with the LSI ACS Generator\n\n### Phase 3.2: Series Metadata Management\n\n1. Implement the core SeriesRegistry class\n2. Develop the series assignment and sequence number management\n3. Implement multi-publisher series support\n4. Create the series metadata schema and storage mechanism\n5. Develop the series management UI\n6. Integrate with the LSI ACS Generator\n\n### Phase 3.3: Enhanced Field Completion\n\n1. Extend the LLMFieldCompleter with enhanced strategies\n2. Implement specialized field completion for each required field\n3. Develop the field completion reporter\n4. Enhance the validation framework\n5. Update the field mapping registry with new strategies\n6. Integrate with the LSI ACS Generator\n\n## Conclusion\n\nThe LSI Field Enhancement Phase 3 design builds upon the existing infrastructure to provide comprehensive ISBN management, series metadata management, and enhanced field completion. These features will significantly improve the automation and quality of LSI metadata generation, reducing manual effort and ensuring consistent, high-quality metadata for book distribution.\n\nThe modular design ensures that each component can be developed, tested, and deployed independently, while still working together seamlessly in the overall system. The robust error handling and testing strategy will ensure reliability and maintainability as the system evolves.",
      "size": 13188,
      "modified": "2025-07-18T21:02:00.256294",
      "spec_directory": "lsi-field-enhancement-phase3",
      "directory_modified": "2025-07-18T21:02:00.256615"
    },
    {
      "file": "acceptance_test_fixes.md",
      "content": "# LSI Field Enhancement Acceptance Test Fixes\n\n## Issues Identified\n\n1. **LLM Completion Storage**: LLM completion model responses are not being saved in the expected directory structure (metadata/ parallel to covers/ and interiors/).\n\n2. **LSI CSV Output**: LLM completions from lsi_field_completion_prompts are not being included in the final LSI CSV output.\n\n3. **Field Completion Visibility**: Need a checklist table showing which fields are being completed by which strategy and their actual val",
      "full_content": "# LSI Field Enhancement Acceptance Test Fixes\n\n## Issues Identified\n\n1. **LLM Completion Storage**: LLM completion model responses are not being saved in the expected directory structure (metadata/ parallel to covers/ and interiors/).\n\n2. **LSI CSV Output**: LLM completions from lsi_field_completion_prompts are not being included in the final LSI CSV output.\n\n3. **Field Completion Visibility**: Need a checklist table showing which fields are being completed by which strategy and their actual values.\n\n## Implementation Plan\n\n### 1. Fix LLM Completion Storage\n\n- [ ] 1.1 Modify LLMFieldCompleter to save responses in metadata/ directory\n  - Update the `complete_missing_fields` method to save responses to disk\n  - Create directory structure parallel to covers/ and interiors/\n  - Save responses in JSON format with timestamp and metadata ID\n  - Add configuration option for output directory\n\n### 2. Ensure LLM Completions in LSI CSV Output\n\n- [ ] 2.1 Verify field mapping from LLM completions to CSV\n  - Audit the field mapping registry to ensure LLM completions are properly mapped\n  - Add explicit mapping for each field in lsi_field_completion_prompts.json\n  - Test with sample data to verify completions appear in CSV\n\n- [ ] 2.2 Update LLMCompletionStrategy\n  - Ensure the strategy correctly retrieves values from llm_completions\n  - Add fallback to direct field access if not in llm_completions\n  - Add logging for field completion source\n\n### 3. Create Field Completion Report\n\n- [ ] 3.1 Implement field completion report generator\n  - Create a new class `LSIFieldCompletionReporter`\n  - Add method to generate field strategy report\n  - Include columns: field name, field strategy, actual value, source\n  - Output as CSV and HTML formats\n\n- [ ] 3.2 Integrate with Book Pipeline\n  - Add option to generate field completion report in run_book_pipeline.py\n  - Save report alongside LSI CSV output\n  - Add summary statistics to pipeline output\n\n### 4. Verify Book Pipeline Integration\n\n- [ ] 4.1 Create comprehensive test with live API/real books\n  - Set up test with real book data from production\n  - Run full pipeline with LLM completion enabled\n  - Verify all expected fields are completed\n  - Generate and analyze field completion report\n\n- [ ] 4.2 Add validation checks\n  - Add checks for critical LSI fields\n  - Implement warning system for incomplete fields\n  - Create field completion percentage metric\n\n## Testing Plan\n\n1. **Unit Tests**:\n   - Test LLMFieldCompleter with mock responses\n   - Test field mapping with sample data\n   - Test report generator with known input\n\n2. **Integration Tests**:\n   - Test full pipeline with sample book\n   - Verify file storage locations\n   - Check CSV output against expected values\n\n3. **Acceptance Tests**:\n   - Run with real book data\n   - Verify all fields are properly populated\n   - Check report for field completion strategies\n   - Validate against LSI requirements\n\n## Deliverables\n\n1. Updated LLMFieldCompleter with proper storage\n2. Fixed field mapping for LLM completions\n3. Field completion report generator\n4. Book Pipeline integration\n5. Comprehensive test results\n6. Documentation updates",
      "size": 3158,
      "modified": "2025-07-18T21:02:00.256713",
      "spec_directory": "lsi-field-enhancement",
      "directory_modified": "2025-07-18T21:02:00.257098"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThe LSI ACS Implementation will transform the current LSI ACS Generator from a basic field mapper to a comprehensive, configurable system that handles all 100+ LSI template fields. The design focuses on extensibility, validation, and maintainability while ensuring LSI submission compliance.\n\nThe enhancement will introduce a layered architecture with field mapping strategies, validation pipelines, and configuration management to support diverse publishing scenarios",
      "full_content": "# Design Document\n\n## Overview\n\nThe LSI ACS Implementation will transform the current LSI ACS Generator from a basic field mapper to a comprehensive, configurable system that handles all 100+ LSI template fields. The design focuses on extensibility, validation, and maintainability while ensuring LSI submission compliance.\n\nThe enhancement will introduce a layered architecture with field mapping strategies, validation pipelines, and configuration management to support diverse publishing scenarios and territorial requirements.\n\nThe classes share some information with other existing classes and objects such as CodexesMetadata and the values should be consistent unless explicitly allowed.\n\n## Architecture\n\n### Core Components\n\n```\n\n                    LSI ACS Generator                        \n\n       \n     Field Mapper       Validator         Config      \n     - Strategy         - Rules           - Defaults  \n     - Transform        - Formats         - Overrides \n       \n\n       \n   Enhanced            File Manager       Reporter    \n   Metadata Model      - PDF Check        - Logging   \n   - LSI Fields        - FTP Stage        - Errors    \n       \n\n```\n\n### Field Mapping Strategy Pattern\n\nThe system will use a strategy pattern to handle different field mapping approaches:\n\n- **DirectMappingStrategy**: Simple 1:1 field mappings\n- **ComputedMappingStrategy**: Fields requiring calculation or transformation\n- **DefaultMappingStrategy**: Fields with fallback values\n- **ConditionalMappingStrategy**: Fields dependent on other field values\n- **LLMCompletionStrategy**: Fields requiring LLM inference using existing litellm infrastructure\n\n### LLM-Based Field Completion\n\nThe system will leverage the existing `llm_caller.py` and `prompt_manager.py` infrastructure to intelligently populate missing LSI fields:\n\n```python\nclass LLMFieldCompleter:\n    def __init__(self, llm_caller: LLMCaller, prompt_manager: PromptManager)\n    def complete_missing_fields(self, metadata: CodexMetadata) -> CodexMetadata\n    def generate_contributor_bio(self, author: str, title: str) -> str\n    def suggest_bisac_codes(self, title: str, summary: str) -> List[str]\n    def generate_marketing_copy(self, metadata: CodexMetadata) -> Dict[str, str]\n```\n\nThis approach will use targeted prompts to fill gaps in metadata, particularly for:\n- Contributor biographies and affiliations\n- Enhanced descriptions and marketing copy\n- BISAC code suggestions\n- Territorial market analysis\n\n## Components and Interfaces\n\n### Enhanced LSI ACS Generator\n\n```python\nclass EnhancedLsiAcsGenerator(BaseGenerator):\n    def __init__(self, template_path: str, config_path: Optional[str] = None)\n    def generate(self, metadata: CodexMetadata, output_path: str, **kwargs)\n    def validate_submission(self, metadata: CodexMetadata) -> ValidationResult\n    def generate_with_validation(self, metadata: CodexMetadata, output_path: str) -> GenerationResult\n```\n\n### Field Mapping System\n\n```python\nclass FieldMappingRegistry:\n    def register_strategy(self, field_name: str, strategy: MappingStrategy)\n    def get_mapping_for_field(self, field_name: str) -> MappingStrategy\n    def apply_all_mappings(self, metadata: CodexMetadata, headers: List[str]) -> List[str]\n\nclass MappingStrategy(ABC):\n    @abstractmethod\n    def map_field(self, metadata: CodexMetadata, context: MappingContext) -> str\n\nclass DirectMappingStrategy(MappingStrategy):\n    def __init__(self, metadata_field: str, default_value: str = \"\")\n\nclass ComputedMappingStrategy(MappingStrategy):\n    def __init__(self, computation_func: Callable[[CodexMetadata], str])\n```\n\n### Validation System\n\n```python\nclass LSIValidationPipeline:\n    def __init__(self, validators: List[FieldValidator])\n    def validate(self, metadata: CodexMetadata) -> ValidationResult\n    def validate_field(self, field_name: str, value: str) -> FieldValidationResult\n\nclass FieldValidator(ABC):\n    @abstractmethod\n    def validate(self, field_name: str, value: str, metadata: CodexMetadata) -> FieldValidationResult\n\nclass ISBNValidator(FieldValidator):\nclass PricingValidator(FieldValidator):\nclass DateValidator(FieldValidator):\nclass BISACValidator(FieldValidator):\nclass PDFValidator(FieldValidator):\n```\n\n### Configuration Management\n\n```python\nclass LSIConfiguration:\n    def __init__(self, config_path: Optional[str] = None)\n    def get_default_value(self, field_name: str) -> str\n    def get_field_override(self, field_name: str) -> Optional[str]\n    def get_imprint_config(self, imprint: str) -> Dict[str, str]\n    def get_territorial_config(self, territory: str) -> Dict[str, str]\n\n# Configuration file structure (YAML)\ndefaults:\n  publisher: \"Nimble Books LLC\"\n  imprint: \"Xynapse Traces\"\n  rendition_booktype: \"Perfect Bound\"\n  \nfield_overrides:\n  lightning_source_account: \"6024045\"\n  cover_submission_method: \"FTP\"\n  \nimprint_configs:\n  \"Xynapse Traces\":\n    publisher: \"Nimble Books LLC\"\n    us_wholesale_discount: \"40\"\n    returnability: \"Yes-Destroy\"\n    \nterritorial_configs:\n  UK:\n    wholesale_discount_percent: \"40\"\n  EU:\n    wholesale_discount_percent: \"40\"\n    returnability: \"Yes-Destroy\"\n\nand so on for all other territories\n\nNote: the system must support multiple configurable publishers and imprints. Store as json files in publishers/ and imprints/ directories.\n\n```\n\n## Data Models\n\n### Enhanced CodexMetadata Extensions\n\nThe existing CodexMetadata class will be extended with additional fields to support all LSI requirements:\n\n```python\n@dataclass\nclass CodexMetadata:\n    # ... existing fields ...\n    \n    # LSI Account and Submission Information\n    lightning_source_account: str = \"\" \n    metadata_contact_dictionary: dict = {} to come\n    parent_isbn: str = \"\"\n    cover_submission_method: str = \"FTP\"  # FTP, Email, Portal\n    text_block_submission_method: str = \"FTP\"\n    \n    # Enhanced Contributor Information\n    contributor_one_bio: str = \"\"\n    contributor_one_affiliations: str = \"\"\n    contributor_one_professional_position: str = \"\"\n    contributor_one_location: str = \"\"\n    contributor_one_location_type_code: str = \"\" # lookup table\n    contributor_one_prior_work: str = \"\"\n    \n    # Physical Specifications\n    weight_lbs: str = \"\" # calculate\n    carton_pack_quantity: str = \"1\"\n    \n    # Publication Timing\n    street_date: str = \"\"  # Different from pub_date\n    \n    # Territorial Rights\n    territorial_rights: str = \"World\" # default.  Alternatives should be validated against lookup table.\n    \n    # Edition Information\n    edition_number: str = \"\"\n    edition_description: str = \"\" \n    \n    # File Paths for Submission\n    jacket_path_filename: str = \"\"\n    interior_path_filename: str = \"\"\n    cover_path_filename: str = \"\"\n    \n    # Special LSI Fields\n\n    lsi_special_category: str = \"\"\n    stamped_text_left: str = \"\"\n    stamped_text_center: str = \"\"\n    stamped_text_right: str = \"\"\n    order_type_eligibility: str = \"\"\n    \n    # LSI Flex Fields (5 configurable fields)\n    lsi_flexfield1: str = \"\"\n    lsi_flexfield2: str = \"\"\n    lsi_flexfield3: str = \"\"\n    lsi_flexfield4: str = \"\"\n    lsi_flexfield5: str = \"\"\n    \n    # Publisher Reference\n    publisher_reference_id: str = \"\" # safe folder name for digital artifacts\n    \n    # Marketing\n    marketing_image: str = \"\" # path to artifact created during production\n```\n\n### Validation Result Models\n\n```python\n@dataclass\nclass FieldValidationResult:\n    field_name: str\n    is_valid: bool\n    error_message: str = \"\"\n    warning_message: str = \"\"\n    suggested_value: str = \"\"\n\n@dataclass\nclass ValidationResult:\n    is_valid: bool\n    field_results: List[FieldValidationResult]\n    errors: List[str]\n    warnings: List[str]\n    \n    def get_errors_by_field(self, field_name: str) -> List[str]\n    def has_blocking_errors(self) -> bool\n\n@dataclass\nclass GenerationResult:\n    success: bool\n    output_path: str\n    validation_result: ValidationResult\n    populated_fields_count: int\n    empty_fields_count: int\n    generation_timestamp: str\n```\n\n## Error Handling\n\n### Validation Error Categories\n\n1. **Blocking Errors**: Prevent LSI submission\n   - Invalid ISBN format\n   - Missing required fields (Title, ISBN, Publisher)\n   - Invalid date formats\n   - PDF validation failures\n   - Invalid trim sizes or paper weights\n   - Empty pricing fields for territories\n\n2. **Information Notices**: Optimization suggestions\n   - Default values used\n   - Fields auto-calculated\n   - Recommended field population\n\n### Error Recovery Strategies\n\n```python\nclass ErrorRecoveryManager:\n    def attempt_isbn_correction(self, isbn: str) -> str\n    def suggest_bisac_codes(self, title: str, keywords: str) -> List[str]\n    def calculate_missing_pricing(self, base_price: float, territory: str) -> str\n    def generate_default_contributor_info(self, author: str) -> Dict[str, str]\n```\n\n## Testing Strategy\n\n### Unit Testing\n\n1. **Field Mapping Tests**\n   - Test each mapping strategy independently\n   - Verify field transformations and calculations\n   - Test default value application\n\n2. **Validation Tests**\n   - Test each validator with valid/invalid inputs\n   - Test validation pipeline integration\n   - Test error message generation\n\n3. **Configuration Tests**\n   - Test configuration loading and parsing\n   - Test imprint and territorial overrides\n   - Test default value resolution\n\n### Integration Testing\n\n1. **End-to-End Generation Tests**\n   - Test complete metadata  CSV generation\n   - Test with various metadata completeness levels\n   - Test with different imprint configurations\n\n2. **File System Integration Tests**\n   - Test PDF validation with actual files\n   - Test FTP staging area checks\n   - Test file naming conventions\n\n### Validation Testing\n\n1. **LSI Compliance Tests**\n   - Test generated CSV against LSI template\n   - Test field ordering and formatting\n   - Test with LSI sample data\n\n2. **Real-world Data Tests**\n   - Test with existing book metadata\n   - Test with edge cases and unusual data\n   - Performance testing with large datasets\n\n### Test Data Management\n\n```python\n# Test fixtures for different scenarios\n@pytest.fixture\ndef minimal_metadata():\n    \"\"\"Metadata with only required fields\"\"\"\n\n@pytest.fixture  \ndef complete_metadata():\n    \"\"\"Metadata with all fields populated\"\"\"\n\n@pytest.fixture\ndef multi_contributor_metadata():\n    \"\"\"Metadata with multiple contributors\"\"\"\n\n@pytest.fixture\ndef international_metadata():\n    \"\"\"Metadata with international pricing\"\"\"\n```\n\n## Implementation Phases\n\n### Phase 1: Core Infrastructure\n- Enhanced metadata model with LSI fields\n- Field mapping registry and strategy pattern\n- Basic validation framework\n\n### Phase 2: Comprehensive Field Support\n- All 100+ LSI field mappings\n- Configuration system implementation\n- Advanced validation rules\n\n### Phase 3: File Integration\n- PDF validation system\n- FTP staging area integration\n- File naming and path management\n\n### Phase 4: Reporting and Monitoring\n- Detailed logging system\n- Error reporting and recovery\n- Generation analytics and metrics",
      "size": 11522,
      "modified": "2025-07-16T03:17:05.403268",
      "spec_directory": "lsi-field-enhancement",
      "directory_modified": "2025-07-18T21:02:00.257098"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nLSI Field Enhancement Phase 4 is designed to significantly improve the field population rate in the LSI CSV output, addressing the current issue where 62.18% of fields are empty. This phase builds upon the previous enhancements and focuses on four key areas:\n\n1. **Enhanced LLM Field Completion**: Improving the quality and success rate of LLM-based field completion for subjective fields.\n2. **Expanded Computed Fields**: Adding more computed field strategies to deri",
      "full_content": "# Design Document\n\n## Overview\n\nLSI Field Enhancement Phase 4 is designed to significantly improve the field population rate in the LSI CSV output, addressing the current issue where 62.18% of fields are empty. This phase builds upon the previous enhancements and focuses on four key areas:\n\n1. **Enhanced LLM Field Completion**: Improving the quality and success rate of LLM-based field completion for subjective fields.\n2. **Expanded Computed Fields**: Adding more computed field strategies to derive values from existing metadata.\n3. **Comprehensive Default Values**: Providing intelligent default values at multiple levels (global, publisher, imprint).\n4. **Robust Field Mapping**: Enhancing field mapping strategies to handle all common field name variations.\n\nThe design aims to achieve 100% field population rate (up from current 37.82%) while ensuring high-quality data for all critical LSI fields, as required by publishers rushing books to market.\n\n## Architecture\n\nThe existing architecture will be maintained, with enhancements to key components:\n\n```\n\n                    LSI ACS Generator                        \n\n       \n     Field Mapper       Validator         Config      \n     - Strategy         - Rules           - Defaults  \n     - Transform        - Formats         - Overrides \n       \n\n       \n   Enhanced            File Manager       Reporter    \n   Metadata Model      - PDF Check        - Logging   \n   - LSI Fields        - FTP Stage        - Errors    \n       \n\n```\n\n### Enhanced LLM Field Completion\n\nThe LLM Field Completer will be enhanced to improve the quality and success rate of field completion, addressing the issue where only 2/12 LLM completions are showing up in metadata:\n\n```python\nclass EnhancedLLMFieldCompleter:\n    def __init__(self, model_name: str = \"gemini/gemini-2.5-flash\", \n                prompts_path: str = \"prompts/enhanced_lsi_field_completion_prompts.json\"):\n        # Initialize with improved prompts\n        \n    def complete_missing_fields(self, metadata: CodexMetadata, book_content: Optional[str] = None, \n                           save_to_disk: bool = True, output_dir: Optional[str] = None) -> CodexMetadata:\n        # Enhanced field completion with better error handling and fallbacks\n        \n    def _process_prompt(self, prompt_key: str, metadata: CodexMetadata, book_content: Optional[str] = None) -> Any:\n        # Improved prompt processing with retry logic and better context handling\n        \n    def _save_llm_completions(self, metadata: CodexMetadata, field_name: str, value: str) -> None:\n        # Save LLM completions to metadata BEFORE filtering via field mapping strategies\n        # This addresses the requirement to save all LLM completions\n        \n    def _provide_fallback_value(self, field_name: str, metadata: CodexMetadata) -> str:\n        # New method to provide intelligent fallback values when LLM completion fails\n```\n\n### Expanded Computed Fields\n\nNew computed field strategies will be added to derive values from existing metadata:\n\n```python\ndef _compute_territorial_pricing(metadata: CodexMetadata, context: MappingContext) -> str:\n    # Calculate territorial pricing based on US price and exchange rates\n    \ndef _compute_physical_specs(metadata: CodexMetadata, context: MappingContext) -> str:\n    # Calculate physical specifications based on page count and trim size\n    \ndef _compute_dates(metadata: CodexMetadata, context: MappingContext) -> str:\n    # Calculate dates based on available date information\n    \ndef _compute_file_paths(metadata: CodexMetadata, context: MappingContext) -> str:\n    # Calculate file paths based on ISBN and standard naming conventions\n```\n\n### Comprehensive Default Values\n\nThe configuration system will be enhanced to support multiple levels of default values:\n\n```python\nclass EnhancedLSIConfiguration:\n    def __init__(self, config_path: Optional[str] = None, \n                publisher_config_path: Optional[str] = None,\n                imprint_config_path: Optional[str] = None):\n        # Initialize with multiple configuration sources\n        \n    def get_default_value(self, field_name: str) -> str:\n        # Get default value with priority: imprint > publisher > global\n        \n    def get_field_override(self, field_name: str) -> Optional[str]:\n        # Get field override with priority: imprint > publisher > global\n```\n\n### Robust Field Mapping\n\nThe field mapping system will be enhanced to handle all common field name variations:\n\n```python\nclass EnhancedFieldMappingRegistry:\n    def __init__(self):\n        # Initialize with enhanced field name normalization\n        \n    def register_strategy(self, field_name: str, strategy: MappingStrategy) -> None:\n        # Register strategy with normalized field name\n        \n    def get_strategy(self, field_name: str) -> Optional[MappingStrategy]:\n        # Get strategy with normalized field name lookup\n        \n    def _normalize_field_name(self, field_name: str) -> str:\n        # Normalize field name for consistent lookup\n```\n\n## Components and Interfaces\n\n### Enhanced LLM Field Completion Prompts\n\nThe LLM field completion prompts will be enhanced to improve the quality and success rate of field completion:\n\n```json\n{\n  \"generate_contributor_bio\": {\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a professional book metadata specialist with expertise in creating contributor biographies for LSI submissions.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Generate a professional biography (100-150 words) for the author of this book:\\n\\nTitle: {title}\\nAuthor: {author}\\nPublisher: {publisher}\\nSummary: {summary_short}\\nBook Content: {book_content}\\n\\nThe biography should highlight the author's expertise, credentials, and background relevant to the book topic. Include academic affiliations, professional positions, and notable achievements if available in the content. Format the biography in third person and maintain a professional tone suitable for book metadata.\"\n      }\n    ],\n    \"params\": {\n      \"temperature\": 0.7,\n      \"max_tokens\": 300\n    },\n    \"fallback\": \"A respected expert in the field with extensive knowledge and experience related to {title}.\"\n  }\n}\n```\n\n### Expanded Computed Field Strategies\n\nNew computed field strategies will be implemented to derive values from existing metadata:\n\n```python\nclass TerritorialPricingStrategy(ComputedMappingStrategy):\n    def __init__(self, territory_code: str, exchange_rate: float, \n                 base_price_field: str = \"list_price_usd\"):\n        self.territory_code = territory_code\n        self.exchange_rate = exchange_rate\n        self.base_price_field = base_price_field\n        \n    def map_field(self, metadata: CodexMetadata, context: MappingContext) -> str:\n        # Calculate territorial pricing based on US price and exchange rate\n        \nclass PhysicalSpecsStrategy(ComputedMappingStrategy):\n    def __init__(self, spec_type: str):\n        self.spec_type = spec_type  # \"weight\", \"spine_width\", etc.\n        \n    def map_field(self, metadata: CodexMetadata, context: MappingContext) -> str:\n        # Calculate physical specifications based on page count and trim size\n        \nclass DateComputationStrategy(ComputedMappingStrategy):\n    def __init__(self, date_type: str, offset_days: int = 0):\n        self.date_type = date_type  # \"pub_date\", \"street_date\", etc.\n        self.offset_days = offset_days\n        \n    def map_field(self, metadata: CodexMetadata, context: MappingContext) -> str:\n        # Calculate dates based on available date information\n        \nclass FilePathStrategy(ComputedMappingStrategy):\n    def __init__(self, file_type: str):\n        self.file_type = file_type  # \"cover\", \"interior\", \"jacket\", etc.\n        \n    def map_field(self, metadata: CodexMetadata, context: MappingContext) -> str:\n        # Calculate file paths based on ISBN and standard naming conventions\n```\n\n### Enhanced Configuration System\n\nThe configuration system will be enhanced to support multiple levels of default values:\n\n```python\nclass ConfigurationLevel:\n    def __init__(self, config_data: Dict[str, Any]):\n        self.defaults = config_data.get(\"defaults\", {})\n        self.field_overrides = config_data.get(\"field_overrides\", {})\n        self.territorial_configs = config_data.get(\"territorial_configs\", {})\n        \nclass MultiLevelConfiguration:\n    def __init__(self):\n        self.levels = []\n        \n    def add_level(self, level: ConfigurationLevel, priority: int):\n        # Add configuration level with priority\n        \n    def get_value(self, section: str, key: str) -> Optional[str]:\n        # Get value with priority from highest to lowest\n```\n\n### Field Name Normalization\n\nA field name normalization system will be implemented to handle all common field name variations:\n\n```python\nclass FieldNameNormalizer:\n    def __init__(self):\n        self.normalization_rules = [\n            # Remove special characters\n            (r'[^\\w\\s]', ' '),\n            # Convert to lowercase\n            (lambda s: s.lower()),\n            # Remove extra whitespace\n            (r'\\s+', ' '),\n            # Remove common words\n            (r'\\b(the|a|an|of|in|for|to|with|by|at|from|on)\\b', ''),\n            # Trim whitespace\n            (lambda s: s.strip())\n        ]\n        \n    def normalize(self, field_name: str) -> str:\n        # Apply normalization rules\n        \n    def get_variations(self, field_name: str) -> List[str]:\n        # Generate common variations of field name\n```\n\n## Data Models\n\n### Enhanced CodexMetadata\n\nThe CodexMetadata class will be enhanced with additional fields and methods:\n\n```python\n@dataclass\nclass EnhancedCodexMetadata(CodexMetadata):\n    # Additional fields for LSI submission\n    \n    # Enhanced contributor information\n    contributor_one_bio: str = \"\"\n    contributor_one_affiliations: str = \"\"\n    contributor_one_professional_position: str = \"\"\n    contributor_one_location: str = \"\"\n    contributor_one_location_type_code: str = \"\"\n    contributor_one_prior_work: str = \"\"\n    \n    # Enhanced physical specifications\n    spine_width_in: float = 0.0\n    weight_lbs: str = \"\"\n    \n    # Enhanced pricing information\n    territorial_pricing: Dict[str, Dict[str, Any]] = field(default_factory=dict)\n    \n    # Enhanced file paths\n    jacket_path_filename: str = \"\"\n    interior_path_filename: str = \"\"\n    cover_path_filename: str = \"\"\n    \n    # LLM completion tracking - addressing the requirement to save all LLM completions\n    llm_completions: Dict[str, Any] = field(default_factory=dict)\n    llm_completion_attempts: Dict[str, int] = field(default_factory=dict)\n    llm_completion_failures: Dict[str, List[str]] = field(default_factory=dict)\n    \n    def compute_derived_fields(self):\n        # Compute derived fields based on existing metadata\n        \n    def get_field_population_stats(self) -> Dict[str, Any]:\n        # Get statistics on field population\n```\n\n### Enhanced Validation Result\n\nThe validation system will be enhanced to provide more detailed information:\n\n```python\n@dataclass\nclass EnhancedFieldValidationResult:\n    field_name: str\n    is_valid: bool\n    error_message: str = \"\"\n    warning_message: str = \"\"\n    suggested_value: str = \"\"\n    severity: str = \"info\"  # \"info\", \"warning\", \"error\", \"critical\"\n    \n@dataclass\nclass EnhancedValidationResult:\n    is_valid: bool\n    field_results: List[EnhancedFieldValidationResult]\n    errors: List[str]\n    warnings: List[str]\n    suggestions: List[str]\n    \n    def get_errors_by_severity(self, severity: str) -> List[str]:\n        # Get errors filtered by severity\n        \n    def has_critical_errors(self) -> bool:\n        # Check if there are any critical errors\n```\n\n## Error Handling\n\n### Enhanced Error Recovery\n\nThe error recovery system will be enhanced to provide better fallback values and recovery strategies:\n\n```python\nclass EnhancedErrorRecoveryManager:\n    def __init__(self, config: EnhancedLSIConfiguration):\n        self.config = config\n        self.recovery_strategies = {\n            \"contributor_bio\": self._recover_contributor_bio,\n            \"bisac_codes\": self._recover_bisac_codes,\n            \"territorial_pricing\": self._recover_territorial_pricing,\n            \"physical_specs\": self._recover_physical_specs,\n            \"dates\": self._recover_dates,\n            \"file_paths\": self._recover_file_paths\n        }\n        \n    def recover_field(self, field_name: str, metadata: CodexMetadata) -> str:\n        # Recover field value using appropriate strategy\n        \n    def _recover_contributor_bio(self, metadata: CodexMetadata) -> str:\n        # Generate a generic contributor bio based on available metadata\n        \n    def _recover_bisac_codes(self, metadata: CodexMetadata) -> str:\n        # Generate BISAC codes based on title and keywords\n        \n    def _recover_territorial_pricing(self, metadata: CodexMetadata) -> Dict[str, str]:\n        # Calculate territorial pricing based on US price\n        \n    def _recover_physical_specs(self, metadata: CodexMetadata) -> Dict[str, str]:\n        # Calculate physical specifications based on page count\n        \n    def _recover_dates(self, metadata: CodexMetadata) -> Dict[str, str]:\n        # Calculate dates based on available date information\n        \n    def _recover_file_paths(self, metadata: CodexMetadata) -> Dict[str, str]:\n        # Calculate file paths based on ISBN\n```\n\n### Enhanced Logging System\n\nThe logging system will be enhanced to provide high transparency and filterability, addressing Requirements 2 and 3:\n\n```python\nclass EnhancedLSILoggingManager:\n    def __init__(self, log_dir: str = \"logs/lsi_generation\", \n                 verbosity: str = \"normal\"):\n        self.log_dir = log_dir\n        self.verbosity = verbosity  # \"minimal\", \"normal\", \"detailed\"\n        self.session_id = f\"lsi_gen_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}\"\n        self.field_mapping_log = []\n        self.validation_log = []\n        self.error_log = []\n        self.performance_log = []\n        \n    def set_verbosity(self, verbosity: str):\n        # Set logging verbosity level\n        # \"minimal\" - only warnings, errors, and major decisions (for systems operators)\n        # \"normal\" - errors, warnings, and important info\n        # \"detailed\" - all events including debug information (for programmers)\n        self.verbosity = verbosity\n        \n    def log_field_mapping(self, field_name: str, strategy_name: str, value: str, source: str, severity: str = \"info\"):\n        # Log field mapping details if verbosity level allows\n        if self._should_log(severity):\n            self.field_mapping_log.append({\n                \"timestamp\": datetime.now().isoformat(),\n                \"event_type\": \"field_mapping\",\n                \"field_name\": field_name,\n                \"strategy_name\": strategy_name,\n                \"value\": value,\n                \"source\": source,\n                \"severity\": severity\n            })\n        \n    def log_validation(self, field_name: str, is_valid: bool, message: str, severity: str):\n        # Log validation details if verbosity level allows\n        if self._should_log(severity):\n            self.validation_log.append({\n                \"timestamp\": datetime.now().isoformat(),\n                \"event_type\": \"validation\",\n                \"field_name\": field_name,\n                \"is_valid\": is_valid,\n                \"message\": message,\n                \"severity\": severity\n            })\n        \n    def log_error(self, error_type: str, message: str, field_name: str = None, severity: str = \"error\"):\n        # Log error details if verbosity level allows\n        if self._should_log(severity):\n            self.error_log.append({\n                \"timestamp\": datetime.now().isoformat(),\n                \"event_type\": \"error\",\n                \"error_type\": error_type,\n                \"message\": message,\n                \"field_name\": field_name,\n                \"severity\": severity\n            })\n        \n    def log_performance(self, operation: str, duration_ms: float, severity: str = \"info\"):\n        # Log performance details if verbosity level allows\n        if self._should_log(severity):\n            self.performance_log.append({\n                \"timestamp\": datetime.now().isoformat(),\n                \"event_type\": \"performance\",\n                \"operation\": operation,\n                \"duration_ms\": duration_ms,\n                \"severity\": severity\n            })\n        \n    def _should_log(self, severity: str) -> bool:\n        # Determine if event should be logged based on verbosity level\n        if self.verbosity == \"detailed\":\n            return True\n        elif self.verbosity == \"normal\":\n            return severity in [\"info\", \"warning\", \"error\", \"critical\"]\n        else:  # minimal - for systems operators\n            return severity in [\"warning\", \"error\", \"critical\", \"major_decision\"]\n        \n    def get_filtered_log(self, severity_filter: Optional[List[str]] = None) -> Dict[str, List[Dict[str, Any]]]:\n        # Get filtered log based on severity\n        severity_filter = severity_filter or [\"info\", \"warning\", \"error\", \"critical\"]\n        \n        return {\n            \"field_mapping\": [entry for entry in self.field_mapping_log if entry[\"severity\"] in severity_filter],\n            \"validation\": [entry for entry in self.validation_log if entry[\"severity\"] in severity_filter],\n            \"error\": [entry for entry in self.error_log if entry[\"severity\"] in severity_filter],\n            \"performance\": [entry for entry in self.performance_log if entry[\"severity\"] in severity_filter]\n        }\n        \n    def save_session_log(self):\n        # Save session log to file\n        os.makedirs(os.path.join(self.log_dir, \"logs\"), exist_ok=True)\n        log_path = os.path.join(self.log_dir, \"logs\", f\"{self.session_id}.json\")\n        \n        with open(log_path, \"w\") as f:\n            json.dump({\n                \"session_id\": self.session_id,\n                \"timestamp\": datetime.now().isoformat(),\n                \"verbosity\": self.verbosity,\n                \"field_mapping_log\": self.field_mapping_log,\n                \"validation_log\": self.validation_log,\n                \"error_log\": self.error_log,\n                \"performance_log\": self.performance_log\n            }, f, indent=2)\n            \n        return log_path\n```\n\n## Comprehensive Reporting System\n\nA comprehensive reporting system will be implemented, with a focus on HTML reports as specified in Requirement 4:\n\n```python\nclass EnhancedLSIFieldReportGenerator:\n    def __init__(self, output_dir: str = \"output\"):\n        self.output_dir = output_dir\n        \n    def generate_report(self, metadata: CodexMetadata, csv_path: str, log_path: str) -> Dict[str, Any]:\n        # Generate comprehensive field population report\n        field_stats = metadata.get_field_population_stats()\n        \n        # Load log data\n        with open(log_path, \"r\") as f:\n            log_data = json.load(f)\n            \n        # Generate report data\n        report_data = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"csv_path\": csv_path,\n            \"field_population_rate\": field_stats[\"population_rate\"],\n            \"total_fields\": field_stats[\"total_fields\"],\n            \"populated_fields\": field_stats[\"populated_fields\"],\n            \"empty_fields\": field_stats[\"empty_fields\"],\n            \"field_details\": field_stats[\"field_details\"],\n            \"low_population_fields\": [\n                field for field, details in field_stats[\"field_details\"].items()\n                if not details[\"is_populated\"]\n            ],\n            \"validation_errors\": [\n                entry for entry in log_data[\"validation_log\"]\n                if not entry[\"is_valid\"] and entry[\"severity\"] in [\"error\", \"critical\"]\n            ],\n            \"llm_completion_failures\": metadata.llm_completion_failures\n        }\n        \n        # Generate recommendations\n        report_data[\"recommendations\"] = self._generate_recommendations(report_data)\n        \n        # Save report - only HTML by default, as specified in Requirement 4\n        self._save_report(report_data, formats=[\"html\"])\n        \n        return report_data\n        \n    def _generate_recommendations(self, report_data: Dict[str, Any]) -> List[str]:\n        # Generate recommendations for improving field population rates\n        recommendations = []\n        \n        # Check for low population fields\n        if len(report_data[\"low_population_fields\"]) > 0:\n            recommendations.append(\n                f\"Consider adding default values for {len(report_data['low_population_fields'])} empty fields.\"\n            )\n            \n        # Check for LLM completion failures\n        if len(report_data[\"llm_completion_failures\"]) > 0:\n            recommendations.append(\n                f\"Review LLM prompts for {len(report_data['llm_completion_failures'])} fields with completion failures.\"\n            )\n            \n        # Check for validation errors\n        if len(report_data[\"validation_errors\"]) > 0:\n            recommendations.append(\n                f\"Fix {len(report_data['validation_errors'])} validation errors to improve field quality.\"\n            )\n            \n        # General recommendations\n        if report_data[\"field_population_rate\"] < 100:\n            recommendations.append(\n                \"Consider adding more computed field strategies to derive values from existing metadata.\"\n            )\n            \n        return recommendations\n        \n    def _save_report(self, report_data: Dict[str, Any], formats=[\"html\"]):\n        # Save report in specified formats\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        \n        # Save HTML report (default)\n        if \"html\" in formats:\n            html_report_path = os.path.join(self.output_dir, f\"field_report__{timestamp}.html\")\n            self._save_html_report(report_data, html_report_path)\n            \n        # Save CSV report (optional)\n        if \"csv\" in formats:\n            csv_report_path = os.path.join(self.output_dir, f\"field_report__{timestamp}.csv\")\n            self._save_csv_report(report_data, csv_report_path)\n            \n        # Save JSON report (optional)\n        if \"json\" in formats:\n            json_report_path = os.path.join(self.output_dir, f\"field_report__{timestamp}.json\")\n            self._save_json_report(report_data, json_report_path)\n        \n        return {\n            \"html_report_path\": html_report_path if \"html\" in formats else None,\n            \"csv_report_path\": csv_report_path if \"csv\" in formats else None,\n            \"json_report_path\": json_report_path if \"json\" in formats else None\n        }\n        \n    def _save_csv_report(self, report_data: Dict[str, Any], path: str):\n        # Save report as CSV\n        with open(path, \"w\", newline=\"\") as f:\n            writer = csv.writer(f)\n            writer.writerow([\"Field Name\", \"Populated\", \"Value\", \"Source\", \"Validation\"])\n            \n            for field, details in report_data[\"field_details\"].items():\n                writer.writerow([\n                    field,\n                    \"Yes\" if details[\"is_populated\"] else \"No\",\n                    details.get(\"value\", \"\"),\n                    details.get(\"source\", \"\"),\n                    details.get(\"validation_status\", \"\")\n                ])\n                \n    def _save_html_report(self, report_data: Dict[str, Any], path: str):\n        # Save report as HTML with visualizations\n        # Implementation details omitted for brevity\n        pass\n        \n    def _save_json_report(self, report_data: Dict[str, Any], path: str):\n        # Save report as JSON\n        with open(path, \"w\") as f:\n            json.dump(report_data, f, indent=2)\n```\n\n## Testing Strategy\n\n### Unit Testing\n\n1. **Enhanced LLM Field Completion Tests**\n   - Test improved prompts with sample book content\n   - Test fallback value generation\n   - Test retry logic and error handling\n   - Test saving of all LLM completions to metadata\n\n2. **Expanded Computed Fields Tests**\n   - Test territorial pricing calculation\n   - Test physical specifications calculation\n   - Test date calculation\n   - Test file path generation\n\n3. **Comprehensive Default Values Tests**\n   - Test multi-level configuration\n   - Test default value resolution with different priorities\n   - Test field override resolution\n\n4. **Robust Field Mapping Tests**\n   - Test field name normalization\n   - Test field name variation handling\n   - Test field mapping with different CSV templates\n\n5. **Enhanced Logging Tests**\n   - Test different verbosity levels\n   - Test severity filtering\n   - Test structured log format\n\n6. **Comprehensive Reporting Tests**\n   - Test report generation with different field population rates\n   - Test recommendation generation\n   - Test HTML report format\n\n### Integration Testing\n\n1. **End-to-End Generation Tests**\n   - Test complete metadata  CSV generation\n   - Test with various metadata completeness levels\n   - Test with different imprint configurations\n   - **Test with rows 1-12 of xynapse_traces_schedule.json** (Requirement 5)\n\n2. **Performance Testing**\n   - Test field completion performance\n   - Test CSV generation performance\n   - Test with large datasets\n\n3. **Logging and Reporting Integration Tests**\n   - Test logging during CSV generation\n   - Test report generation after CSV generation\n   - Test log filtering and analysis\n\n### Validation Testing\n\n1. **LSI Compliance Tests**\n   - Test generated CSV against LSI template\n   - Test field formatting and validation\n   - Test with LSI sample data\n\n2. **Error Recovery Tests**\n   - Test recovery from LLM completion failures\n   - Test recovery from validation failures\n   - Test recovery from configuration issues\n\n## Implementation Phases\n\n### Phase 1: Enhanced LLM Field Completion\n- Improve LLM field completion prompts\n- Add retry logic and better error handling\n- Implement intelligent fallback values\n- **Ensure all LLM completions are saved to metadata** (Requirement 4)\n\n### Phase 2: Expanded Computed Fields\n- Implement territorial pricing calculation\n- Implement physical specifications calculation\n- Implement date calculation\n- Implement file path generation\n\n### Phase 3: Comprehensive Default Values\n- Implement multi-level configuration\n- Add imprint-specific default values\n- Add publisher-specific default values\n- Enhance global default values\n\n### Phase 4: Robust Field Mapping\n- Implement field name normalization\n- Add support for field name variations\n- Enhance field mapping registry\n\n### Phase 5: Enhanced Logging and Reporting\n- Implement configurable logging system\n- Add severity-based filtering\n- Create comprehensive HTML reporting system\n\n### Phase 6: Integration and Testing\n- Integrate all components\n- Comprehensive testing\n- Performance optimization\n- **Test with rows 1-12 of xynapse_traces_schedule.json** (Requirement 5)",
      "size": 27499,
      "modified": "2025-07-19T01:06:33.863912",
      "spec_directory": "lsi-field-enhancement-phase4",
      "directory_modified": "2025-07-19T17:23:37.888292"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThe Xynapse Tranche 1 design addresses 15 specific punch list items to ensure successful LSI metadata generation for 12 xynapse traces books. The design focuses on targeted fixes for each identified issue rather than broad architectural changes, leveraging the existing LSI field enhancement infrastructure.\n\n## Architecture\n\n### Punch List Processing Pipeline\n\n```mermaid\ngraph TB\n    A[12 Book Metadata] --> B[Punch List Processor]\n    B --> C[Lightning Source Accou",
      "full_content": "# Design Document\n\n## Overview\n\nThe Xynapse Tranche 1 design addresses 15 specific punch list items to ensure successful LSI metadata generation for 12 xynapse traces books. The design focuses on targeted fixes for each identified issue rather than broad architectural changes, leveraging the existing LSI field enhancement infrastructure.\n\n## Architecture\n\n### Punch List Processing Pipeline\n\n```mermaid\ngraph TB\n    A[12 Book Metadata] --> B[Punch List Processor]\n    B --> C[Lightning Source Account Fix]\n    B --> D[ISBN Database & Assignment]\n    B --> E[Tranche Configuration]\n    B --> F[Field Validation Engine]\n    B --> G[Smart Date Assignment]\n    B --> H[Content Enhancement]\n    B --> I[BISAC Subject Processing]\n    B --> J[Length Validation]\n    B --> K[Thema Subject Correction]\n    B --> L[GC Pricing Standardization]\n    \n    C --> M[LSI CSV Generator]\n    D --> M\n    E --> M\n    F --> M\n    G --> M\n    H --> M\n    I --> M\n    J --> M\n    K --> M\n    L --> M\n    \n    M --> N[Production LSI Files]\n```\n\n## Components and Interfaces\n\n### 1. Lightning Source Account Manager\n\n**Purpose**: Handle Lightning Source account configuration and field exclusions\n\n**Key Features**:\n- Set Lightning Source Account # to \"6024045\" for all books\n- Exclude \"Metadata Contact Dictionary\" from CSV output\n- Ensure Parent ISBN remains empty for this tranche\n\n**Implementation**:\n```python\nclass LightningSourceAccountManager:\n    def set_account_number(self, metadata: CodexMetadata) -> None\n    def exclude_metadata_contact_dictionary(self, csv_headers: List[str]) -> List[str]\n    def clear_parent_isbn(self, metadata: CodexMetadata) -> None\n```\n\n### 2. ISBN Database Manager\n\n**Purpose**: Initialize and manage ISBN database with real data\n\n**Key Features**:\n- Load ISBN database from real data source\n- Assign unique ISBNs to each book in tranche\n- Track ISBN assignment status\n- Prevent duplicate ISBN assignments\n\n**Implementation**:\n```python\nclass ISBNDatabaseManager:\n    def initialize_database(self, data_source: str) -> None\n    def assign_unique_isbn(self, book_id: str) -> str\n    def mark_isbn_assigned(self, isbn: str) -> None\n    def validate_isbn_availability(self, isbn: str) -> bool\n```\n\n### 3. Tranche Configuration Manager\n\n**Purpose**: Apply consistent configuration across all books in the tranche\n\n**Key Features**:\n- Load tranche-level configuration settings\n- Override individual book settings with tranche settings\n- Ensure consistency across all 12 books\n- Handle configuration conflicts with tranche priority\n\n**Implementation**:\n```python\nclass TrancheConfigurationManager:\n    def load_tranche_config(self, tranche_id: str) -> TrancheConfig\n    def apply_tranche_settings(self, metadata: CodexMetadata, config: TrancheConfig) -> CodexMetadata\n    def validate_consistency(self, metadata_list: List[CodexMetadata]) -> ValidationResult\n```\n\n### 4. Field Validation Engine\n\n**Purpose**: Validate fields against LSI valid value lists\n\n**Key Features**:\n- Validate rendition booktype against lsi_valid_rendition_booktypes.txt\n- Validate Contributor Role One against lsi_valid_contributor_codes.csv\n- Provide specific error messages with valid options\n- Support dynamic validation list updates\n\n**Implementation**:\n```python\nclass FieldValidationEngine:\n    def load_valid_rendition_booktypes(self) -> List[str]\n    def load_valid_contributor_codes(self) -> List[str]\n    def validate_rendition_booktype(self, booktype: str) -> ValidationResult\n    def validate_contributor_role(self, role: str) -> ValidationResult\n```\n\n### 5. Smart Publication Date Assigner\n\n**Purpose**: Intelligently assign publication dates across Tuesdays\n\n**Key Features**:\n- Extract month/year from schedule.json\n- Identify all Tuesdays in target month\n- Distribute 12 books evenly across available Tuesdays\n- Avoid date conflicts and ensure proper spacing\n\n**Implementation**:\n```python\nclass SmartPublicationDateAssigner:\n    def extract_schedule_info(self, schedule_path: str) -> ScheduleInfo\n    def find_tuesdays_in_month(self, year: int, month: int) -> List[datetime]\n    def distribute_books_across_dates(self, books: List[CodexMetadata], dates: List[datetime]) -> Dict[str, datetime]\n```\n\n### 6. Enhanced Content Generator\n\n**Purpose**: Generate enhanced annotations combining LLM results with boilerplate\n\n**Key Features**:\n- Combine LLM completion results with configuration boilerplate\n- Source boilerplate strings from configuration dictionary\n- Ensure proper formatting and content flow\n- Handle content length limits appropriately\n\n**Implementation**:\n```python\nclass EnhancedContentGenerator:\n    def load_boilerplate_dictionary(self, config_path: str) -> Dict[str, str]\n    def combine_llm_and_boilerplate(self, llm_result: str, boilerplate: Dict[str, str]) -> str\n    def format_annotation_content(self, content: str) -> str\n```\n\n### 7. BISAC Subject Processor\n\n**Purpose**: Handle BISAC subject formatting and tranche-level overrides\n\n**Key Features**:\n- Format BISAC subjects as category names only (no codes)\n- Apply consistent formatting to BISAC Subject, Subject 2, and Subject 3\n- Support tranche-level BISAC subject overrides\n- Validate BISAC category names against valid lists\n\n**Implementation**:\n```python\nclass BISACSubjectProcessor:\n    def format_bisac_subject(self, subject: str) -> str\n    def apply_tranche_bisac_override(self, metadata: CodexMetadata, override: str) -> CodexMetadata\n    def validate_bisac_category_name(self, category: str) -> bool\n```\n\n### 8. Content Length Validator\n\n**Purpose**: Validate and enforce content length limits\n\n**Key Features**:\n- Validate short description is  350 bytes\n- Truncate content while preserving meaning\n- Log original and truncated content for audit\n- Report length violations in validation reports\n\n**Implementation**:\n```python\nclass ContentLengthValidator:\n    def validate_short_description_length(self, description: str) -> ValidationResult\n    def truncate_preserving_meaning(self, content: str, max_bytes: int) -> str\n    def log_truncation(self, original: str, truncated: str) -> None\n```\n\n### 9. Thema Subject Corrector\n\n**Purpose**: Correct truncated Thema subjects to full multi-letter codes\n\n**Key Features**:\n- Identify truncated single-letter Thema codes\n- Restore full multi-letter Thema subject codes\n- Validate corrected codes against Thema classification\n- Handle multiple Thema subjects consistently\n\n**Implementation**:\n```python\nclass ThemaSubjectCorrector:\n    def identify_truncated_codes(self, thema_code: str) -> bool\n    def restore_full_thema_code(self, truncated_code: str) -> str\n    def validate_thema_code(self, code: str) -> bool\n```\n\n### 10. GC Market Pricing Standardizer\n\n**Purpose**: Standardize GC market pricing to match US List Price\n\n**Key Features**:\n- Identify all GC market price fields\n- Set GC market prices equal to US List Price\n- Ensure consistency across all GC markets\n- Validate pricing consistency in final output\n\n**Implementation**:\n```python\nclass GCMarketPricingStandardizer:\n    def identify_gc_market_fields(self, headers: List[str]) -> List[str]\n    def standardize_gc_pricing(self, metadata: CodexMetadata, us_price: str) -> CodexMetadata\n    def validate_gc_pricing_consistency(self, metadata: CodexMetadata) -> ValidationResult\n```\n\n## Data Models\n\n### Punch List Item\n```python\n@dataclass\nclass PunchListItem:\n    id: int\n    description: str\n    component: str\n    priority: int\n    validation_rule: Optional[str]\n    fix_applied: bool = False\n```\n\n### Tranche Configuration\n```python\n@dataclass\nclass TrancheConfig:\n    tranche_id: str\n    lightning_source_account: str\n    exclude_fields: List[str]\n    bisac_subject_override: Optional[str]\n    boilerplate_dictionary: Dict[str, str]\n    publication_month: int\n    publication_year: int\n```\n\n### Validation Result\n```python\n@dataclass\nclass ValidationResult:\n    field_name: str\n    is_valid: bool\n    error_message: Optional[str]\n    suggested_values: List[str]\n    punch_list_item: int\n```\n\n## Error Handling\n\n### Punch List Item Failures\n- **Account Configuration Errors**: Clear error messages with configuration file references\n- **ISBN Assignment Failures**: Detailed logging with available ISBN counts\n- **Validation Failures**: Specific error messages with valid value lists\n- **Date Assignment Conflicts**: Alternative date suggestions with conflict resolution\n\n### Recovery Strategies\n- **Partial Processing**: Continue with remaining punch list items when individual items fail\n- **Fallback Values**: Use safe default values when preferred values are invalid\n- **Manual Intervention**: Clear indicators when human review is required for specific items\n\n## Testing Strategy\n\n### Punch List Item Testing\n- Test each of the 15 punch list items independently\n- Verify fixes are applied correctly and consistently\n- Test error handling for each punch list item\n- Validate integration between related punch list items\n\n### End-to-End Testing\n- Process all 12 books with all punch list fixes applied\n- Verify LSI CSV output meets all requirements\n- Test batch processing consistency\n- Validate final output against LSI specifications\n\n## Implementation Priority\n\n### Phase 1: Critical Infrastructure (Items 1-5)\n1. Lightning Source Account # configuration\n2. Metadata Contact Dictionary exclusion\n3. Parent ISBN handling\n4. ISBN database initialization and assignment\n5. Tranche configuration system\n\n### Phase 2: Validation and Content (Items 6-10)\n6. Rendition booktype validation\n7. Contributor role validation\n8. Smart publication date assignment\n9. Enhanced annotation generation\n10. BISAC subject processing\n\n### Phase 3: Content Quality (Items 11-15)\n11. BISAC subject formatting\n12. Tranche BISAC overrides\n13. Short description length validation\n14. Thema subject correction\n15. GC market pricing standardization",
      "size": 9827,
      "modified": "2025-07-19T15:15:02.825416",
      "spec_directory": "xynapse-tranche-1",
      "directory_modified": "2025-07-20T15:23:30.266950"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThis design addresses the bug in `imprints/xynapse_traces/prepress.py` where the system incorrectly counts `\\textbf` LaTeX commands. The current implementation uses `mnemonics_tex.count('\\n\\\\textbf')` which matches any `\\textbf` command that appears after a newline, regardless of whether there are other characters before it on the same line.\n\nThe fix involves replacing the simple string counting approach with a regex-based solution that specifically matches `\\text",
      "full_content": "# Design Document\n\n## Overview\n\nThis design addresses the bug in `imprints/xynapse_traces/prepress.py` where the system incorrectly counts `\\textbf` LaTeX commands. The current implementation uses `mnemonics_tex.count('\\n\\\\textbf')` which matches any `\\textbf` command that appears after a newline, regardless of whether there are other characters before it on the same line.\n\nThe fix involves replacing the simple string counting approach with a regex-based solution that specifically matches `\\textbf` commands at the beginning of lines.\n\n## Architecture\n\nThe solution will be implemented as a targeted fix within the existing `prepress.py` file structure. No new modules or significant architectural changes are required.\n\n### Current Implementation\n```python\nmnemonics_count = mnemonics_tex.count('\\n\\\\textbf')\n```\n\n### Proposed Implementation\n```python\nimport re\n# Count \\textbf commands that appear at the beginning of lines\nmnemonics_count = len(re.findall(r'^\\\\textbf', mnemonics_tex, re.MULTILINE))\n```\n\n## Components and Interfaces\n\n### Modified Component\n- **File**: `imprints/xynapse_traces/prepress.py`\n- **Function**: The function that processes mnemonics LaTeX content (around line 343)\n- **Change**: Replace string counting with regex pattern matching\n\n### Regex Pattern Analysis\n- **Pattern**: `r'^\\\\textbf'`\n- **Flags**: `re.MULTILINE` to treat each line as a potential match start\n- **Explanation**: \n  - `^` matches the beginning of a line (when MULTILINE flag is used)\n  - `\\\\textbf` matches the literal string `\\textbf` (double backslash because of Python string escaping)\n\n## Data Models\n\nNo changes to data models are required. The function will continue to:\n- Input: `mnemonics_tex` string containing LaTeX content\n- Output: `mnemonics_count` integer representing the count of line-beginning `\\textbf` commands\n\n## Error Handling\n\nThe regex approach is more robust than string counting:\n- If `mnemonics_tex` is None or empty, `re.findall()` will return an empty list, and `len()` will return 0\n- Invalid regex patterns would raise exceptions during development/testing, not at runtime with this simple pattern\n- The existing error handling in the surrounding code remains unchanged\n\n## Testing Strategy\n\n### Unit Testing Approach\n1. **Test Cases for Regex Pattern**:\n   - `\\textbf` at beginning of line (should count)\n   - `\\textbf` with leading whitespace (should not count)\n   - `\\textbf` in middle of line (should not count)\n   - Multiple `\\textbf` commands on same line (should count only if first is at line start)\n   - Empty string input (should return 0)\n   - String with no `\\textbf` commands (should return 0)\n\n2. **Integration Testing**:\n   - Test with actual mnemonics LaTeX content\n   - Verify that practice page generation uses the correct count\n   - Ensure logging output reflects accurate count\n\n### Test Data Examples\n```python\n# Should count 2\ntest_content_1 = \"\"\"\\\\textbf{First mnemonic}\nSome content here\n\\\\textbf{Second mnemonic}\n    \\\\textbf{This should not count - has leading spaces}\"\"\"\n\n# Should count 1  \ntest_content_2 = \"\"\"Some intro text\n\\\\textbf{Only mnemonic}\nMore content with \\\\textbf{inline bold} text\"\"\"\n\n# Should count 0\ntest_content_3 = \"\"\"No mnemonics here\nJust regular content\n    \\\\textbf{This has leading spaces}\"\"\"\n```\n\n## Implementation Notes\n\n### Import Requirements\nThe `re` module needs to be imported at the top of the file if not already present.\n\n### Backward Compatibility\nThis change maintains full backward compatibility:\n- Same input parameters\n- Same output type and meaning\n- Same integration with surrounding code\n- Only the counting logic changes\n\n### Performance Considerations\n- Regex matching is slightly more expensive than string counting\n- For typical mnemonics content size, performance impact is negligible\n- The accuracy improvement far outweighs the minimal performance cost",
      "size": 3865,
      "modified": "2025-07-20T18:37:00.836332",
      "spec_directory": "textbf-line-detection-fix",
      "directory_modified": "2025-07-20T19:51:45.228220"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThis design implements a specialized mnemonic practice layout system that places each mnemonic on a verso (left-hand) page followed by a corresponding recto (right-hand) dot grid page for practice. The system builds upon the existing LaTeX template infrastructure and dot grid generation capabilities while introducing new layout commands and processing logic to ensure proper page sequencing.\n\n## Architecture\n\nThe mnemonic practice layout system consists of three ma",
      "full_content": "# Design Document\n\n## Overview\n\nThis design implements a specialized mnemonic practice layout system that places each mnemonic on a verso (left-hand) page followed by a corresponding recto (right-hand) dot grid page for practice. The system builds upon the existing LaTeX template infrastructure and dot grid generation capabilities while introducing new layout commands and processing logic to ensure proper page sequencing.\n\n## Architecture\n\nThe mnemonic practice layout system consists of three main components:\n\n1. **LaTeX Template Extensions**: New commands for mnemonic-practice page pairs\n2. **Prepress Processing Logic**: Enhanced mnemonic processing to generate alternating layouts\n3. **Page Sequencing System**: Logic to ensure proper verso/recto page alignment\n\n### Component Integration\n\n```\nBook Data (JSON)  Prepress Processor  LaTeX Generation  PDF Output\n                      \n                 Mnemonic Detection\n                      \n                 Practice Layout Generation\n                      \n                 Page Sequencing\n```\n\n## Components and Interfaces\n\n### 1. LaTeX Template Commands\n\n#### New Commands to Add\n\n```latex\n% Command for mnemonic on verso page with automatic recto practice page\n\\newcommand{\\mnemonicwithpractice}[2]{%\n    \\cleartoverso  % Ensure we start on verso (left) page\n    \\formattedquote{#1}  % Display the mnemonic content\n    \\newpage  % Move to recto (right) page\n    \\fullpagedotgridwithinstruction{#2}  % Practice page with instruction\n}\n\n% Enhanced dot grid command with bottom instruction\n\\newcommand{\\fullpagedotgridwithinstruction}[1]{%\n    \\thispagestyle{empty}\n    \\vfill\n    \\begin{center}\n        \\includegraphics[height=0.75\\textheight,keepaspectratio]{dotgrid.png}\n    \\end{center}\n    \\vfill\n    \\begin{center}\n        \\small\\sffamily #1\n    \\end{center}\n    \\vspace{0.5in}\n}\n\n% Counter for practice page numbering\n\\newcounter{mnemonicpractice}\n\\setcounter{mnemonicpractice}{0}\n```\n\n#### Modified Existing Commands\n\nThe existing `\\formattedquote` command will be used for mnemonic display, and the dot grid system will be enhanced to support instruction text at the bottom.\n\n### 2. Prepress Processing Enhancement\n\n#### Current Processing Flow\n```python\n# Current: mnemonics_tex  count \\textbf  add practice pages after all mnemonics\nmnemonics_parts = [full_page_grid_command_def]\nmnemonics_parts.append(mnemonics_tex)\n# ... count and add practice pages at end\n```\n\n#### New Processing Flow\n```python\n# New: Parse individual mnemonics  create alternating verso/recto pairs\ndef process_mnemonic_practice_layout(mnemonics_tex):\n    # Parse individual mnemonic entries\n    mnemonic_entries = extract_individual_mnemonics(mnemonics_tex)\n    \n    layout_parts = []\n    for i, mnemonic in enumerate(mnemonic_entries, 1):\n        # Create verso/recto pair for each mnemonic\n        instruction = f\"Mnemonic Practice {i}\"\n        layout_parts.append(f\"\\\\mnemonicwithpractice{{{mnemonic}}}{{{instruction}}}\")\n    \n    return layout_parts\n```\n\n#### Mnemonic Extraction Logic\n\n```python\ndef extract_individual_mnemonics(mnemonics_tex):\n    \"\"\"\n    Extract individual mnemonic entries from LaTeX content.\n    Assumes each mnemonic starts with \\textbf at line beginning.\n    \"\"\"\n    import re\n    \n    # Split on line-beginning \\textbf commands\n    pattern = r'^\\\\textbf'\n    parts = re.split(pattern, mnemonics_tex, flags=re.MULTILINE)\n    \n    mnemonics = []\n    for i, part in enumerate(parts[1:], 1):  # Skip first empty part\n        # Reconstruct the mnemonic with its \\textbf command\n        mnemonic_content = f\"\\\\textbf{part.strip()}\"\n        mnemonics.append(mnemonic_content)\n    \n    return mnemonics\n```\n\n### 3. Page Sequencing System\n\n#### Verso/Recto Alignment\n\nThe system uses LaTeX's `\\cleartoverso` command to ensure each mnemonic starts on a verso (left-hand) page, followed by the practice page on the recto (right-hand) page.\n\n```latex\n\\cleartoverso  % Forces next content to verso page\n% Mnemonic content here\n\\newpage       % Moves to recto page\n% Practice page here\n```\n\n#### Page Style Management\n\nPractice pages use `\\thispagestyle{empty}` to remove headers/footers, while mnemonic pages maintain the standard page style.\n\n## Data Models\n\n### Input Data Structure\n\nThe system expects mnemonic data in the existing format:\n\n```json\n{\n  \"mnemonics_tex\": \"\\\\textbf{First Mnemonic}\\nContent here...\\n\\n\\\\textbf{Second Mnemonic}\\nMore content...\"\n}\n```\n\n### Processing Data Structure\n\n```python\nclass MnemonicEntry:\n    content: str          # LaTeX content for the mnemonic\n    practice_number: int  # Sequential practice page number\n    \nclass MnemonicLayout:\n    entries: List[MnemonicEntry]\n    total_pages: int      # Total pages (mnemonics + practice pages)\n```\n\n## Error Handling\n\n### Mnemonic Parsing Errors\n\n- **Empty Content**: If `mnemonics_tex` is empty, system generates no mnemonic pages\n- **Malformed LaTeX**: Invalid LaTeX content is passed through with warning logs\n- **Missing \\textbf**: Content without proper mnemonic markers is treated as single mnemonic\n\n### Page Layout Errors\n\n- **Odd Page Count**: System ensures even number of pages by adding blank page if needed\n- **Missing Dot Grid**: Fallback text displayed if `dotgrid.png` not found\n- **Font Issues**: Graceful degradation to default fonts if custom fonts unavailable\n\n### File System Errors\n\n- **Build Directory**: Creates necessary directories if they don't exist\n- **File Permissions**: Logs errors and continues with available content\n- **Template Missing**: Falls back to basic layout if enhanced templates unavailable\n\n## Testing Strategy\n\n### Unit Tests\n\n1. **Mnemonic Extraction Tests**\n   ```python\n   def test_extract_individual_mnemonics():\n       # Test with multiple mnemonics\n       # Test with single mnemonic\n       # Test with empty content\n       # Test with malformed content\n   ```\n\n2. **Layout Generation Tests**\n   ```python\n   def test_mnemonic_practice_layout():\n       # Test verso/recto pairing\n       # Test practice numbering\n       # Test page count calculations\n   ```\n\n### Integration Tests\n\n1. **End-to-End Processing**\n   - Test complete pipeline from JSON input to LaTeX output\n   - Verify correct page sequencing in generated PDF\n   - Test with various mnemonic counts (1, 5, 10+ mnemonics)\n\n2. **LaTeX Compilation Tests**\n   - Verify generated LaTeX compiles without errors\n   - Test with different font configurations\n   - Validate page layout in compiled PDF\n\n### Visual Verification Tests\n\n1. **Page Layout Verification**\n   - Manual inspection of generated PDFs\n   - Verify verso/recto alignment\n   - Check instruction placement at page bottom\n   - Validate dot grid scaling and positioning\n\n## Implementation Notes\n\n### Backward Compatibility\n\nThe new system maintains compatibility with existing mnemonic processing:\n- Falls back to current behavior if new layout commands unavailable\n- Preserves existing `mnemonics_tex` data format\n- Maintains current prompt system for mnemonic generation\n\n### Performance Considerations\n\n- Regex parsing adds minimal overhead compared to LaTeX compilation\n- Page count calculations are O(n) where n is number of mnemonics\n- Memory usage scales linearly with mnemonic content size\n\n### Configuration Options\n\nFuture enhancement could include configuration for:\n- Practice page instruction format\n- Dot grid density and appearance\n- Page style customization\n- Alternative practice page layouts",
      "size": 7441,
      "modified": "2025-07-20T20:00:09.178212",
      "spec_directory": "mnemonic-practice-layout",
      "directory_modified": "2025-07-22T02:31:54.566794"
    },
    {
      "file": "design.md",
      "content": "# Cover Fixes Design Document\n\n## Overview\n\nThis design addresses two critical issues in the cover generation system: back cover text variable substitution and front cover Korean text formatting. The solution involves enhancing the cover generator's text processing pipeline to properly handle template variables and LaTeX command escaping.\n\n## Architecture\n\nThe cover generation system follows this flow:\n1. **Data Loading**: Book metadata is loaded from CSV/JSON\n2. **Template Processing**: LaTeX t",
      "full_content": "# Cover Fixes Design Document\n\n## Overview\n\nThis design addresses two critical issues in the cover generation system: back cover text variable substitution and front cover Korean text formatting. The solution involves enhancing the cover generator's text processing pipeline to properly handle template variables and LaTeX command escaping.\n\n## Architecture\n\nThe cover generation system follows this flow:\n1. **Data Loading**: Book metadata is loaded from CSV/JSON\n2. **Template Processing**: LaTeX template is populated with book data\n3. **Text Processing**: Variables are substituted and text is escaped for LaTeX\n4. **Compilation**: LaTeX is compiled to PDF\n5. **Conversion**: PDF is converted to PNG images\n\nThe fixes will be implemented in the **Text Processing** stage.\n\n## Components and Interfaces\n\n### Enhanced Variable Substitution System\n\n**Location**: `src/codexes/modules/covers/cover_generator.py`\n\n**New Function**: `substitute_template_variables(text: str, data: Dict[str, Any]) -> str`\n- Input: Raw text with template variables, book metadata\n- Output: Text with all variables substituted\n- Handles: {stream}, {title}, {description}, {quotes_per_book}, etc.\n\n**Enhanced Function**: `_escape_latex(text: str) -> str`\n- Current: Basic LaTeX character escaping\n- Enhancement: Preserve \\korean{} commands while escaping other content\n- Strategy: Use placeholder system to protect LaTeX commands during escaping\n\n### Korean Text Processing Pipeline\n\n**Problem**: The current system either:\n1. Over-escapes Korean text, showing \\korean{} in output, OR\n2. Under-escapes Korean text, causing LaTeX compilation errors\n\n**Solution**: Selective escaping system\n1. Identify \\korean{} commands in text\n2. Extract and preserve them with placeholders\n3. Escape the remaining text normally\n4. Restore \\korean{} commands after escaping\n\n## Data Models\n\n### Template Variable Mapping\n```python\nTEMPLATE_VARIABLES = {\n    'stream': lambda data: data.get('subject', data.get('title', 'this topic')),\n    'title': lambda data: data.get('title', 'Untitled'),\n    'description': lambda data: data.get('description', data.get('storefront_publishers_note_en', 'this important topic')),\n    'quotes_per_book': lambda data: str(data.get('quotes_per_book', '90'))\n}\n```\n\n### Korean Text Pattern\n```python\nKOREAN_COMMAND_PATTERN = r'\\\\korean\\{[^}]+\\}'\n```\n\n## Error Handling\n\n### Variable Substitution Errors\n- **Missing Variables**: Use fallback values instead of leaving placeholders\n- **Circular References**: Detect and prevent infinite substitution loops\n- **Invalid Data Types**: Convert all values to strings safely\n\n### Korean Text Processing Errors\n- **Malformed Commands**: Log warnings but continue processing\n- **Font Issues**: Fallback to basic text if Korean font unavailable\n- **Encoding Problems**: Ensure UTF-8 handling throughout pipeline\n\n## Testing Strategy\n\n### Unit Tests\n1. **Variable Substitution Tests**\n   - Test each variable type individually\n   - Test multiple variables in same text\n   - Test missing/invalid data scenarios\n   - Test circular reference prevention\n\n2. **Korean Text Processing Tests**\n   - Test \\korean{} command preservation\n   - Test mixed Korean/English text\n   - Test malformed command handling\n   - Test escaping around Korean commands\n\n### Integration Tests\n1. **End-to-End Cover Generation**\n   - Generate cover with all variable types\n   - Verify Korean text renders correctly\n   - Check LaTeX compilation success\n   - Validate final PNG output\n\n### Visual Verification Tests\n1. **Back Cover Content**\n   - Verify no {variable} placeholders remain\n   - Check text makes grammatical sense\n   - Validate all substitutions occurred\n\n2. **Front Cover Korean Text**\n   - Verify no visible \\korean{} commands\n   - Check Korean characters display properly\n   - Validate font rendering",
      "size": 3819,
      "modified": "2025-07-28T00:27:14.914097",
      "spec_directory": "cover-fixes",
      "directory_modified": "2025-07-28T00:27:39.502680"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThis design implements LLM-based back cover text generation by integrating it into Stage 1 (LLM Content Generation) of the book pipeline. Instead of using basic string substitution during cover generation, the system will generate clean, final back cover text during the initial content generation phase when all book metadata is available and other LLM calls are already being made.\n\n## Architecture\n\nThe system integrates into the existing book pipeline workflow:\n\n`",
      "full_content": "# Design Document\n\n## Overview\n\nThis design implements LLM-based back cover text generation by integrating it into Stage 1 (LLM Content Generation) of the book pipeline. Instead of using basic string substitution during cover generation, the system will generate clean, final back cover text during the initial content generation phase when all book metadata is available and other LLM calls are already being made.\n\n## Architecture\n\nThe system integrates into the existing book pipeline workflow:\n\n```\nStage 1: LLM Content Generation\n Load book metadata and generate quotes\n Process other metadata fields (title, description, etc.)\n **NEW: Generate final back cover text**\n    Use book metadata and quotes context\n    Call LLM with back cover prompt\n    Store clean, final text in book data\n    No variables or substitutions needed\n Save processed data to JSON\n Continue to Stage 2...\n\nStage 3: Cover Generation\n Load book metadata from JSON\n Use pre-generated back_cover_text (no processing needed)\n Apply LaTeX escaping and formatting\n Generate cover PDF\n```\n\nThis approach eliminates the need for variable substitution during cover generation since the text is already finalized.\n\n## Components and Interfaces\n\n### 1. Stage 1 Content Generation Enhancement\n\n**Location:** `src/codexes/modules/builders/llm_get_book_data.py`\n\nThe existing content generation process will be enhanced to include back cover text generation:\n\n```python\ndef generate_back_cover_text(book_data: Dict[str, Any], quotes: List[Dict]) -> str:\n    \"\"\"Generate final back cover text using LLM with full book context.\"\"\"\n    # Use existing LLM infrastructure to generate clean, final text\n    # No variables or substitutions - just clean, ready-to-use text\n```\n\n**Integration Points:**\n- Called after quote generation when full context is available\n- Uses existing `call_model_with_prompt()` infrastructure\n- Stores result in `book_data['back_cover_text']` for later use\n\n### 2. Cover Generator Simplification\n\n**Location:** `src/codexes/modules/covers/cover_generator.py`\n\nThe existing `create_cover_latex()` function will be simplified:\n- Remove `substitute_template_variables()` function call\n- Use `data.get('back_cover_text', '')` directly (already processed)\n- Apply existing LaTeX escaping to the final text\n\n### 3. Enhanced Prompt Configuration\n\n**Location:** `prompts/prompts.json`\n\nThe existing `back_cover_text` prompt will be enhanced to generate final text directly:\n\n```json\n{\n  \"back_cover_text\": {\n    \"params\": {\n      \"temperature\": 0.7,\n      \"max_tokens\": 500\n    },\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a professional book marketing copywriter. Generate engaging back cover text that flows naturally and is ready for publication.\"\n      },\n      {\n        \"role\": \"user\", \n        \"content\": \"Generate back cover text for '{title}' by {author} in the {stream} domain. The book contains {quotes_per_book} quotations with bibliography and verification log. Description: {description}. Return clean, final text with no variables or placeholders.\"\n      }\n    ]\n  }\n}\n```\n\n## Data Models\n\n### Stage 1 Input Data\n```python\n{\n    \"book_data\": {\n        \"title\": str,\n        \"author\": str,\n        \"imprint\": str,\n        \"subject\": str,  # Stream/topic\n        \"description\": str,\n        \"quotes_per_book\": int,\n        \"storefront_publishers_note_en\": str,\n        # ... other metadata\n    },\n    \"generated_quotes\": List[Dict],  # Available for context\n}\n```\n\n### LLM Response Structure\n```python\n{\n    \"back_cover_text\": str  # Clean, final text ready for cover placement\n}\n```\n\n### Updated Book Data Structure\nAfter Stage 1 processing, the book data will contain:\n```python\n{\n    # ... existing fields\n    \"back_cover_text\": str,  # Final, processed text (no variables)\n    # ... other fields\n}\n```\n\n## Error Handling\n\n### LLM Generation Errors\n1. **Rate Limiting:** Use existing retry logic in `call_model_with_prompt()`\n2. **Invalid Response:** Validate JSON response format and retry if malformed\n3. **Timeout:** Use existing timeout handling in LLM caller\n4. **Service Unavailable:** Retry with exponential backoff per existing patterns\n\n### Fallback Strategy\nWhen back cover text generation fails during Stage 1:\n1. Log the failure with full context\n2. Generate a simple fallback text using book metadata\n3. Store fallback text in `back_cover_text` field\n4. Continue with pipeline processing\n5. Add warning to processing log for manual review\n\n### Validation Rules\n- Response must contain `back_cover_text` field\n- Text length must be reasonable (50-200 words)\n- Text must end with complete sentence\n- No template variables or placeholders remaining\n- Text must be suitable for LaTeX processing\n\n## Testing Strategy\n\n### Unit Tests\n- `test_back_cover_text_processor.py`: Test the processor class in isolation\n- Mock LLM responses to test various scenarios\n- Test fallback behavior when LLM fails\n- Validate response parsing and error handling\n\n### Integration Tests\n- `test_cover_generation_with_llm.py`: Test full cover generation pipeline\n- Test with real book data and LLM calls\n- Verify LaTeX compilation with processed text\n- Test performance under various load conditions\n\n### Test Data\n- Sample back cover templates with various variable combinations\n- Mock book metadata covering different genres and formats\n- Edge cases: missing metadata, malformed templates, special characters\n\n## Implementation Phases\n\n### Phase 1: Stage 1 Integration\n- Add back cover text generation to `llm_get_book_data.py`\n- Enhance existing `back_cover_text` prompt in `prompts.json`\n- Implement LLM call using existing infrastructure\n- Add validation and fallback logic\n\n### Phase 2: Cover Generator Simplification\n- Remove variable substitution from `cover_generator.py`\n- Update to use pre-generated `back_cover_text` directly\n- Remove unused `substitute_template_variables()` function\n- Test cover generation with new approach\n\n### Phase 3: Testing and Validation\n- Test full pipeline from Stage 1 through cover generation\n- Validate text quality and LaTeX compatibility\n- Add monitoring for back cover text generation success rates\n- Fine-tune prompt for optimal results\n\n## Configuration Options\n\n### LLM Settings\n```json\n{\n  \"back_cover_llm_config\": {\n    \"model\": \"gemini/gemini-2.5-flash\",\n    \"max_retries\": 3,\n    \"timeout_seconds\": 30,\n    \"temperature\": 0.7,\n    \"max_tokens\": 500\n  }\n}\n```\n\n### Processing Options\n```json\n{\n  \"back_cover_processing\": {\n    \"enable_llm_processing\": true,\n    \"fallback_on_failure\": true,\n    \"cache_responses\": true,\n    \"max_text_length\": 200,\n    \"required_variables\": [\"title\", \"author\", \"stream\"]\n  }\n}\n```\n\n## Performance Considerations\n\n### Caching Strategy\n- Cache processed text based on template + metadata hash\n- Use file-based cache with TTL for development\n- Consider Redis for production environments\n\n### Cost Optimization\n- Use faster, cheaper models for simple substitutions\n- Implement request batching where possible\n- Monitor token usage and implement alerts\n\n### Latency Management\n- Async processing where cover generation allows\n- Parallel processing of multiple covers\n- Precompute common templates during off-peak hours",
      "size": 7262,
      "modified": "2025-07-28T03:55:01.745142",
      "spec_directory": "llm-back-cover-text-processing",
      "directory_modified": "2025-07-28T07:17:57.229868"
    },
    {
      "file": "design.md",
      "content": "# LSI CSV Generator Project - Design Document\n\n## Overview\n\nThe LSI CSV Generator is a comprehensive system for transforming book metadata into Lightning Source Inc. (LSI) compliant CSV files. The system leverages AI-powered field completion, robust validation, and flexible configuration management to produce high-quality distribution-ready files.\n\n## Architecture\n\n### High-Level Architecture\n\n```\n        \n   Input Layer         Process",
      "full_content": "# LSI CSV Generator Project - Design Document\n\n## Overview\n\nThe LSI CSV Generator is a comprehensive system for transforming book metadata into Lightning Source Inc. (LSI) compliant CSV files. The system leverages AI-powered field completion, robust validation, and flexible configuration management to produce high-quality distribution-ready files.\n\n## Architecture\n\n### High-Level Architecture\n\n```\n        \n   Input Layer         Processing           Output Layer   \n                          Layer                            \n  JSON Metadata   Field Mapping   LSI CSV Files \n  Book Content        AI Completion       Validation    \n  Configuration       Validation           Reports       \n        \n```\n\n### Core Components\n\n#### 1. Metadata Processing Engine\n- **Purpose**: Transform raw book metadata into structured LSI format\n- **Key Classes**: `MetadataProcessor`, `FieldMapper`, `DataTransformer`\n- **Responsibilities**:\n  - Parse input metadata from various formats\n  - Apply field mapping rules\n  - Handle data type conversions\n  - Manage field dependencies\n\n#### 2. AI Field Completion System\n- **Purpose**: Generate missing metadata fields using LLM\n- **Key Classes**: `AIFieldCompleter`, `PromptManager`, `ResponseValidator`\n- **Responsibilities**:\n  - Identify missing or incomplete fields\n  - Generate contextually appropriate content\n  - Validate AI-generated responses\n  - Provide fallback mechanisms\n\n#### 3. Validation Framework\n- **Purpose**: Ensure LSI compliance and data quality\n- **Key Classes**: `LSIValidator`, `FieldValidator`, `FormatChecker`\n- **Responsibilities**:\n  - Validate field formats and lengths\n  - Check required field completeness\n  - Verify character encoding\n  - Generate validation reports\n\n#### 4. Configuration Management\n- **Purpose**: Handle flexible system configuration\n- **Key Classes**: `ConfigManager`, `PublisherConfig`, `ImrintConfig`\n- **Responsibilities**:\n  - Load and validate configuration files\n  - Apply publisher/imprint-specific settings\n  - Manage field mapping rules\n  - Handle environment-specific configurations\n\n#### 5. Batch Processing Engine\n- **Purpose**: Handle multiple books efficiently\n- **Key Classes**: `BatchProcessor`, `JobManager`, `ProgressTracker`\n- **Responsibilities**:\n  - Coordinate multi-book processing\n  - Manage resource allocation\n  - Track processing progress\n  - Handle error recovery\n\n#### 6. Reporting System\n- **Purpose**: Generate comprehensive processing reports\n- **Key Classes**: `ReportGenerator`, `AnalyticsEngine`, `OutputFormatter`\n- **Responsibilities**:\n  - Generate completion statistics\n  - Create validation reports\n  - Produce analytics dashboards\n  - Export reports in multiple formats\n\n## Components and Interfaces\n\n### Core Interfaces\n\n#### IMetadataProcessor\n```python\nclass IMetadataProcessor:\n    def process_metadata(self, metadata: Dict[str, Any]) -> ProcessedMetadata:\n        \"\"\"Process raw metadata into structured format\"\"\"\n        pass\n    \n    def validate_input(self, metadata: Dict[str, Any]) -> ValidationResult:\n        \"\"\"Validate input metadata structure\"\"\"\n        pass\n```\n\n#### IAIFieldCompleter\n```python\nclass IAIFieldCompleter:\n    def complete_fields(self, metadata: ProcessedMetadata, \n                       missing_fields: List[str]) -> CompletionResult:\n        \"\"\"Complete missing fields using AI\"\"\"\n        pass\n    \n    def get_completion_confidence(self, field: str, value: str) -> float:\n        \"\"\"Get confidence score for AI-generated content\"\"\"\n        pass\n```\n\n#### ILSIValidator\n```python\nclass ILSIValidator:\n    def validate_csv(self, csv_data: str) -> ValidationResult:\n        \"\"\"Validate complete CSV file\"\"\"\n        pass\n    \n    def validate_field(self, field_name: str, value: str) -> FieldValidationResult:\n        \"\"\"Validate individual field\"\"\"\n        pass\n```\n\n### Data Models\n\n#### ProcessedMetadata\n```python\n@dataclass\nclass ProcessedMetadata:\n    title: str\n    author: str\n    isbn13: str\n    publisher: str\n    publication_date: datetime\n    page_count: int\n    # ... additional LSI fields\n    \n    def to_lsi_dict(self) -> Dict[str, str]:\n        \"\"\"Convert to LSI field format\"\"\"\n        pass\n```\n\n#### ValidationResult\n```python\n@dataclass\nclass ValidationResult:\n    is_valid: bool\n    errors: List[ValidationError]\n    warnings: List[ValidationWarning]\n    field_completeness: Dict[str, bool]\n    \n    def get_summary(self) -> str:\n        \"\"\"Get human-readable validation summary\"\"\"\n        pass\n```\n\n#### CompletionResult\n```python\n@dataclass\nclass CompletionResult:\n    completed_fields: Dict[str, str]\n    confidence_scores: Dict[str, float]\n    fallback_used: Dict[str, bool]\n    processing_time: float\n    \n    def get_quality_score(self) -> float:\n        \"\"\"Calculate overall completion quality\"\"\"\n        pass\n```\n\n## Data Models\n\n### LSI Field Schema\n\nThe system supports all ~119 LSI fields organized into categories:\n\n#### Basic Information\n- `title`, `subtitle`, `author_first_name`, `author_last_name`\n- `isbn13`, `publisher`, `imprint`, `publication_date`\n\n#### Physical Specifications\n- `binding_type`, `page_count`, `trim_size_width`, `trim_size_height`\n- `paper_color`, `paper_weight`, `cover_finish`\n\n#### Content Description\n- `bisac_category_1`, `bisac_category_2`, `bisac_category_3`\n- `keywords`, `short_description`, `long_description`\n- `audience`, `min_age`, `max_age`\n\n#### Distribution Settings\n- `list_price`, `discount_code`, `territory_rights`\n- `return_policy`, `availability_date`\n\n#### Marketing Information\n- `series_name`, `series_number`, `volume_number`\n- `edition_number`, `edition_description`\n\n### Configuration Schema\n\n#### Publisher Configuration\n```yaml\npublisher:\n  name: \"Publisher Name\"\n  default_settings:\n    territory_rights: \"World\"\n    return_policy: \"Standard\"\n    discount_code: \"45\"\n  \n  field_mappings:\n    author: \"contributor_one_last_name\"\n    description: \"long_description\"\n  \n  ai_completion:\n    enabled: true\n    model: \"gemini/gemini-2.5-flash\"\n    confidence_threshold: 0.7\n```\n\n#### Imprint Configuration\n```yaml\nimprint:\n  name: \"Imprint Name\"\n  parent_publisher: \"Publisher Name\"\n  \n  overrides:\n    imprint_name: \"Custom Imprint\"\n    publisher_city: \"New York\"\n  \n  field_exclusions:\n    - \"series_name\"\n    - \"volume_number\"\n```\n\n## Error Handling\n\n### Error Categories\n\n1. **Input Errors**: Invalid metadata format, missing required fields\n2. **Processing Errors**: Field mapping failures, data conversion issues\n3. **AI Errors**: LLM failures, invalid AI responses, rate limiting\n4. **Validation Errors**: LSI compliance failures, format violations\n5. **Output Errors**: File writing failures, permission issues\n\n### Error Recovery Strategies\n\n1. **Graceful Degradation**: Continue processing with available data\n2. **Fallback Values**: Use default values for missing fields\n3. **Retry Logic**: Retry failed AI completions with backoff\n4. **Partial Success**: Generate CSV with warnings for incomplete data\n5. **Error Reporting**: Detailed logs and user-friendly error messages\n\n## Testing Strategy\n\n### Unit Testing\n- Test individual field processors\n- Validate AI completion logic\n- Test configuration loading\n- Verify validation rules\n\n### Integration Testing\n- Test end-to-end CSV generation\n- Validate batch processing workflows\n- Test configuration inheritance\n- Verify error handling paths\n\n### Performance Testing\n- Benchmark single book processing\n- Test batch processing scalability\n- Measure AI completion performance\n- Validate memory usage patterns\n\n### Validation Testing\n- Test with real LSI submissions\n- Validate against LSI specification\n- Test edge cases and error conditions\n- Verify output format compliance\n\n## Security Considerations\n\n### Data Protection\n- Sanitize all input data\n- Validate file paths and names\n- Protect against injection attacks\n- Secure temporary file handling\n\n### API Security\n- Rate limiting for AI services\n- Secure credential management\n- Input validation and sanitization\n- Error message sanitization\n\n### Configuration Security\n- Validate configuration files\n- Secure default settings\n- Protect sensitive configuration data\n- Audit configuration changes\n\n## Performance Optimization\n\n### Processing Efficiency\n- Lazy loading of large datasets\n- Efficient field mapping algorithms\n- Optimized validation routines\n- Memory-efficient batch processing\n\n### AI Optimization\n- Response caching for repeated queries\n- Batch AI requests where possible\n- Intelligent retry strategies\n- Model selection based on field type\n\n### I/O Optimization\n- Streaming CSV generation\n- Efficient file handling\n- Parallel processing where appropriate\n- Optimized logging and reporting\n\n## Deployment Architecture\n\n### Standalone Deployment\n- Command-line interface\n- Configuration file management\n- Local file processing\n- Integrated reporting\n\n### Service Deployment\n- REST API interface\n- Database integration\n- Queue-based processing\n- Web-based reporting dashboard\n\n### Integration Deployment\n- Library/module integration\n- Plugin architecture\n- Event-driven processing\n- Programmatic API access",
      "size": 9239,
      "modified": "2025-07-28T14:16:19.003230",
      "spec_directory": "lsi-csv-generator-project",
      "directory_modified": "2025-07-28T14:16:23.003675"
    },
    {
      "file": "known-issues.md",
      "content": "# LSI CSV Generation - Known Issues\n\n## Overview\n\nThis document tracks specific, reproducible issues in the LSI CSV generation system. Each issue includes reproduction steps, error details, and current status.\n\n## Critical Issues (Blocking Production)\n\n### ISSUE-001: Bibliography Prompt JSON Parsing Error\n**Status**:  Resolved  \n**Priority**: Critical  \n**Component**: LLM Field Completion  \n**Discovered**: 2025-07-28  \n**Resolved**: 2025-07-28  \n\n**Description**: Bibliography prompt returns con",
      "full_content": "# LSI CSV Generation - Known Issues\n\n## Overview\n\nThis document tracks specific, reproducible issues in the LSI CSV generation system. Each issue includes reproduction steps, error details, and current status.\n\n## Critical Issues (Blocking Production)\n\n### ISSUE-001: Bibliography Prompt JSON Parsing Error\n**Status**:  Resolved  \n**Priority**: Critical  \n**Component**: LLM Field Completion  \n**Discovered**: 2025-07-28  \n**Resolved**: 2025-07-28  \n\n**Description**: Bibliography prompt returns conversational text instead of JSON, causing parsing failures.\n\n**Error Message**:\n```\n2025-07-28 07:00:53,785 - ERROR - codexes.core.llm_caller._parse_llm_response:31 - Failed to decode JSON from response: Expecting value: line 1 column 1 (char 0). Raw content: I apologize, but the quotes containing the source citations were not provided. To create the bibliography, I need the text of the quotes so I can identify and extract the cited sources. Please provide...\n```\n\n**Root Cause**: Old-style prompt format without proper JSON enforcement\n\n**Solution**: \n- Converted bibliography_prompt to modern messages format\n- Added explicit JSON enforcement in system message\n- Implemented robust JSON parsing with multiple fallback strategies\n\n**Fix Status**:  Resolved - Prompt modernized and JSON parsing enhanced\n\n---\n\n### ISSUE-002: Old-Style Prompts Causing JSON Failures\n**Status**:  Resolved  \n**Priority**: Critical  \n**Component**: Prompt System  \n**Discovered**: 2025-07-28  \n**Resolved**: 2025-07-28  \n\n**Description**: Multiple prompts using old \"prompt\" format instead of \"messages\" format, causing inconsistent JSON responses.\n\n**Affected Prompts**:\n- `bibliography_prompt`\n- `gemini_get_basic_info`\n- `bibliographic_key_phrases`\n- `custom_transcription_note_prompt`\n- `storefront_get_*` prompts\n\n**Solution**:\n- Created prompt modernization utility\n- Converted all old-style prompts to messages format\n- Added JSON enforcement to all system messages\n- Implemented comprehensive testing framework\n\n**Fix Status**:  Resolved - All prompts converted to modern format\n\n---\n\n## High Priority Issues\n\n### ISSUE-003: BISAC Code Validation Failures\n**Status**:  Open  \n**Priority**: High  \n**Component**: Field Validation  \n\n**Description**: Generated BISAC codes often invalid or outdated, causing LSI submission rejections.\n\n**Examples of Invalid Codes**:\n- `BUS999999` (non-existent category)\n- `ABC123456` (invalid format)\n- Outdated codes from previous BISAC versions\n\n**Impact**: LSI submissions rejected due to invalid subject classification\n\n**Reproduction Steps**:\n1. Generate LSI CSV with AI field completion\n2. Check BISAC codes against current BISAC database\n3. Many codes fail validation\n\n---\n### ISSUE-015: One BISAC Category Per BISAC Column\n**Status**:  Open  \n**Priority**: Medium  \n**Component**: Field Validation\n\n**Description**: Each of the three BISAC fields -- BISAC Category, BISAC Category 2, and BISAC Category 3 -- shall have exactly one BISAC category name.\n\n### ISSUE-004: Description Length Violations\n**Status**:  Open  \n**Priority**: High  \n**Component**: Field Completion  \n\n**Description**: Generated descriptions exceed LSI character limits, causing truncation or rejection.\n\n**Field Limits**:\n- `short_description`: 350 characters\n- `long_description`: 4000 characters\n- `annotation`: 4000 characters\n\n**Current Behavior**: AI generates text without length constraints, leading to truncated submissions\n\n---\n### ISSUE-016: Price Calculations Missing\n**Status**:  Open  \n**Priority**: High  \n**Component**: Field Completion  \n\n---\n### ISSUE-017: Prices Are Two-Decimal Floats\n**Status**:  Open  \n**Priority**: Medium  \n**Component**: Field Completion  \n\n**Description**: all prices, including USD and non-USD prices, should be two-decimal floats with no currency sign, just the numeral.\n---\n### ISSUE-018: Some Calculated Prices Are Missing\n**Status**:  Open  \n**Priority**: Medium  \n**Component**: Field Completion  \n\n**Description**: EU, CA, and AU prices are missing and should be calculated based on method that includes exchange rate, wiggle room, and optional special fees.\n\n### ISSUE-019: Some Replicated Prices Are Missing\n**Status**:  Open  \n**Priority**: Medium  \n**Component**: Field Completion  \n\n**Description**: Prices for the following fields are missing and should be the same as the US List Price.\nUAEUSD Suggested List Price (mode 2)\nUSBR1 Suggested List Price (mode 2)\nUSCN1 Suggested List Price (mode 2)\nUSDE1 Suggested List Price (mode 2)\nUSIN1 Suggested List Price (mode 2)\nUSJP2 Suggested List Price(mode 2)\nUSKR1 Suggested List Price (mode 2)\nUSPL1 Suggested List Price (mode 2)\nUSRU1 Suggested List Price (mode 2)\n\n### ISSUE-005: Configuration Inheritance Not Working\n**Status**:  Open  \n**Priority**: High  \n**Component**: Configuration System  \n\n**Description**: Multi-level configuration (default  publisher  imprint  tranche) not applying correctly.\n\n**Symptoms**:\n- Tranche-specific settings ignored\n- Default values not loading\n- Publisher overrides not applying\n\n**Impact**: Books processed with incorrect settings, affecting metadata quality\n\n---\n\n### ISSUE-012: [Calculated Spine Should Trump Configs]\n**Status**:  Open /  In Progress /  Resolved  \n**Priority**: Critical / High / Medium / Low  \n**Component**: [System Component]  \n**Discovered**: [Date]  \n**Assigned**: [Developer]  \n\n**Description**: Calculated spine width should always override configured. There is no value to setting configured spine width since there must always be a calculation using actual rendition type and page count.\n\n### ISSUE-014: File paths\n**Status**:  Open  \n**Priority**: Medium  \n**Component**: Batch Processing  \n\n**Description**: Values for interior, cover and file paths should be filenames exactly matching the file names of the final deliverable artifacts for each.\n\n### ISSUE-006: Batch Processing Memory Leaks\n**Status**:  Open  \n**Priority**: Medium  \n**Component**: Batch Processing  \n\n**Description**: Memory usage increases during large batch processing, eventually causing system slowdown or crashes.\n\n**Reproduction**:\n1. Process 50+ books in batch mode\n2. Monitor memory usage over time\n3. Memory usage grows without being released\n\n**Impact**: Cannot process large catalogs reliably\n\n---\n\n### ISSUE-007: Contributor Bio Generation Quality\n**Status**:  Open  \n**Priority**: Medium  \n**Component**: Field Completion  \n\n**Description**: Generated contributor biographies are generic and don't reflect book content or author expertise.\n\n**Current Output Example**:\n```\n\"A respected expert in the field with extensive knowledge and experience.\"\n```\n\n**Desired Output**: Contextual bio based on book content and subject matter for newly encountered authors for this imprint, OR defined standard bio for repeated authors, such as AI Lab For Book-Lovers\n\n---\n\n### ISSUE-008: Age Range Validation Inconsistencies\n**Status**:  Open  \n**Priority**: Medium  \n**Component**: Field Validation  \n\n**Description**: Age range fields sometimes contain invalid values or inconsistent ranges.\n\n**Invalid Examples**:\n- `min_age: \"Adult\"` (should be numeric)\n- `max_age: \"5\"` with `min_age: \"18\"` (inverted range)\n- Non-standard age values\n\n---\n\n### ISSUE-013: Blank Contributor Roles ShouldMatch Blank Contributor names\n**Status**:  Open  \n**Priority**: Medium  \n**Component**: Field Validation  \n\n**Description**: 1) Sometimes with blank Contributor Two or Three, the codes for Contributor Role Two and Contributor Role Three are populated.\n\n**Desired Output**: if Contributor Two or Three are blank, the corresponding Contributor Two and Three Roles should also be blank.\n\n---\n---\n\n### ISSUE-013: Contributor Roles Should Match Interior of Book, Be Accurate, and Be Valid\n\n**Status**:  Open  \n**Priority**: Medium  \n**Component**: Field Validation  \n\n**Description**: 1) Sometimes with blank Contributor Two or Three, the codes for Contributor Role Two and Contributor Role Three are populated.\n\n**Desired Output**: if Contributor Two or Three are blank, the corresponding Contributor Two and Three Roles should also be blank.\n\n\n## Low Priority Issues\n\n### ISSUE-009: Verbose Logging Performance Impact\n**Status**:  Open  \n**Priority**: Low  \n**Component**: Logging System  \n\n**Description**: Verbose logging mode significantly slows down processing due to excessive I/O.\n\n**Impact**: Development and debugging workflows are slower than necessary\n\n---\n\n### ISSUE-010: Error Messages Not User-Friendly\n**Status**:  Open  \n**Priority**: Medium\n**Component**: Error Handling  \n\n**Description**: Technical error messages shown to users without context or suggested fixes.\n\n**Example**:\n```\nAttributeError: 'NoneType' object has no attribute 'get'\n```\n\n**Desired**: \"Missing required field 'title' in book metadata. Please check your input file.\"\n\n---\n\n### ISSUE-020: Fields Always Blank\n**Status**:  Open  \n**Priority**: Low  \n**Component**: Field Completion  \n\n**Description**: The following fields should always be blank in current xynapse_traces workflow:\n\nReserved 1\nReserved 2\nReserved 3\nReserved 4\nCustom Trim Width (inches)\nCustom Trim Height (inches)\nWeight(Lbs)\nReserved5\nReserved6\nReserved7\nReserved8\nInterior Path / Filename\nCover Path / Filename\nAnnotation / Summary\nReserved (Special Instructions)\nLSI Special Category  (please consult LSI before using\nStamped Text LEFT\nStamped Text CENTER\nStamped Text RIGHT\nLSI FlexField1 (please consult LSI before using)\nLSI FlexField2 (please consult LSI before using)\nLSI FlexField3 (please consult LSI before using)\nLSI FlexField4 (please consult LSI before using)\nLSI FlexField5 (please consult LSI before using)\nReserved11\nReserved12\nReserved9\nReserved10\n\n## Resolved Issues\n\n### ISSUE-R001: JSON Response Validation Enhanced\n**Status**:  Resolved  \n**Priority**: Critical  \n**Component**: LLM Response Processing  \n**Resolved**: 2025-07-28  \n\n**Description**: Enhanced JSON parsing with multiple fallback strategies to handle malformed LLM responses.\n\n**Solution**: \n- Implemented 4-tier JSON parsing strategy\n- Added conversational response parsing\n- Enhanced error logging with fallback indicators\n- Added JSON extraction from markdown/mixed content\n\n### ISSUE-R002: BISAC Code Validation System\n**Status**:  Resolved  \n**Priority**: High  \n**Component**: Field Validation  \n**Resolved**: 2025-07-28  \n\n**Description**: Created comprehensive BISAC code validation and suggestion system.\n\n**Solution**:\n- Built current BISAC codes database (2024 standards)\n- Implemented format validation and correction suggestions\n- Added keyword-based BISAC code suggestions\n- Created fallback codes for failed generation\n\n### ISSUE-R003: Text Formatting and Length Validation\n**Status**:  Resolved  \n**Priority**: High  \n**Component**: Field Completion  \n**Resolved**: 2025-07-28  \n\n**Description**: Enhanced text formatting with intelligent truncation and validation.\n\n**Solution**:\n- Created LSI text formatter with field-specific limits\n- Implemented intelligent truncation at sentence boundaries\n- Added HTML annotation formatting\n- Enhanced keyword formatting and deduplication\n\n### ISSUE-R004: Multi-Level Configuration System\n**Status**:  Resolved  \n**Priority**: High  \n**Component**: Configuration System  \n**Resolved**: 2025-07-28  \n\n**Description**: Fixed configuration inheritance and loading issues.\n\n**Solution**:\n- Fixed configuration loading order and context tracking\n- Implemented proper inheritance hierarchy\n- Added configuration debugging utilities\n- Enhanced error handling for missing config files\n\n### ISSUE-R005: Batch Processing Error Isolation\n**Status**:  Resolved  \n**Priority**: High  \n**Component**: Batch Processing  \n**Resolved**: 2025-07-28  \n\n**Description**: Enhanced batch processing with error isolation and recovery.\n\n**Solution**:\n- Implemented error isolation for individual book processing\n- Added comprehensive batch processing statistics\n- Enhanced error reporting and logging\n- Maintained CSV structure even with failed books\n\n### ISSUE-R006: Enhanced LLM Retry Logic\n**Status**:  Resolved  \n**Priority**: Medium  \n**Component**: LLM Integration  \n**Resolved**: 2025-07-28  \n\n**Description**: Improved LLM call reliability with enhanced retry mechanisms.\n\n**Solution**:\n- Added intelligent retry logic for different error types\n- Implemented exponential backoff with maximum delay limits\n- Enhanced error classification for retryable vs non-retryable errors\n- Added network and timeout error handling\n\n---\n\n## Issue Tracking Template\n\n### ISSUE-XXX: [Issue Title]\n**Status**:  Open /  In Progress /  Resolved  \n**Priority**: Critical / High / Medium / Low  \n**Component**: [System Component]  \n**Discovered**: [Date]  \n**Assigned**: [Developer]  \n\n**Description**: [Detailed description of the issue]\n\n**Error Message** (if applicable):\n```\n[Exact error message or log output]\n```\n\n**Reproduction Steps**:\n1. [Step 1]\n2. [Step 2]\n3. [Expected vs Actual behavior]\n\n**Root Cause**: [Analysis of why this happens]\n\n**Impact**: [Business/technical impact]\n\n**Fix Status**: [Current status and any attempted solutions]\n\n**Related Issues**: [Links to related issues]\n\n---\n\n## Status Legend\n\n-  **Open**: Issue identified, not yet started\n-  **In Progress**: Actively being worked on\n-  **Resolved**: Issue fixed and verified\n-  **Blocked**: Cannot proceed due to dependencies\n-  **Testing**: Fix implemented, undergoing testing\n\n## Priority Definitions\n\n- **Critical**: Blocks production use, causes system failures\n- **High**: Significantly impacts functionality or data quality\n- **Medium**: Affects user experience or system efficiency\n- **Low**: Minor issues, cosmetic problems, or nice-to-have improvements",
      "size": 13684,
      "modified": "2025-07-28T19:54:39.530065",
      "spec_directory": "lsi-csv-bug-fixes",
      "directory_modified": "2025-07-28T22:26:09.548904"
    },
    {
      "file": "design.md",
      "content": "# LSI CSV Bug Fixes Project - Design Document\n\n## Overview\n\nThis project addresses critical bugs and issues in the existing LSI CSV generation system. The focus is on improving reliability, fixing prompt-related issues, enhancing validation, and ensuring robust error handling throughout the LSI generation pipeline.\n\n## Architecture\n\n### Current System Analysis\n\nThe existing LSI CSV generation system consists of:\n\n```\nBook Metadata (JSON) \n    \nEnhanced LLM Field Completer\n    \nField Mapping Re",
      "full_content": "# LSI CSV Bug Fixes Project - Design Document\n\n## Overview\n\nThis project addresses critical bugs and issues in the existing LSI CSV generation system. The focus is on improving reliability, fixing prompt-related issues, enhancing validation, and ensuring robust error handling throughout the LSI generation pipeline.\n\n## Architecture\n\n### Current System Analysis\n\nThe existing LSI CSV generation system consists of:\n\n```\nBook Metadata (JSON) \n    \nEnhanced LLM Field Completer\n    \nField Mapping Registry\n    \nLSI ACS Generator\n    \nValidation & Output (CSV)\n```\n\n### Identified Problem Areas\n\n1. **Prompt System Issues**\n   - Old-style prompts causing JSON parsing failures\n   - Inconsistent LLM response formats\n   - Missing system messages for role definition\n\n2. **Field Completion Problems**\n   - Inaccurate BISAC code generation\n   - Description length violations\n   - Poor contributor information quality\n\n3. **Validation Failures**\n   - Incorrect field length checking\n   - Missing required field detection issues\n   - Format validation bugs\n\n4. **Configuration Issues**\n   - Multi-level config inheritance problems\n   - Tranche configuration not applying correctly\n   - Default value handling failures\n\n5. **Error Handling Gaps**\n   - Poor error recovery mechanisms\n   - Insufficient logging and debugging info\n   - Batch processing failure cascades\n\n## Components and Interfaces\n\n### 1. Enhanced Prompt System\n\n#### Current Issues\n- Bibliography prompt returning conversational text instead of JSON\n- Inconsistent system message formatting\n- Missing JSON enforcement in prompts\n\n#### Proposed Fixes\n```python\nclass ModernizedPromptManager:\n    def ensure_json_response(self, prompt_config: Dict) -> Dict:\n        \"\"\"Ensure all prompts enforce JSON-only responses\"\"\"\n        if 'messages' not in prompt_config:\n            return self.convert_to_messages_format(prompt_config)\n        \n        # Add JSON enforcement to system message\n        system_msg = prompt_config['messages'][0]['content']\n        if 'JSON format' not in system_msg:\n            system_msg += \" You MUST respond ONLY in valid JSON format.\"\n            prompt_config['messages'][0]['content'] = system_msg\n        \n        return prompt_config\n```\n\n#### Implementation Plan\n- Convert all remaining old-style prompts to messages format\n- Add explicit JSON enforcement to all system messages\n- Implement response validation with fallback handling\n- Add prompt testing framework\n\n### 2. Improved Field Completion\n\n#### Current Issues\n- BISAC codes often invalid or outdated\n- Descriptions exceeding character limits\n- Generic contributor biographies\n\n#### Proposed Fixes\n```python\nclass EnhancedFieldValidator:\n    def validate_bisac_code(self, code: str) -> ValidationResult:\n        \"\"\"Validate BISAC code against current standards\"\"\"\n        # Check format: 3 letters + 6 digits\n        if not re.match(r'^[A-Z]{3}\\d{6}$', code):\n            return ValidationResult(False, \"Invalid BISAC format\")\n        \n        # Check against current BISAC database\n        if not self.bisac_db.is_valid(code):\n            return ValidationResult(False, f\"BISAC code {code} not found in current standards\")\n        \n        return ValidationResult(True, \"Valid BISAC code\")\n    \n    def validate_description_length(self, description: str, max_length: int) -> ValidationResult:\n        \"\"\"Validate description length with proper truncation\"\"\"\n        if len(description) <= max_length:\n            return ValidationResult(True, \"Description length valid\")\n        \n        # Intelligent truncation at sentence boundary\n        truncated = self.truncate_at_sentence(description, max_length)\n        return ValidationResult(False, f\"Description too long, suggested: {truncated}\")\n```\n\n#### Implementation Plan\n- Update BISAC validation with current standards\n- Implement intelligent text truncation\n- Enhance contributor bio generation with book context\n- Add field-specific validation rules\n\n### 3. Robust Validation Framework\n\n#### Current Issues\n- Validation rules not matching LSI requirements\n- Poor error messages\n- Validation bypassed in some code paths\n\n#### Proposed Fixes\n```python\nclass LSIComplianceValidator:\n    def __init__(self):\n        self.field_rules = self.load_lsi_field_rules()\n        self.required_fields = self.load_required_fields()\n    \n    def validate_complete_record(self, metadata: CodexMetadata) -> ValidationReport:\n        \"\"\"Comprehensive validation of all LSI fields\"\"\"\n        report = ValidationReport()\n        \n        # Check required fields\n        for field in self.required_fields:\n            if not getattr(metadata, field, None):\n                report.add_error(f\"Required field '{field}' is missing\")\n        \n        # Validate field formats and lengths\n        for field_name, value in metadata.to_dict().items():\n            if field_name in self.field_rules:\n                rule = self.field_rules[field_name]\n                result = self.validate_field(field_name, value, rule)\n                if not result.is_valid:\n                    report.add_error(f\"Field '{field_name}': {result.message}\")\n        \n        return report\n```\n\n#### Implementation Plan\n- Create comprehensive LSI field rules database\n- Implement field-by-field validation\n- Add validation report generation\n- Ensure validation runs at all critical points\n\n### 4. Configuration System Fixes\n\n#### Current Issues\n- Multi-level config inheritance not working\n- Tranche configurations not applying\n- Default values not loading properly\n\n#### Proposed Fixes\n```python\nclass FixedConfigLoader:\n    def load_hierarchical_config(self, publisher: str, imprint: str, tranche: str = None) -> Dict:\n        \"\"\"Load and merge configurations in correct hierarchy\"\"\"\n        config = {}\n        \n        # Load in order: default -> publisher -> imprint -> tranche\n        config.update(self.load_default_config())\n        config.update(self.load_publisher_config(publisher))\n        config.update(self.load_imprint_config(imprint))\n        \n        if tranche:\n            config.update(self.load_tranche_config(tranche))\n        \n        # Validate merged configuration\n        self.validate_config(config)\n        return config\n```\n\n#### Implementation Plan\n- Fix configuration inheritance order\n- Add configuration validation\n- Implement proper default handling\n- Add configuration debugging tools\n\n### 5. Enhanced Error Handling\n\n#### Current Issues\n- Errors causing complete pipeline failures\n- Poor error messages and logging\n- No retry mechanisms for transient failures\n\n#### Proposed Fixes\n```python\nclass RobustErrorHandler:\n    def __init__(self):\n        self.retry_config = {\n            'max_retries': 3,\n            'backoff_factor': 2,\n            'retry_exceptions': [ConnectionError, TimeoutError]\n        }\n    \n    def with_retry(self, func, *args, **kwargs):\n        \"\"\"Execute function with retry logic\"\"\"\n        for attempt in range(self.retry_config['max_retries']):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                if attempt == self.retry_config['max_retries'] - 1:\n                    logger.error(f\"Final attempt failed: {e}\")\n                    raise\n                \n                if type(e) in self.retry_config['retry_exceptions']:\n                    wait_time = self.retry_config['backoff_factor'] ** attempt\n                    logger.warning(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s: {e}\")\n                    time.sleep(wait_time)\n                else:\n                    raise\n```\n\n#### Implementation Plan\n- Add retry logic for LLM calls\n- Implement graceful error recovery\n- Enhance logging with context\n- Add error aggregation for batch processing\n\n## Data Models\n\n### Enhanced Validation Models\n\n```python\n@dataclass\nclass ValidationResult:\n    is_valid: bool\n    message: str\n    suggested_fix: Optional[str] = None\n    severity: str = \"error\"  # error, warning, info\n\n@dataclass\nclass ValidationReport:\n    errors: List[ValidationResult]\n    warnings: List[ValidationResult]\n    field_completeness: Dict[str, bool]\n    overall_score: float\n    \n    def is_lsi_compliant(self) -> bool:\n        return len(self.errors) == 0 and self.overall_score >= 0.8\n```\n\n### Error Tracking Models\n\n```python\n@dataclass\nclass ProcessingError:\n    timestamp: datetime\n    component: str\n    error_type: str\n    message: str\n    context: Dict[str, Any]\n    stack_trace: Optional[str] = None\n    \n@dataclass\nclass BatchProcessingResult:\n    total_books: int\n    successful: int\n    failed: int\n    errors: List[ProcessingError]\n    processing_time: float\n    \n    def success_rate(self) -> float:\n        return self.successful / self.total_books if self.total_books > 0 else 0.0\n```\n\n## Error Handling\n\n### Error Categories and Responses\n\n1. **Prompt/LLM Errors**\n   - JSON parsing failures  Use fallback values\n   - Rate limiting  Implement exponential backoff\n   - Model unavailable  Switch to backup model\n\n2. **Validation Errors**\n   - Field format violations  Provide correction suggestions\n   - Missing required fields  Attempt AI completion\n   - Length violations  Intelligent truncation\n\n3. **Configuration Errors**\n   - Missing config files  Use defaults with warnings\n   - Invalid config syntax  Detailed error reporting\n   - Inheritance conflicts  Clear precedence rules\n\n4. **System Errors**\n   - Memory issues  Implement streaming processing\n   - File I/O errors  Retry with different paths\n   - Network failures  Offline mode with cached data\n\n## Testing Strategy\n\n### Bug Reproduction Tests\n- Create test cases for each identified bug\n- Implement regression tests to prevent reoccurrence\n- Add edge case testing for boundary conditions\n\n### Integration Testing\n- Test complete LSI generation pipeline\n- Validate configuration loading and inheritance\n- Test batch processing with error scenarios\n\n### Performance Testing\n- Memory usage monitoring during batch processing\n- LLM call performance and caching effectiveness\n- Large dataset processing capabilities\n\n### Validation Testing\n- Test against real LSI submission requirements\n- Validate generated CSV files with IngramSpark tools\n- Cross-reference with successful submissions\n\n## Implementation Phases\n\n### Phase 1: Critical Bug Fixes (Week 1-2)\n- Fix JSON parsing errors in prompts\n- Resolve configuration loading issues\n- Implement basic error recovery\n\n### Phase 2: Validation Improvements (Week 3-4)\n- Update field validation rules\n- Implement comprehensive LSI compliance checking\n- Add validation reporting\n\n### Phase 3: Enhanced Error Handling (Week 5-6)\n- Add retry logic and fallback mechanisms\n- Implement batch processing error recovery\n- Enhance logging and debugging\n\n### Phase 4: Performance and Polish (Week 7-8)\n- Optimize memory usage and performance\n- Add comprehensive testing\n- Documentation and deployment improvements",
      "size": 10916,
      "modified": "2025-07-28T14:17:53.556117",
      "spec_directory": "lsi-csv-bug-fixes",
      "directory_modified": "2025-07-28T22:26:09.548904"
    },
    {
      "file": "design.md",
      "content": "# LSI Pricing System Fix Design\n\n## Overview\n\nThis design addresses critical pricing regressions in the LSI CSV generation system. The current system outputs prices with currency symbols and fails to calculate territorial prices, making the CSV incompatible with Lightning Source requirements.\n\n## Architecture\n\n### Core Components\n\n1. **Enhanced Pricing Strategy**: Unified pricing strategy that handles all price fields\n2. **Currency Formatter**: Utility to format prices as decimal numbers without",
      "full_content": "# LSI Pricing System Fix Design\n\n## Overview\n\nThis design addresses critical pricing regressions in the LSI CSV generation system. The current system outputs prices with currency symbols and fails to calculate territorial prices, making the CSV incompatible with Lightning Source requirements.\n\n## Architecture\n\n### Core Components\n\n1. **Enhanced Pricing Strategy**: Unified pricing strategy that handles all price fields\n2. **Currency Formatter**: Utility to format prices as decimal numbers without symbols\n3. **Territorial Price Calculator**: Service to calculate prices for different markets\n4. **Field Mapping Optimizer**: System to eliminate duplicate mapping strategies\n\n## Components and Interfaces\n\n### 1. Enhanced Pricing Strategy\n\n```python\nclass EnhancedPricingStrategy:\n    def __init__(self, exchange_rates: Dict[str, float], default_discount: int = 40):\n        self.exchange_rates = exchange_rates\n        self.default_discount = default_discount\n    \n    def map_field(self, field_name: str, metadata: Dict) -> str:\n        \"\"\"Map pricing fields with proper formatting and calculation\"\"\"\n        \n    def format_price(self, amount: float) -> str:\n        \"\"\"Format price as decimal without currency symbols\"\"\"\n        \n    def calculate_territorial_price(self, base_price: float, currency: str) -> float:\n        \"\"\"Calculate price for specific territory\"\"\"\n```\n\n### 2. Currency Formatter\n\n```python\nclass CurrencyFormatter:\n    @staticmethod\n    def format_decimal_price(price_str: str) -> str:\n        \"\"\"Convert price string to decimal format\"\"\"\n        # Remove currency symbols and format to 2 decimal places\n        \n    @staticmethod\n    def extract_numeric_value(price_str: str) -> float:\n        \"\"\"Extract numeric value from price string\"\"\"\n```\n\n### 3. Territorial Price Calculator\n\n```python\nclass TerritorialPriceCalculator:\n    def __init__(self, exchange_rates: Dict[str, float]):\n        self.exchange_rates = exchange_rates\n        \n    def calculate_all_territorial_prices(self, us_price: float) -> Dict[str, float]:\n        \"\"\"Calculate prices for all territories\"\"\"\n        \n    def get_exchange_rate(self, currency: str) -> float:\n        \"\"\"Get exchange rate with fallback defaults\"\"\"\n```\n\n## Data Models\n\n### Price Configuration\n\n```python\n@dataclass\nclass PriceConfiguration:\n    us_list_price: float\n    wholesale_discount: int = 40\n    territorial_rates: Dict[str, float] = field(default_factory=dict)\n    specialty_markets: List[str] = field(default_factory=list)\n```\n\n### Pricing Result\n\n```python\n@dataclass\nclass PricingResult:\n    us_price: str  # \"19.95\"\n    uk_price: str  # \"15.99\"\n    eu_price: str  # \"18.50\"\n    ca_price: str  # \"26.95\"\n    au_price: str  # \"29.95\"\n    specialty_prices: Dict[str, str]  # All equal to US price\n    wholesale_discounts: Dict[str, str]  # All \"40\"\n```\n\n## Error Handling\n\n### Price Parsing Errors\n- Invalid price formats should default to \"0.00\"\n- Missing exchange rates should use fallback defaults\n- Calculation errors should be logged and use safe defaults\n\n### Field Mapping Errors\n- Duplicate strategy registration should be prevented\n- Unknown price fields should use default pricing strategy\n- Missing price data should result in empty string, not error\n\n## Testing Strategy\n\n### Unit Tests\n1. **Currency Formatter Tests**\n   - Test removal of currency symbols\n   - Test decimal formatting\n   - Test edge cases (negative prices, zero prices)\n\n2. **Territorial Calculator Tests**\n   - Test exchange rate calculations\n   - Test fallback mechanisms\n   - Test all supported currencies\n\n3. **Enhanced Pricing Strategy Tests**\n   - Test all price field mappings\n   - Test specialty market pricing\n   - Test wholesale discount application\n\n### Integration Tests\n1. **Full Pipeline Tests**\n   - Test complete CSV generation with proper pricing\n   - Test field mapping optimization\n   - Test performance with reduced strategy count\n\n### Validation Tests\n1. **LSI Compliance Tests**\n   - Verify all prices are decimal format\n   - Verify no currency symbols in output\n   - Verify all required price fields are populated\n\n## Implementation Plan\n\n### Phase 1: Currency Formatting Fix\n1. Create CurrencyFormatter utility\n2. Update existing pricing strategies to use formatter\n3. Test price format compliance\n\n### Phase 2: Territorial Price Calculation\n1. Implement TerritorialPriceCalculator\n2. Add exchange rate management\n3. Calculate EU, CA, AU prices\n\n### Phase 3: Specialty Market Pricing\n1. Implement GC and specialty market pricing\n2. Ensure all specialty markets equal US price\n3. Standardize wholesale discounts\n\n### Phase 4: Field Mapping Optimization\n1. Audit current field mapping strategies\n2. Eliminate duplicate registrations\n3. Consolidate pricing strategies\n\n### Phase 5: Integration and Testing\n1. Integrate all components\n2. Run comprehensive tests\n3. Validate LSI compliance\n\n## Performance Considerations\n\n- **Reduced Strategy Count**: Eliminate duplicate mappings to reduce from 190 to ~119 strategies\n- **Cached Exchange Rates**: Use cached rates to avoid repeated API calls\n- **Optimized Price Calculation**: Calculate all territorial prices in single pass\n- **Memory Efficiency**: Reuse pricing strategy instances across fields\n\n## Monitoring and Logging\n\n- Log all price calculations for audit trail\n- Monitor exchange rate usage and cache hits\n- Track field mapping strategy registration counts\n- Alert on pricing calculation failures",
      "size": 5439,
      "modified": "2025-07-29T00:18:06.061493",
      "spec_directory": "lsi-pricing-fixes",
      "directory_modified": "2025-07-29T00:53:34.598584"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThe BISAC category system needs to be redesigned to properly populate all three BISAC category fields with full category names using LLM assistance and validation. The current system has conflicting field mappings and doesn't properly generate multiple categories.\n\n## Architecture\n\n### Current Issues Analysis\n\n1. **Conflicting Field Mappings**: Multiple strategies are registered for the same BISAC fields, causing overrides\n2. **Code vs Name Confusion**: Some field",
      "full_content": "# Design Document\n\n## Overview\n\nThe BISAC category system needs to be redesigned to properly populate all three BISAC category fields with full category names using LLM assistance and validation. The current system has conflicting field mappings and doesn't properly generate multiple categories.\n\n## Architecture\n\n### Current Issues Analysis\n\n1. **Conflicting Field Mappings**: Multiple strategies are registered for the same BISAC fields, causing overrides\n2. **Code vs Name Confusion**: Some fields return codes (PSY031000) instead of full names\n3. **Single Category Generation**: Only one category is being populated instead of three\n4. **Incomplete LLM Integration**: LLM completion strategies exist but aren't working properly\n\n### Proposed Solution\n\nCreate a unified BISAC category generation system that:\n- Uses LLM to generate multiple relevant categories\n- Validates all categories against current BISAC standards\n- Properly formats categories as full names without codes\n- Handles fallbacks gracefully\n\n## Components and Interfaces\n\n### 1. Enhanced BISAC Category Strategy\n\n```python\nclass EnhancedBISACCategoryStrategy(MappingStrategy):\n    \"\"\"\n    Unified strategy for generating and validating BISAC categories.\n    Generates up to 3 relevant categories using LLM assistance with tranche overrides.\n    \"\"\"\n    \n    def __init__(self, category_number: int, llm_completer, bisac_validator):\n        self.category_number = category_number  # 1, 2, or 3\n        self.llm_completer = llm_completer\n        self.bisac_validator = bisac_validator\n    \n    def map_field(self, metadata: CodexMetadata, context: MappingContext) -> str:\n        # Check for tranche config override for primary category\n        # Generate all categories at once with diversity requirements\n        # Return the requested category number\n        pass\n```\n\n### 2. BISAC Category Generator\n\n```python\nclass BISACCategoryGenerator:\n    \"\"\"\n    Generates multiple BISAC categories using LLM assistance and validation.\n    Supports tranche overrides and ensures diversity across top-level categories.\n    \"\"\"\n    \n    def generate_categories(self, metadata: CodexMetadata, context: MappingContext, max_categories: int = 3) -> List[str]:\n        # Check for tranche config override for primary category\n        # Use LLM to generate categories based on book metadata\n        # Ensure at least 2 categories from different top-level categories\n        # Validate each category against BISAC standards\n        # Return full category names (not codes)\n        pass\n    \n    def apply_tranche_override(self, context: MappingContext) -> Optional[str]:\n        # Check tranche config for BISAC category override\n        # Validate override category\n        # Return override category if valid\n        pass\n    \n    def ensure_category_diversity(self, categories: List[str]) -> List[str]:\n        # Analyze top-level categories (BUS, SEL, COM, etc.)\n        # Reorder/replace to maximize diversity\n        # Prefer categories from different top-levels\n        pass\n    \n    def validate_and_format_category(self, category: str) -> Optional[str]:\n        # Validate category against BISAC standards\n        # Convert code to full name if needed\n        # Return formatted category name\n        pass\n```\n\n### 3. Updated BISAC Validator\n\nEnhance the existing `BISACValidator` to:\n- Support category name validation (not just codes)\n- Provide category name lookup from codes\n- Suggest similar category names\n\n### 4. LLM Prompt Enhancement\n\nCreate specialized prompts for BISAC category generation:\n- Analyze book title, subtitle, description, and keywords\n- Generate 3 most relevant BISAC categories with diversity preference\n- Prioritize categories from different top-level categories (BUS, SEL, COM, etc.)\n- Return full category names in proper format\n- Rank by relevance to book content while maintaining diversity\n- Handle cases where tranche override is already specified\n\n## Data Models\n\n### BISAC Category Result\n\n```python\n@dataclass\nclass BISACCategoryResult:\n    \"\"\"Result of BISAC category generation.\"\"\"\n    categories: List[str]  # Full category names\n    primary_category: str  # Most relevant category (may be tranche override)\n    confidence_scores: List[float]  # Confidence for each category\n    validation_results: List[BISACValidationResult]\n    top_level_categories: List[str]  # Top-level prefixes (BUS, SEL, COM, etc.)\n    diversity_score: float  # Measure of category diversity\n    tranche_override_used: bool = False\n    fallback_used: bool = False\n```\n\n### Enhanced Metadata Fields\n\nEnsure metadata supports:\n- `bisac_category_1`: Primary BISAC category (full name)\n- `bisac_category_2`: Secondary BISAC category (full name)  \n- `bisac_category_3`: Tertiary BISAC category (full name)\n- `bisac_codes`: Original codes for reference\n\n## Error Handling\n\n### Validation Failures\n- If generated category is invalid, use validator suggestions\n- If no valid suggestions, use fallback categories\n- Log all validation failures with details\n\n### LLM Generation Failures\n- Retry with simplified prompt\n- Use existing metadata BISAC information as fallback\n- Use safe default categories if all else fails\n\n### Fallback Strategy\n1. Apply tranche config override if specified\n2. Use existing metadata BISAC information\n3. Generate categories based on title/description keywords with diversity preference\n4. Use safe default categories from different top-levels: \"GENERAL\", \"BUSINESS & ECONOMICS\", \"REFERENCE\"\n\n## Testing Strategy\n\n### Unit Tests\n- Test BISAC category generation with various book types\n- Test validation against current BISAC standards\n- Test fallback strategies for edge cases\n- Test category name formatting and code conversion\n\n### Integration Tests\n- Test full pipeline with BISAC category generation\n- Verify all three category fields are populated\n- Test with books that have existing BISAC information\n- Test with books that need LLM generation\n\n### Validation Tests\n- Test against complete BISAC 2024 standards database\n- Test category name to code conversion\n- Test invalid category handling\n- Test fallback category selection\n\n## Implementation Plan\n\n### Phase 1: Core Infrastructure\n1. Create `EnhancedBISACCategoryStrategy` class\n2. Create `BISACCategoryGenerator` class\n3. Enhance `BISACValidator` with name validation\n4. Create specialized LLM prompts\n\n### Phase 2: Field Mapping Integration\n1. Remove conflicting BISAC field registrations\n2. Register new enhanced strategies for all three fields\n3. Update field mapping configuration\n4. Test field population\n\n### Phase 3: Validation and Testing\n1. Implement comprehensive test suite\n2. Test with real book data\n3. Validate against BISAC standards\n4. Performance optimization\n\n### Phase 4: Error Handling and Logging\n1. Implement robust error handling\n2. Add detailed logging for debugging\n3. Create monitoring for category generation quality\n4. Document troubleshooting procedures",
      "size": 6943,
      "modified": "2025-07-29T02:03:15.768337",
      "spec_directory": "bisac-category-fixes",
      "directory_modified": "2025-07-29T02:25:21.321841"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThis design addresses six specific field mapping corrections in the LSI CSV generation system. The solution involves enhancing the existing field mapping strategies, adding new data extraction utilities, implementing tranche override logic, and ensuring proper field validation and formatting.\n\n## Architecture\n\nThe corrections will be implemented through modifications to existing components in the LSI field mapping system:\n\n- **Field Mapping Registry**: Enhanced to",
      "full_content": "# Design Document\n\n## Overview\n\nThis design addresses six specific field mapping corrections in the LSI CSV generation system. The solution involves enhancing the existing field mapping strategies, adding new data extraction utilities, implementing tranche override logic, and ensuring proper field validation and formatting.\n\n## Architecture\n\nThe corrections will be implemented through modifications to existing components in the LSI field mapping system:\n\n- **Field Mapping Registry**: Enhanced to support tranche override precedence\n- **Data Extraction Utilities**: New utilities for JSON metadata parsing\n- **Field Mapping Strategies**: Updated strategies for specific field types\n- **Validation Layer**: Enhanced validation for extracted data\n\n## Components and Interfaces\n\n### 1. Tranche Override Manager\n\n**Location**: `src/codexes/modules/distribution/tranche_override_manager.py`\n\n```python\nclass TrancheOverrideManager:\n    def apply_overrides(self, field_name: str, llm_value: str, tranche_value: str, \n                       field_type: str = \"replace\") -> str:\n        \"\"\"Apply tranche overrides with support for append operations\"\"\"\n        \n    def is_append_field(self, field_name: str) -> bool:\n        \"\"\"Check if field should append rather than replace\"\"\"\n        \n    def should_override(self, tranche_value: Any) -> bool:\n        \"\"\"Determine if tranche value should override LLM value\"\"\"\n```\n\n### 2. JSON Metadata Extractor\n\n**Location**: `src/codexes/modules/distribution/json_metadata_extractor.py`\n\n```python\nclass JSONMetadataExtractor:\n    def extract_thema_subjects(self, metadata: dict) -> List[str]:\n        \"\"\"Extract up to 3 thema subject codes from metadata\"\"\"\n        \n    def extract_age_range(self, metadata: dict) -> Tuple[Optional[int], Optional[int]]:\n        \"\"\"Extract and validate min/max age values\"\"\"\n        \n    def validate_age_value(self, age: Any) -> Optional[int]:\n        \"\"\"Validate and convert age value to integer\"\"\"\n```\n\n### 3. Series-Aware Description Processor\n\n**Location**: `src/codexes/modules/distribution/series_description_processor.py`\n\n```python\nclass SeriesDescriptionProcessor:\n    def process_description(self, description: str, series_name: Optional[str]) -> str:\n        \"\"\"Process description to include series reference when applicable\"\"\"\n        \n    def has_series_context(self, series_name: Optional[str]) -> bool:\n        \"\"\"Check if series name is valid and should be included\"\"\"\n```\n\n### 4. Enhanced Field Mapping Strategies\n\n**Modifications to**: `src/codexes/modules/distribution/enhanced_field_mappings.py`\n\nNew strategies will be added:\n- `ThemaSubjectStrategy`: Extract and map thema subjects\n- `AgeRangeStrategy`: Extract and validate age ranges  \n- `SeriesAwareDescriptionStrategy`: Process series-aware descriptions\n- `BlankIngramPricingStrategy`: Ensure specific pricing fields remain blank\n- `TrancheFilePathStrategy`: Generate file paths from tranche configuration\n\n## Data Models\n\n### Tranche Override Configuration\n\n```python\n@dataclass\nclass TrancheOverrideConfig:\n    field_overrides: Dict[str, Any]\n    append_fields: List[str]  # Fields that append rather than replace\n    file_path_templates: Dict[str, str]\n    \n@dataclass\nclass FieldOverrideResult:\n    final_value: str\n    override_applied: bool\n    override_type: str  # \"replace\", \"append\", \"none\"\n```\n\n### Metadata Extraction Results\n\n```python\n@dataclass\nclass ThemaExtractionResult:\n    subject_1: Optional[str]\n    subject_2: Optional[str] \n    subject_3: Optional[str]\n    warnings: List[str]\n\n@dataclass\nclass AgeRangeResult:\n    min_age: Optional[int]\n    max_age: Optional[int]\n    validation_errors: List[str]\n```\n\n## Error Handling\n\n### Validation and Recovery\n\n1. **Thema Subject Validation**:\n   - Validate thema codes against known formats\n   - Log warnings for invalid codes\n   - Skip invalid entries, continue with valid ones\n\n2. **Age Range Validation**:\n   - Ensure values are numeric and within reasonable bounds (0-150)\n   - Log validation errors for out-of-range values\n   - Default to empty when validation fails\n\n3. **Series Name Validation**:\n   - Check for non-empty, non-null series names\n   - Handle edge cases where series name contains special characters\n\n4. **File Path Validation**:\n   - Sanitize file paths according to LSI requirements\n   - Remove or replace invalid characters\n   - Ensure paths don't exceed length limits\n\n### Error Recovery Strategies\n\n- **Graceful Degradation**: When extraction fails, fall back to existing values\n- **Partial Success**: Process valid data even when some fields fail validation\n- **Comprehensive Logging**: Log all validation issues with context for debugging\n\n## Testing Strategy\n\n### Unit Tests\n\n1. **Tranche Override Tests**:\n   - Test replace vs append behavior\n   - Test precedence rules\n   - Test edge cases (null, empty values)\n\n2. **JSON Extraction Tests**:\n   - Test thema subject extraction with various array sizes\n   - Test age range extraction with valid/invalid values\n   - Test malformed JSON handling\n\n3. **Series Description Tests**:\n   - Test \"This book\" replacement logic\n   - Test edge cases (no series, empty series)\n   - Test descriptions without \"This book\"\n\n4. **Field Mapping Tests**:\n   - Test new strategies with sample data\n   - Test integration with existing mapping system\n   - Test blank field enforcement for Ingram pricing\n\n### Integration Tests\n\n1. **End-to-End Pipeline Tests**:\n   - Test complete LSI generation with corrected mappings\n   - Verify all six corrections work together\n   - Test with real book metadata samples\n\n2. **Configuration Tests**:\n   - Test tranche configuration loading\n   - Test override precedence in multi-level configs\n   - Test file path template processing\n\n### Performance Tests\n\n1. **Extraction Performance**:\n   - Benchmark JSON parsing and extraction\n   - Test with large metadata objects\n   - Ensure no significant performance regression\n\n## Implementation Plan\n\n### Phase 1: Core Infrastructure\n- Implement TrancheOverrideManager\n- Implement JSONMetadataExtractor\n- Add unit tests for core utilities\n\n### Phase 2: Field Strategies\n- Implement new field mapping strategies\n- Update existing strategies to use tranche overrides\n- Add strategy-specific tests\n\n### Phase 3: Integration\n- Integrate new components with existing LSI generator\n- Update field mapping registry\n- Add integration tests\n\n### Phase 4: Validation and Polish\n- Implement comprehensive validation\n- Add error recovery mechanisms\n- Performance testing and optimization\n\n## Configuration Changes\n\n### Tranche Configuration Schema\n\n```json\n{\n  \"field_overrides\": {\n    \"Series Name\": \"Transcriptive Meditation\",\n    \"annotation_boilerplate\": \" This series explores...\"\n  },\n  \"append_fields\": [\"annotation_boilerplate\"],\n  \"file_path_templates\": {\n    \"interior\": \"interior/{title_slug}_interior.pdf\",\n    \"cover\": \"covers/{title_slug}_cover.pdf\"\n  },\n  \"blank_fields\": [\n    \"US-Ingram-Only* Suggested List Price (mode 2)\",\n    \"US-Ingram-Only* Wholesale Discount % (Mode 2)\",\n    \"US - Ingram - GAP * Suggested List Price (mode 2)\",\n    \"US - Ingram - GAP * Wholesale Discount % (Mode 2)\",\n    \"SIBI - EDUC - US * Suggested List Price (mode 2)\",\n    \"SIBI - EDUC - US * Wholesale Discount % (Mode 2)\"\n  ]\n}\n```\n\n## Backward Compatibility\n\nAll changes will maintain backward compatibility:\n- Existing field mappings continue to work unchanged\n- New functionality is additive, not replacing existing logic\n- Configuration changes are optional with sensible defaults\n- Existing tranche configurations remain valid",
      "size": 7570,
      "modified": "2025-07-31T17:59:34.614484",
      "spec_directory": "lsi-field-mapping-corrections",
      "directory_modified": "2025-07-31T18:33:35.171851"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThe Streamlit UI Configuration Enhancement design creates a comprehensive interface for managing the multi-level configuration system in Codexes Factory. The design focuses on enhancing the existing Book Pipeline page and related UI components to support all current parameters while maintaining usability and performance.\n\n## Architecture\n\n### Enhanced UI Architecture\n\n```mermaid\ngraph TB\n    A[Streamlit UI] --> B[Configuration Manager]\n    B --> C[Multi-Level Conf",
      "full_content": "# Design Document\n\n## Overview\n\nThe Streamlit UI Configuration Enhancement design creates a comprehensive interface for managing the multi-level configuration system in Codexes Factory. The design focuses on enhancing the existing Book Pipeline page and related UI components to support all current parameters while maintaining usability and performance.\n\n## Architecture\n\n### Enhanced UI Architecture\n\n```mermaid\ngraph TB\n    A[Streamlit UI] --> B[Configuration Manager]\n    B --> C[Multi-Level Config Loader]\n    B --> D[Parameter Validator]\n    B --> E[UI State Manager]\n    \n    C --> F[Default Config]\n    C --> G[Publisher Config]\n    C --> H[Imprint Config]\n    C --> I[Tranche Config]\n    \n    D --> J[LSI Validator]\n    D --> K[Territorial Validator]\n    D --> L[Field Validator]\n    \n    E --> M[Session State]\n    E --> N[Form State]\n    E --> O[Configuration Cache]\n    \n    B --> P[Pipeline Interface]\n    P --> Q[Command Builder]\n    P --> R[Parameter Serializer]\n    P --> S[Execution Monitor]\n    \n    Q --> T[run_book_pipeline.py]\n```\n\n## Components and Interfaces\n\n### 1. Enhanced Configuration Manager\n\n**Purpose**: Central component for managing multi-level configurations and UI state\n\n**Key Features**:\n- Load and merge configurations from all levels\n- Validate configuration parameters\n- Handle configuration inheritance and overrides\n- Manage UI state and form data\n\n**Implementation**:\n```python\nclass EnhancedConfigurationManager:\n    def __init__(self):\n        self.config_loader = MultiLevelConfigLoader()\n        self.validator = ConfigurationValidator()\n        self.ui_state = UIStateManager()\n    \n    def load_available_configs(self) -> Dict[str, List[str]]\n    def merge_configurations(self, publisher: str, imprint: str, tranche: str) -> Dict[str, Any]\n    def validate_configuration(self, config: Dict[str, Any]) -> ValidationResult\n    def get_parameter_groups(self) -> Dict[str, List[str]]\n    def save_configuration_snapshot(self, config: Dict[str, Any]) -> str\n```\n\n### 2. Dynamic Configuration Loader\n\n**Purpose**: Scan and load configuration files from the file system\n\n**Key Features**:\n- Scan configs/ directory for available configurations\n- Load and parse JSON configuration files\n- Handle configuration file validation\n- Provide fallback for missing configurations\n\n**Implementation**:\n```python\nclass DynamicConfigurationLoader:\n    def scan_publishers(self) -> List[str]\n    def scan_imprints(self, publisher: str = None) -> List[str]\n    def scan_tranches(self, imprint: str = None) -> List[str]\n    def load_configuration_file(self, config_type: str, name: str) -> Dict[str, Any]\n    def validate_configuration_structure(self, config: Dict[str, Any], config_type: str) -> bool\n```\n\n### 3. Parameter Group Manager\n\n**Purpose**: Organize parameters into logical groups for UI presentation\n\n**Key Features**:\n- Group related parameters together\n- Handle parameter dependencies and visibility\n- Provide parameter metadata (help text, validation rules)\n- Support dynamic parameter groups based on configuration\n\n**Parameter Groups**:\n- **Core Settings**: Basic publishing information (publisher, imprint, tranche)\n- **LSI Configuration**: Lightning Source specific settings, field mappings\n- **Territorial Pricing**: Multi-currency pricing and discounts\n- **Physical Specifications**: Trim size, page count, binding options\n- **Metadata Defaults**: BISAC, Thema, contributor information\n- **Distribution Settings**: Submission methods, carton quantities\n- **Pipeline Control**: Stage selection, book selection, execution options\n- **LLM & AI Configuration**: Model parameters, retry settings, field completion rules\n- **Debug & Monitoring**: Debug settings, logging, validation rules, performance monitoring\n- **Complete Configuration Preview**: Mandatory final configuration inspection before submission\n\n**Implementation**:\n```python\nclass ParameterGroupManager:\n    def get_parameter_groups(self) -> Dict[str, ParameterGroup]\n    def get_group_parameters(self, group_name: str) -> List[Parameter]\n    def validate_parameter_dependencies(self, params: Dict[str, Any]) -> List[str]\n    def get_parameter_help_text(self, param_name: str) -> str\n```\n\n### 4. Enhanced UI Components\n\n**Purpose**: Streamlit components for configuration management with complete parameter inspection\n\n**Key Features**:\n- Configuration selection dropdowns with dynamic loading\n- Parameter input widgets with validation\n- Expandable parameter groups with display mode controls (Simple/Advanced/Expert)\n- **Mandatory complete configuration preview before submission**\n- Real-time validation feedback\n- Command-line preview showing exact parameters being passed\n- Configuration comparison and diff viewing\n\n**Implementation**:\n```python\nclass ConfigurationUI:\n    def render_configuration_selector(self) -> Tuple[str, str, str]\n    def render_parameter_group(self, group: ParameterGroup, display_mode: str) -> Dict[str, Any]\n    def render_complete_configuration_preview(self, config: Dict[str, Any]) -> None\n    def render_command_line_preview(self, command: List[str]) -> None\n    def render_validation_results(self, results: ValidationResult) -> None\n    def render_parameter_inspector(self, config: Dict[str, Any]) -> None\n    def render_configuration_diff(self, config1: Dict, config2: Dict) -> None\n    def render_execution_monitor(self, process: subprocess.Popen) -> None\n```\n\n### 5. Configuration Validator\n\n**Purpose**: Validate configuration parameters and combinations\n\n**Key Features**:\n- Real-time parameter validation\n- Cross-parameter dependency checking\n- LSI-specific validation rules\n- Territorial pricing validation\n\n**Implementation**:\n```python\nclass ConfigurationValidator:\n    def validate_lsi_parameters(self, config: Dict[str, Any]) -> ValidationResult\n    def validate_territorial_pricing(self, config: Dict[str, Any]) -> ValidationResult\n    def validate_physical_specifications(self, config: Dict[str, Any]) -> ValidationResult\n    def validate_parameter_dependencies(self, config: Dict[str, Any]) -> ValidationResult\n```\n\n### 6. Command Builder and Serializer\n\n**Purpose**: Convert UI configuration to command-line arguments\n\n**Key Features**:\n- Build command-line arguments from UI state\n- Handle complex parameter serialization\n- Support file uploads and temporary file management\n- Generate audit-ready command logs\n\n**Implementation**:\n```python\nclass CommandBuilder:\n    def build_pipeline_command(self, config: Dict[str, Any]) -> List[str]\n    def serialize_complex_parameters(self, params: Dict[str, Any]) -> Dict[str, str]\n    def handle_file_uploads(self, files: Dict[str, Any]) -> Dict[str, str]\n    def generate_command_audit_log(self, command: List[str]) -> str\n```\n\n## Data Models\n\n### Configuration Parameter\n```python\n@dataclass\nclass ConfigurationParameter:\n    name: str\n    display_name: str\n    parameter_type: str  # text, number, select, multiselect, file, checkbox\n    default_value: Any\n    help_text: str\n    validation_rules: List[str]\n    dependencies: List[str]\n    group: str\n    required: bool = False\n    options: List[str] = None\n```\n\n### Parameter Group\n```python\n@dataclass\nclass ParameterGroup:\n    name: str\n    display_name: str\n    description: str\n    parameters: List[ConfigurationParameter]\n    expanded_by_default: bool = False\n    dependencies: List[str] = None\n```\n\n### Configuration State\n```python\n@dataclass\nclass ConfigurationState:\n    publisher: str\n    imprint: str\n    tranche: str\n    parameters: Dict[str, Any]\n    validation_results: ValidationResult\n    last_updated: datetime\n    configuration_hash: str\n```\n\n### Validation Result\n```python\n@dataclass\nclass ValidationResult:\n    is_valid: bool\n    errors: List[ValidationError]\n    warnings: List[ValidationWarning]\n    parameter_status: Dict[str, str]  # parameter_name -> status\n```\n\n## User Interface Design\n\n### Enhanced Book Pipeline Page Layout\n\n```\n\n  Book Production Pipeline                                  \n\n Configuration Selection                                      \n    \n  Publisher    Imprint      Tranche      Load Config    \n  [Dropdown]   [Dropdown]   [Dropdown]   [Button]       \n    \n\n Display Mode: [Simple] [Advanced] [Expert]                  \n\n  Core Settings                                             \n    Schedule File: [File Upload/Select]                     \n    Primary Model: [Dropdown]                               \n    Verifier Model: [Dropdown]                              \n\n  Pipeline Stages                                           \n    Start Stage: [Dropdown]                                 \n    End Stage: [Dropdown]                                   \n    Stage Options: [Checkboxes]                             \n\n  LSI Configuration                                         \n    Lightning Source Account: [Text Input]                  \n    Territorial Pricing: [Expandable Table]                 \n    Physical Specifications: [Form Fields]                  \n    Field Overrides: [Key-Value Editor]                     \n    Field Mapping Strategies: [Advanced Config]             \n\n  LLM & AI Configuration                                    \n    Model Parameters: [JSON Editor]                         \n    Retry Configuration: [Number Inputs]                    \n    Monitoring Settings: [Checkboxes]                       \n    Field Completion Rules: [Rule Editor]                   \n\n  Debug & Monitoring                                        \n    Debug Settings: [Checkboxes]                            \n    Logging Options: [Multi-select]                         \n    Validation Rules: [Rule Editor]                         \n    Performance Monitoring: [Toggles]                       \n\n  Complete Configuration Preview (MANDATORY)                \n    Final Merged Configuration: [JSON Viewer]               \n    Command Line Preview: [Code Block]                      \n    Parameter Count: [Summary Stats]                        \n    Configuration Hash: [Verification]                      \n\n  Pre-Submission Validation                                 \n    Configuration Validation: [Status Grid]                 \n    Parameter Dependencies: [Dependency Tree]               \n    LSI Compliance Check: [Compliance Report]               \n    Execution Readiness: [Go/No-Go Status]                  \n\n [Run Pipeline] [Save Configuration] [Export Configuration]  \n [Download Config Snapshot] [Compare with Previous]          \n\n```\n\n### Configuration Management Page\n\n```\n\n  Configuration Management                                  \n\n Configuration Type: [Publisher] [Imprint] [Tranche]         \n\n Available Configurations:                                   \n  \n   Publishers/                                            \n    nimble_books.json                                    \n    academic_publishers_inc.json                         \n   Imprints/                                              \n    xynapse_traces.json                                  \n    tech_books_press.json                                \n   Tranches/                                              \n     xynapse_tranche_1.json                               \n  \n\n Actions:                                                    \n [Upload New] [Download Template] [Edit Selected] [Delete]   \n\n Configuration Editor: [JSON Editor with Validation]         \n\n```\n\n## Implementation Strategy\n\n### Phase 1: Core Configuration Management\n1. **Enhanced Configuration Loader**: Extend existing configuration loading to support all parameter types\n2. **Parameter Group Organization**: Organize existing parameters into logical groups\n3. **Dynamic Configuration Discovery**: Implement scanning of configuration directories\n4. **Basic Validation**: Add real-time validation for core parameters\n\n### Phase 2: UI Enhancement\n1. **Expandable Parameter Groups**: Replace flat form with organized, expandable sections\n2. **Configuration Selection**: Add publisher/imprint/tranche selection dropdowns\n3. **Parameter Widgets**: Implement appropriate input widgets for different parameter types\n4. **Validation Feedback**: Add real-time validation feedback and error highlighting\n\n### Phase 3: Advanced Features\n1. **Configuration Preview**: Add configuration preview and comparison functionality\n2. **File Management**: Implement configuration file upload/download\n3. **Audit Trail**: Add configuration history and audit logging\n4. **Batch Operations**: Support batch configuration and execution\n\n### Phase 4: Advanced Features Integration (Core Requirement 10)\n1. **Advanced LLM Configuration**: Full LLM parameter management with model selection, retry parameters, and monitoring settings\n2. **Field Mapping Configuration**: UI for configuring custom field mapping strategies\n3. **Validation Rule Management**: Custom validation rule configuration and testing\n4. **Debug and Monitoring Dashboard**: Advanced debugging modes, detailed logging options, and real-time monitoring\n\n## Error Handling\n\n### Configuration Loading Errors\n- **Missing Configuration Files**: Provide clear error messages and fallback to defaults\n- **Invalid JSON Structure**: Show JSON validation errors with line numbers\n- **Parameter Validation Errors**: Highlight invalid parameters with specific error messages\n- **Configuration Conflicts**: Show inheritance chain and conflict resolution\n\n### UI State Management\n- **Session State Persistence**: Maintain form state across page refreshes\n- **Configuration Caching**: Cache loaded configurations to improve performance\n- **Error Recovery**: Graceful handling of UI errors with user-friendly messages\n- **Validation State**: Maintain validation state and provide real-time feedback\n\n## Testing Strategy\n\n### Component Testing\n- Test configuration loading and merging logic\n- Test parameter validation rules\n- Test UI component rendering and interaction\n- Test command building and serialization\n\n### Integration Testing\n- Test end-to-end configuration flow from UI to pipeline execution\n- Test configuration file management (upload/download)\n- Test multi-level configuration inheritance\n- Test validation across different configuration combinations\n\n### User Experience Testing\n- Test UI responsiveness and performance with large configurations\n- Test form usability and parameter organization\n- Test error handling and user feedback\n- Test configuration preview and comparison features\n\n## Performance Considerations\n\n### Configuration Loading\n- **Lazy Loading**: Load configurations only when needed\n- **Caching Strategy**: Cache frequently accessed configurations\n- **Background Loading**: Load available configurations in background\n- **Incremental Updates**: Update only changed configurations\n\n### UI Responsiveness\n- **Progressive Rendering**: Render parameter groups progressively\n- **Debounced Validation**: Debounce real-time validation to avoid excessive calls\n- **Efficient State Management**: Optimize Streamlit session state usage\n- **Minimal Re-renders**: Minimize unnecessary UI re-renders\n\n## Security Considerations\n\n### Configuration File Security\n- **Input Validation**: Validate all uploaded configuration files\n- **Path Traversal Protection**: Prevent directory traversal attacks\n- **File Size Limits**: Limit configuration file sizes\n- **Content Sanitization**: Sanitize configuration content before processing\n\n### Parameter Security\n- **Input Sanitization**: Sanitize all user inputs\n- **Command Injection Prevention**: Prevent command injection in pipeline execution\n- **Sensitive Data Handling**: Secure handling of API keys and credentials\n- **Access Control**: Implement appropriate access controls for configuration management",
      "size": 17197,
      "modified": "2025-08-02T15:19:45.500566",
      "spec_directory": "streamlit-ui-config-enhancement",
      "directory_modified": "2025-08-02T15:42:18.959595"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThis design addresses the Streamlit form constraint violation by implementing automatic configuration loading with reactive updates. The solution removes the problematic `st.button()` from within the form while providing a better user experience through automatic configuration management.\n\n## Architecture\n\n### Component Structure\n\n```\nConfigurationUI\n render_configuration_selector() [MODIFIED]\n    Automatic configuration loading\n    Session state manage",
      "full_content": "# Design Document\n\n## Overview\n\nThis design addresses the Streamlit form constraint violation by implementing automatic configuration loading with reactive updates. The solution removes the problematic `st.button()` from within the form while providing a better user experience through automatic configuration management.\n\n## Architecture\n\n### Component Structure\n\n```\nConfigurationUI\n render_configuration_selector() [MODIFIED]\n    Automatic configuration loading\n    Session state management\n    Visual feedback system\n _load_configuration() [ENHANCED]\n    Error handling\n    Loading indicators\n    Validation integration\n _render_configuration_inheritance() [UNCHANGED]\n     Configuration chain display\n```\n\n### Data Flow\n\n1. **Selection Change Detection**: Monitor dropdown changes via session state comparison\n2. **Automatic Loading**: Trigger configuration loading when selections change\n3. **State Management**: Update session state with merged configurations\n4. **Validation**: Run validation on loaded configurations\n5. **UI Updates**: Provide visual feedback and status indicators\n\n## Components and Interfaces\n\n### Modified ConfigurationUI Methods\n\n#### render_configuration_selector()\n\n**Purpose**: Render configuration dropdowns with automatic loading\n\n**Changes**:\n- Remove `st.button(\"Load Config\")` \n- Add automatic loading logic based on selection changes\n- Implement loading state management\n- Add visual feedback for loading process\n\n**Interface**:\n```python\ndef render_configuration_selector(self) -> Tuple[str, str, str]:\n    \"\"\"\n    Render configuration selection dropdowns with automatic loading\n    \n    Returns:\n        Tuple[str, str, str]: (publisher, imprint, tranche)\n    \"\"\"\n```\n\n#### _load_configuration()\n\n**Purpose**: Load and merge configurations with enhanced error handling\n\n**Enhancements**:\n- Add loading state indicators\n- Improve error handling and user feedback\n- Add configuration validation\n- Implement graceful fallbacks\n\n**Interface**:\n```python\ndef _load_configuration(self, publisher: str, imprint: str, tranche: str) -> bool:\n    \"\"\"\n    Load and merge configurations with enhanced feedback\n    \n    Args:\n        publisher: Publisher name\n        imprint: Imprint name  \n        tranche: Tranche name\n        \n    Returns:\n        bool: True if loading succeeded, False otherwise\n    \"\"\"\n```\n\n### New Helper Methods\n\n#### _has_selection_changed()\n\n**Purpose**: Detect if configuration selections have changed\n\n```python\ndef _has_selection_changed(self, publisher: str, imprint: str, tranche: str) -> bool:\n    \"\"\"Check if current selections differ from session state\"\"\"\n```\n\n#### _show_loading_feedback()\n\n**Purpose**: Display loading status and progress\n\n```python\ndef _show_loading_feedback(self, loading: bool, message: str = \"\") -> None:\n    \"\"\"Show loading indicators and status messages\"\"\"\n```\n\n## Data Models\n\n### Session State Structure\n\n```python\nconfig_ui_state = {\n    'display_mode': str,\n    'selected_publisher': str,\n    'selected_imprint': str, \n    'selected_tranche': str,\n    'current_config': Dict[str, Any],\n    'validation_results': Optional[ValidationResult],\n    'expanded_groups': Set[str],\n    'loading_state': bool,  # NEW\n    'last_load_time': float,  # NEW\n    'auto_load_enabled': bool  # NEW\n}\n```\n\n### Configuration Loading States\n\n```python\nclass LoadingState(Enum):\n    IDLE = \"idle\"\n    LOADING = \"loading\" \n    SUCCESS = \"success\"\n    ERROR = \"error\"\n```\n\n## Error Handling\n\n### Loading Errors\n\n1. **Configuration File Missing**: Show warning, use defaults\n2. **JSON Parse Error**: Show error message, prevent loading\n3. **Validation Failure**: Show validation errors, allow override\n4. **Network/IO Error**: Show retry option, cache last good config\n\n### Form Constraint Compliance\n\n1. **Button Removal**: Remove all `st.button()` calls from form context\n2. **State Management**: Use session state for interaction tracking\n3. **Event Handling**: Use Streamlit's native change detection\n4. **Validation**: Integrate with existing validation system\n\n## Testing Strategy\n\n### Unit Tests\n\n1. **Configuration Loading**: Test automatic loading logic\n2. **State Management**: Test session state updates\n3. **Error Handling**: Test various error conditions\n4. **Validation Integration**: Test validation workflow\n\n### Integration Tests\n\n1. **Form Compliance**: Verify no Streamlit API violations\n2. **UI Responsiveness**: Test selection change handling\n3. **Configuration Merging**: Test multi-level config loading\n4. **Error Recovery**: Test error handling and recovery\n\n### User Experience Tests\n\n1. **Loading Feedback**: Verify loading indicators work\n2. **Selection Changes**: Test configuration switching\n3. **Error Messages**: Verify clear error communication\n4. **Performance**: Test loading speed and responsiveness\n\n## Implementation Plan\n\n### Phase 1: Core Fixes\n1. Remove `st.button()` from `render_configuration_selector()`\n2. Implement automatic loading detection\n3. Add basic loading feedback\n4. Test form compliance\n\n### Phase 2: Enhanced UX\n1. Add loading state management\n2. Implement visual feedback system\n3. Add configuration change detection\n4. Enhance error handling\n\n### Phase 3: Validation Integration\n1. Integrate with validation system\n2. Add real-time validation feedback\n3. Implement configuration status indicators\n4. Add performance optimizations\n\n## Compatibility Considerations\n\n### Backward Compatibility\n- Maintain existing method signatures\n- Preserve session state structure\n- Keep configuration format unchanged\n- Support existing validation system\n\n### Form Integration\n- Ensure compatibility with `st.form()` constraints\n- Maintain existing form submission workflow\n- Preserve form data integrity\n- Support both form and non-form contexts\n\n### Performance Impact\n- Minimize configuration loading frequency\n- Cache loaded configurations\n- Optimize validation calls\n- Reduce UI update overhead",
      "size": 5944,
      "modified": "2025-08-02T20:03:44.059844",
      "spec_directory": "streamlit-form-button-fix",
      "directory_modified": "2025-08-02T20:13:10.265817"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThis design addresses the critical UI interaction issues in the Streamlit application by implementing controlled state management, debounced updates, and proper event handling to prevent runaway loops while ensuring responsive dropdown refreshes.\n\n## Architecture\n\n### Core Design Principles\n\n1. **Controlled State Updates**: Replace immediate `st.rerun()` calls with controlled state management\n2. **Event Debouncing**: Implement debouncing mechanisms to prevent rapi",
      "full_content": "# Design Document\n\n## Overview\n\nThis design addresses the critical UI interaction issues in the Streamlit application by implementing controlled state management, debounced updates, and proper event handling to prevent runaway loops while ensuring responsive dropdown refreshes.\n\n## Architecture\n\n### Core Design Principles\n\n1. **Controlled State Updates**: Replace immediate `st.rerun()` calls with controlled state management\n2. **Event Debouncing**: Implement debouncing mechanisms to prevent rapid successive updates\n3. **Atomic State Changes**: Ensure session state updates are atomic and consistent\n4. **Separation of Concerns**: Separate validation logic from UI refresh logic\n\n### Component Architecture\n\n```\nConfigurationUI\n DropdownManager (NEW)\n    handle_publisher_change()\n    refresh_dependent_dropdowns()\n    debounce_state_updates()\n ValidationManager (NEW)\n    validate_without_rerun()\n    display_validation_results()\n    prevent_validation_loops()\n StateManager (NEW)\n     update_session_state_atomically()\n     preserve_valid_selections()\n     handle_state_consistency()\n```\n\n## Components and Interfaces\n\n### 1. DropdownManager Class\n\n**Purpose**: Manages dropdown dependencies and refresh logic without causing rerun loops.\n\n**Key Methods**:\n- `handle_publisher_change(old_publisher, new_publisher)`: Manages publisher change logic\n- `refresh_dependent_dropdowns(publisher, force_refresh=False)`: Updates dependent dropdowns\n- `debounce_state_updates(update_func, delay=0.1)`: Prevents rapid successive updates\n\n**State Management**:\n- Uses session state flags to track pending updates\n- Implements change detection without immediate reruns\n- Maintains dropdown option caches for performance\n\n### 2. ValidationManager Class\n\n**Purpose**: Handles configuration validation without triggering UI refresh loops.\n\n**Key Methods**:\n- `validate_configuration_safe(config)`: Validates without causing reruns\n- `display_validation_results_stable(results)`: Shows results without refresh\n- `prevent_validation_loops()`: Implements validation state protection\n\n**Loop Prevention**:\n- Uses validation state flags to prevent multiple simultaneous validations\n- Implements result caching to avoid redundant validations\n- Separates validation logic from UI update logic\n\n### 3. StateManager Class\n\n**Purpose**: Provides atomic session state management and consistency guarantees.\n\n**Key Methods**:\n- `atomic_update(updates_dict)`: Updates multiple session state values atomically\n- `preserve_selections(old_state, new_state)`: Preserves valid selections during changes\n- `ensure_consistency()`: Validates and corrects session state inconsistencies\n\n## Data Models\n\n### Session State Structure\n\n```python\nst.session_state.config_ui_state = {\n    # Selection state\n    'selected_publisher': str,\n    'selected_imprint': str, \n    'selected_tranche': str,\n    \n    # UI state\n    'display_mode': str,\n    'current_config': dict,\n    'validation_results': ValidationResult,\n    'expanded_groups': set,\n    \n    # Control flags (NEW)\n    'dropdown_update_pending': bool,\n    'validation_in_progress': bool,\n    'last_update_timestamp': float,\n    'update_debounce_key': str,\n    \n    # Cache (NEW)\n    'publisher_imprints_cache': dict,\n    'imprint_tranches_cache': dict,\n    'last_cache_update': float\n}\n```\n\n### Update Event Model\n\n```python\n@dataclass\nclass UpdateEvent:\n    event_type: str  # 'publisher_change', 'validation', 'config_load'\n    old_values: dict\n    new_values: dict\n    timestamp: float\n    requires_refresh: bool\n```\n\n## Error Handling\n\n### Rerun Loop Prevention\n\n1. **Update Flags**: Use boolean flags to prevent multiple simultaneous updates\n2. **Timestamp Tracking**: Track update timestamps to implement debouncing\n3. **State Validation**: Validate state consistency before triggering updates\n4. **Error Recovery**: Implement fallback mechanisms for corrupted state\n\n### Validation Error Handling\n\n1. **Safe Validation**: Wrap validation in try-catch blocks\n2. **Result Caching**: Cache validation results to prevent redundant validations\n3. **State Isolation**: Isolate validation state from UI state\n4. **Error Display**: Show validation errors without triggering reruns\n\n## Testing Strategy\n\n### Unit Tests\n\n1. **DropdownManager Tests**:\n   - Test publisher change handling without reruns\n   - Test dependent dropdown refresh logic\n   - Test debouncing mechanisms\n\n2. **ValidationManager Tests**:\n   - Test validation without rerun loops\n   - Test validation result display stability\n   - Test validation state management\n\n3. **StateManager Tests**:\n   - Test atomic state updates\n   - Test selection preservation logic\n   - Test state consistency validation\n\n### Integration Tests\n\n1. **UI Interaction Tests**:\n   - Test complete publisher  imprint  tranche selection flow\n   - Test validation button behavior\n   - Test configuration loading with dropdown updates\n\n2. **Performance Tests**:\n   - Test response time for dropdown updates\n   - Test memory usage with caching\n   - Test debouncing effectiveness\n\n### Manual Testing Scenarios\n\n1. **Dropdown Refresh Test**:\n   - Select \"nimble_books\"  verify \"xynapse_traces\" appears\n   - Change publisher  verify dependent dropdowns clear and repopulate\n   - Rapid publisher changes  verify no rerun loops\n\n2. **Validation Test**:\n   - Click \"Validate Only\"  verify single validation execution\n   - Multiple validation clicks  verify no runaway loops\n   - Validation with errors  verify stable error display\n\n## Implementation Plan\n\n### Phase 1: Core Infrastructure\n1. Create DropdownManager, ValidationManager, and StateManager classes\n2. Implement session state structure updates\n3. Add update event model and debouncing mechanisms\n\n### Phase 2: Dropdown Fix\n1. Replace direct `st.rerun()` calls with controlled updates\n2. Implement publisher change detection and handling\n3. Add dependent dropdown refresh logic with caching\n\n### Phase 3: Validation Fix\n1. Implement safe validation without reruns\n2. Add validation state management and loop prevention\n3. Create stable validation result display\n\n### Phase 4: Integration and Testing\n1. Integrate all components into ConfigurationUI\n2. Update Book Pipeline page to use new managers\n3. Comprehensive testing and debugging\n\n## Performance Considerations\n\n### Caching Strategy\n- Cache publisher  imprints mappings\n- Cache imprint  tranches mappings\n- Implement cache invalidation based on file system changes\n\n### Debouncing Strategy\n- 100ms debounce for dropdown changes\n- 500ms debounce for configuration loading\n- 1000ms debounce for validation operations\n\n### Memory Management\n- Limit cache size to prevent memory leaks\n- Implement LRU eviction for old cache entries\n- Clear temporary state flags after operations complete\n\n## Security Considerations\n\n### State Integrity\n- Validate session state structure on access\n- Prevent state corruption from malformed updates\n- Implement state recovery mechanisms\n\n### Input Validation\n- Validate dropdown selections against available options\n- Sanitize configuration data before validation\n- Prevent injection attacks through configuration parameters\n\n## Deployment Strategy\n\n### Rollout Plan\n1. Deploy to development environment for testing\n2. Conduct user acceptance testing with key workflows\n3. Deploy to production with monitoring for rerun loops\n4. Monitor performance metrics and user feedback\n\n### Monitoring\n- Track rerun loop occurrences\n- Monitor dropdown refresh performance\n- Log validation execution times\n- Alert on excessive session state updates\n\n## Success Metrics\n\n### Functional Metrics\n- Zero rerun loops during normal operation\n- < 200ms dropdown refresh time\n- 100% validation accuracy without loops\n- Zero session state corruption incidents\n\n### User Experience Metrics\n- Improved user workflow completion rates\n- Reduced support tickets for UI issues\n- Positive user feedback on responsiveness\n- Decreased time to complete configuration tasks",
      "size": 7993,
      "modified": "2025-08-02T22:32:50.340227",
      "spec_directory": "streamlit-ui-runaway-fixes",
      "directory_modified": "2025-08-02T22:40:42.397820"
    },
    {
      "file": "design.md",
      "content": "# Configuration Synchronization Fix Design\n\n## Overview\n\nThis design addresses the disconnect between Configuration Selection (outside the form) and Core Settings (inside the form) by implementing a synchronization mechanism that automatically populates core settings with configuration values while maintaining user override capabilities.\n\n## Architecture\n\n### Current State Problem\n```\nConfiguration Selection (Outside Form)\n Publisher: \"nimble_books\"  Selected\n Imprint: \"xynapse_traces\"  ",
      "full_content": "# Configuration Synchronization Fix Design\n\n## Overview\n\nThis design addresses the disconnect between Configuration Selection (outside the form) and Core Settings (inside the form) by implementing a synchronization mechanism that automatically populates core settings with configuration values while maintaining user override capabilities.\n\n## Architecture\n\n### Current State Problem\n```\nConfiguration Selection (Outside Form)\n Publisher: \"nimble_books\"  Selected\n Imprint: \"xynapse_traces\"  Selected  \n Tranche: \"\" (optional)\n\nCore Settings Form (Inside Form)\n Publisher: \"\"  Empty (validation error)\n Imprint: \"\"  Empty (validation error)\n Other settings...\n\nResult: Validation fails despite valid configuration selection\n```\n\n### Proposed Solution Architecture\n```\nConfiguration Selection (Outside Form)\n Publisher: \"nimble_books\"  Selected\n Imprint: \"xynapse_traces\"  Selected  \n Tranche: \"\" (optional)\n        Synchronization Bridge\nCore Settings Form (Inside Form)  \n Publisher: \"nimble_books\"  Auto-populated\n Imprint: \"xynapse_traces\"  Auto-populated\n Other settings...\n\nResult: Validation passes with synchronized values\n```\n\n## Components and Interfaces\n\n### 1. Configuration Synchronization Manager\n\n```python\nclass ConfigurationSynchronizer:\n    \"\"\"Manages synchronization between configuration selection and core settings\"\"\"\n    \n    def __init__(self):\n        self.session_key = 'config_sync_state'\n        self.override_tracking = {}\n    \n    def sync_config_to_form(self, publisher: str, imprint: str, tranche: str) -> Dict[str, Any]:\n        \"\"\"Synchronize configuration selection to form defaults\"\"\"\n        \n    def track_user_override(self, field_name: str, value: Any) -> None:\n        \"\"\"Track when user manually overrides a configuration-derived value\"\"\"\n        \n    def get_effective_value(self, field_name: str, form_value: Any, config_value: Any) -> Any:\n        \"\"\"Get the effective value considering overrides and configuration\"\"\"\n        \n    def get_sync_status(self) -> Dict[str, str]:\n        \"\"\"Get synchronization status for UI indicators\"\"\"\n```\n\n### 2. Enhanced Form Data Builder\n\n```python\nclass EnhancedFormDataBuilder:\n    \"\"\"Builds form data with configuration synchronization\"\"\"\n    \n    def build_form_data_with_config_sync(\n        self, \n        publisher: str, \n        imprint: str, \n        tranche: str,\n        existing_form_data: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Build form data with configuration values as defaults\"\"\"\n        \n        # Start with configuration defaults\n        form_data = {\n            'publisher': publisher,\n            'imprint': imprint,\n            'tranche': tranche\n        }\n        \n        # Apply user overrides if they exist\n        sync_manager = ConfigurationSynchronizer()\n        for field, value in existing_form_data.items():\n            effective_value = sync_manager.get_effective_value(field, value, form_data.get(field))\n            form_data[field] = effective_value\n            \n        return form_data\n```\n\n### 3. Validation Enhancement\n\n```python\nclass ConfigurationAwareValidator:\n    \"\"\"Enhanced validator that considers configuration selection\"\"\"\n    \n    def validate_with_config_context(\n        self, \n        form_data: Dict[str, Any],\n        config_selection: Dict[str, str]\n    ) -> ValidationResult:\n        \"\"\"Validate considering both form data and configuration selection\"\"\"\n        \n        # Merge configuration selection into validation context\n        validation_data = form_data.copy()\n        \n        # Use configuration values for empty required fields\n        if not validation_data.get('publisher') and config_selection.get('publisher'):\n            validation_data['publisher'] = config_selection['publisher']\n            \n        if not validation_data.get('imprint') and config_selection.get('imprint'):\n            validation_data['imprint'] = config_selection['imprint']\n            \n        # Run validation on merged data\n        return self.validate_configuration(validation_data)\n```\n\n### 4. UI Synchronization Components\n\n```python\nclass SynchronizedFormRenderer:\n    \"\"\"Renders form elements with configuration synchronization\"\"\"\n    \n    def render_synchronized_field(\n        self, \n        field_name: str,\n        field_config: Dict[str, Any],\n        config_value: Any,\n        form_value: Any\n    ) -> Any:\n        \"\"\"Render a form field with configuration synchronization\"\"\"\n        \n        # Determine effective value\n        effective_value = form_value if form_value else config_value\n        \n        # Add visual indicators for configuration-derived values\n        if effective_value == config_value and not form_value:\n            # Show as configuration-derived\n            help_text = f\"Auto-populated from configuration: {config_value}\"\n            placeholder = f\"From configuration: {config_value}\"\n        else:\n            # Show as user-entered\n            help_text = field_config.get('help', '')\n            placeholder = field_config.get('placeholder', '')\n            \n        return self.render_field_with_indicators(\n            field_name, effective_value, help_text, placeholder\n        )\n```\n\n## Data Models\n\n### Configuration Sync State\n\n```python\n@dataclass\nclass ConfigSyncState:\n    \"\"\"State tracking for configuration synchronization\"\"\"\n    publisher: str\n    imprint: str\n    tranche: str\n    user_overrides: Dict[str, Any]\n    last_sync_timestamp: datetime\n    sync_status: Dict[str, str]  # field_name -> 'config' | 'user' | 'mixed'\n\n@dataclass\nclass SyncStatus:\n    \"\"\"Status of field synchronization\"\"\"\n    field_name: str\n    source: str  # 'configuration' | 'user_input' | 'default'\n    value: Any\n    is_overridden: bool\n    config_value: Any\n    user_value: Any\n```\n\n## Error Handling\n\n### Synchronization Failures\n\n```python\nclass ConfigSyncError(Exception):\n    \"\"\"Base exception for configuration synchronization errors\"\"\"\n    pass\n\nclass ConfigSyncManager:\n    def safe_sync_config_to_form(self, publisher: str, imprint: str, tranche: str) -> Dict[str, Any]:\n        \"\"\"Safely synchronize configuration with error handling\"\"\"\n        try:\n            return self.sync_config_to_form(publisher, imprint, tranche)\n        except Exception as e:\n            logger.error(f\"Configuration sync failed: {e}\")\n            # Return minimal safe defaults\n            return {\n                'publisher': publisher or '',\n                'imprint': imprint or '',\n                'tranche': tranche or ''\n            }\n```\n\n### Validation Error Context\n\n```python\nclass ConfigAwareValidationError(ValidationError):\n    \"\"\"Validation error with configuration context\"\"\"\n    def __init__(self, field_name: str, error_message: str, config_context: Dict[str, Any]):\n        super().__init__(field_name, error_message, 'config_sync')\n        self.config_context = config_context\n        self.suggested_fix = self._generate_suggested_fix()\n    \n    def _generate_suggested_fix(self) -> str:\n        \"\"\"Generate context-aware fix suggestions\"\"\"\n        if self.field_name in ['publisher', 'imprint']:\n            return f\"Select a {self.field_name} in the Configuration Selection section above\"\n        return f\"Provide a value for {self.field_name}\"\n```\n\n## Testing Strategy\n\n### Unit Tests\n\n```python\nclass TestConfigurationSynchronizer:\n    def test_sync_config_to_form_basic(self):\n        \"\"\"Test basic configuration to form synchronization\"\"\"\n        \n    def test_user_override_tracking(self):\n        \"\"\"Test tracking of user overrides\"\"\"\n        \n    def test_effective_value_resolution(self):\n        \"\"\"Test resolution of effective values with overrides\"\"\"\n\nclass TestConfigurationAwareValidator:\n    def test_validation_with_config_context(self):\n        \"\"\"Test validation considering configuration selection\"\"\"\n        \n    def test_validation_error_messages(self):\n        \"\"\"Test context-aware validation error messages\"\"\"\n```\n\n### Integration Tests\n\n```python\nclass TestConfigSyncIntegration:\n    def test_end_to_end_sync_workflow(self):\n        \"\"\"Test complete synchronization workflow\"\"\"\n        \n    def test_form_submission_with_sync(self):\n        \"\"\"Test form submission with synchronized values\"\"\"\n        \n    def test_validation_with_sync(self):\n        \"\"\"Test validation with synchronized configuration\"\"\"\n```\n\n## Implementation Plan\n\n### Phase 1: Core Synchronization\n1. Implement `ConfigurationSynchronizer` class\n2. Create `EnhancedFormDataBuilder` for form data merging\n3. Add session state management for sync tracking\n4. Basic synchronization between configuration and form\n\n### Phase 2: Validation Enhancement\n1. Implement `ConfigurationAwareValidator`\n2. Update validation logic to consider configuration context\n3. Enhanced error messages with configuration context\n4. Validation result improvements\n\n### Phase 3: UI Enhancements\n1. Implement `SynchronizedFormRenderer`\n2. Add visual indicators for configuration-derived values\n3. Tooltips and help text for synchronized fields\n4. Real-time sync feedback\n\n### Phase 4: Error Handling & Polish\n1. Comprehensive error handling for sync failures\n2. Graceful degradation when sync is unavailable\n3. Performance optimization for real-time sync\n4. User experience polish and testing\n\n## Technical Considerations\n\n### Performance\n- Minimize re-renders during synchronization\n- Cache configuration values to avoid repeated lookups\n- Debounce rapid configuration changes\n\n### State Management\n- Use Streamlit session state for persistence\n- Atomic updates to prevent race conditions\n- Clear separation between configuration and user state\n\n### Backward Compatibility\n- Maintain existing form behavior when configuration is not selected\n- Graceful fallback to original validation logic\n- Support for legacy configuration formats\n\n### User Experience\n- Immediate visual feedback on synchronization\n- Clear distinction between auto-populated and user-entered values\n- Intuitive override behavior with clear reset options",
      "size": 10026,
      "modified": "2025-08-02T23:29:16.439528",
      "spec_directory": "config-sync-fix",
      "directory_modified": "2025-08-02T23:36:25.671807"
    },
    {
      "file": "design.md",
      "content": "# Book Production Fixes and Enhancements Design\n\n## Overview\n\nThis design addresses critical production issues in the book generation pipeline through systematic improvements to typography, error handling, reporting accuracy, caching systems, and new configuration features. The solution focuses on professional publishing standards while maintaining system efficiency and reliability.\n\n## Architecture\n\n### Current Issues Analysis\n```\nBibliography  No hanging indent  Unprofessional appearance\nISB",
      "full_content": "# Book Production Fixes and Enhancements Design\n\n## Overview\n\nThis design addresses critical production issues in the book generation pipeline through systematic improvements to typography, error handling, reporting accuracy, caching systems, and new configuration features. The solution focuses on professional publishing standards while maintaining system efficiency and reliability.\n\n## Architecture\n\n### Current Issues Analysis\n```\nBibliography  No hanging indent  Unprofessional appearance\nISBN Lookup  Repeated API calls  Inefficient processing  \nReporting  Shows 0 quotes  Inaccurate statistics\nError Handling  Generic failures  Difficult debugging\nTypography  Inconsistent formatting  Poor visual quality\n```\n\n### Proposed Solution Architecture\n```\nEnhanced Bibliography System\n Memoir Citation Fields\n 0.15 Hanging Indent\n Consistent Formatting\n\nIntelligent ISBN Caching\n JSON Cache Storage\n Duplicate Detection\n Efficient Lookups\n\nAccurate Reporting System\n Real-time Statistics\n Success Rate Tracking\n Detailed Analytics\n\nRobust Error Handling\n Detailed Logging\n Graceful Fallbacks\n Debug Information\n\nProfessional Typography\n Font Management\n Layout Optimization\n Standards Compliance\n```\n\n## Components and Interfaces\n\n### 1. Bibliography Formatting System\n\n```python\nclass BibliographyFormatter:\n    \"\"\"Enhanced bibliography formatting with memoir citation fields\"\"\"\n    \n    def __init__(self, memoir_config: Dict[str, Any]):\n        self.hanging_indent = \"0.15in\"\n        self.citation_style = memoir_config.get('citation_style', 'chicago')\n    \n    def format_bibliography_entry(self, entry: Dict[str, str]) -> str:\n        \"\"\"Format a single bibliography entry with hanging indent\"\"\"\n        \n    def apply_memoir_citation_field(self, entries: List[Dict]) -> str:\n        \"\"\"Apply memoir citation field formatting to bibliography\"\"\"\n        \n    def generate_latex_bibliography(self, entries: List[Dict]) -> str:\n        \"\"\"Generate LaTeX code for properly formatted bibliography\"\"\"\n```\n\n### 2. ISBN Caching and Lookup System\n\n```python\nclass ISBNLookupCache:\n    \"\"\"Intelligent ISBN lookup with persistent caching\"\"\"\n    \n    def __init__(self, cache_file: str = \"isbn_cache.json\"):\n        self.cache_file = cache_file\n        self.cache_data = self._load_cache()\n        self.processed_documents = set()\n    \n    def lookup_isbn(self, isbn: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Lookup ISBN with caching to avoid duplicate API calls\"\"\"\n        \n    def scan_document_for_isbns(self, document_id: str, content: str) -> List[str]:\n        \"\"\"Scan document for ISBNs, avoiding duplicate scans\"\"\"\n        \n    def cache_isbn_data(self, isbn: str, data: Dict[str, Any]) -> None:\n        \"\"\"Cache ISBN lookup results for future use\"\"\"\n        \n    def is_document_processed(self, document_id: str) -> bool:\n        \"\"\"Check if document has already been scanned for ISBNs\"\"\"\n```\n\n### 3. Enhanced Reporting System\n\n```python\nclass AccurateReportingSystem:\n    \"\"\"Accurate tracking and reporting of prompt success and quote retrieval\"\"\"\n    \n    def __init__(self):\n        self.prompt_stats = {}\n        self.quote_stats = {}\n        self.processing_stats = {}\n    \n    def track_prompt_execution(self, prompt_name: str, success: bool, details: Dict) -> None:\n        \"\"\"Track individual prompt execution results\"\"\"\n        \n    def track_quote_retrieval(self, book_id: str, quotes_retrieved: int, quotes_requested: int) -> None:\n        \"\"\"Track quote retrieval statistics accurately\"\"\"\n        \n    def generate_accurate_report(self) -> Dict[str, Any]:\n        \"\"\"Generate report with accurate statistics matching actual results\"\"\"\n        \n    def validate_reporting_accuracy(self, expected_results: Dict) -> bool:\n        \"\"\"Validate that reported statistics match actual execution results\"\"\"\n```\n\n### 4. Enhanced Error Handling System\n\n```python\nclass EnhancedErrorHandler:\n    \"\"\"Comprehensive error handling with detailed logging and recovery\"\"\"\n    \n    def __init__(self, logger: logging.Logger):\n        self.logger = logger\n        self.error_context = {}\n    \n    def handle_quote_verification_error(self, response: Dict, context: Dict) -> None:\n        \"\"\"Handle quote verification failures with detailed logging\"\"\"\n        \n    def handle_field_completion_error(self, error: Exception, field_name: str) -> Any:\n        \"\"\"Handle field completion errors with fallback behavior\"\"\"\n        \n    def handle_validation_error(self, validation_result: Dict) -> None:\n        \"\"\"Handle validation errors with context and recovery suggestions\"\"\"\n        \n    def log_error_with_context(self, error: Exception, context: Dict) -> None:\n        \"\"\"Log errors with comprehensive context for debugging\"\"\"\n```\n\n### 5. Typography and Layout Manager\n\n```python\nclass TypographyManager:\n    \"\"\"Professional typography and layout management\"\"\"\n    \n    def __init__(self, imprint_config: Dict[str, Any]):\n        self.fonts = self._load_font_config(imprint_config)\n        self.layout_settings = self._load_layout_config(imprint_config)\n    \n    def format_mnemonics_page(self, mnemonics: List[Dict]) -> str:\n        \"\"\"Format mnemonics pages with Adobe Caslon and proper bullet structure\"\"\"\n        \n    def format_title_page_korean(self, korean_text: str) -> str:\n        \"\"\"Format Korean characters on title page with Apple Myungjo font\"\"\"\n        \n    def add_instruction_pages(self, content: str) -> str:\n        \"\"\"Add instructions on every 8th recto page bottom\"\"\"\n        \n    def adjust_chapter_heading_leading(self, chapter_content: str) -> str:\n        \"\"\"Reduce leading underneath chapter headings to 36 points\"\"\"\n```\n\n### 6. Glossary Layout System\n\n```python\nclass GlossaryLayoutManager:\n    \"\"\"Two-column glossary layout with Korean/English term stacking\"\"\"\n    \n    def __init__(self, page_config: Dict[str, Any]):\n        self.column_count = 2\n        self.page_text_area = page_config.get('text_area')\n    \n    def format_glossary_two_column(self, terms: List[Dict]) -> str:\n        \"\"\"Format glossary in exactly 2 columns within page text area\"\"\"\n        \n    def stack_korean_english_terms(self, korean_term: str, english_term: str) -> str:\n        \"\"\"Stack Korean and English terms in left-hand cells\"\"\"\n        \n    def distribute_terms_across_columns(self, terms: List[Dict]) -> Tuple[List, List]:\n        \"\"\"Distribute glossary terms evenly across both columns\"\"\"\n```\n\n### 7. Writing Style Configuration System\n\n```python\nclass WritingStyleManager:\n    \"\"\"Hierarchical writing style configuration system\"\"\"\n    \n    def __init__(self):\n        self.style_hierarchy = ['tranche', 'imprint', 'publisher']\n    \n    def load_writing_style(self, tranche: str, imprint: str, publisher: str) -> Dict[str, Any]:\n        \"\"\"Load writing style configuration with proper hierarchy\"\"\"\n        \n    def construct_style_prompt(self, style_config: Dict[str, Any]) -> str:\n        \"\"\"Construct single prompt from multiple text values in style config\"\"\"\n        \n    def apply_style_to_prompt(self, original_prompt: str, style_prompt: str) -> str:\n        \"\"\"Append style configuration to original prompt\"\"\"\n        \n    def validate_style_config(self, config_path: str) -> bool:\n        \"\"\"Validate writing_style.json file format and content\"\"\"\n```\n\n### 8. Quote Assembly Optimizer\n\n```python\nclass QuoteAssemblyOptimizer:\n    \"\"\"Optimize quote ordering to avoid excessive author repetition\"\"\"\n    \n    def __init__(self, max_consecutive_author: int = 3):\n        self.max_consecutive = max_consecutive_author\n    \n    def optimize_quote_order(self, quotes: List[Dict]) -> List[Dict]:\n        \"\"\"Reorder quotes to minimize author repetition while maintaining coherence\"\"\"\n        \n    def check_author_distribution(self, quotes: List[Dict]) -> Dict[str, int]:\n        \"\"\"Analyze author distribution in quote sequence\"\"\"\n        \n    def reorder_quotes_by_author(self, quotes: List[Dict]) -> List[Dict]:\n        \"\"\"Reorder quotes to improve author variety\"\"\"\n        \n    def validate_author_distribution(self, quotes: List[Dict]) -> bool:\n        \"\"\"Validate that no author appears more than 3 times consecutively\"\"\"\n```\n\n### 9. Tranche Configuration UI Manager\n\n```python\nclass TrancheConfigUIManager:\n    \"\"\"Manage tranche configuration display and selection in Book Pipeline UI\"\"\"\n    \n    def __init__(self, config_loader: MultiLevelConfigLoader):\n        self.config_loader = config_loader\n        self.available_tranches = []\n    \n    def load_available_tranches(self) -> List[Dict[str, Any]]:\n        \"\"\"Load all available tranche configurations for dropdown display\"\"\"\n        \n    def refresh_tranche_dropdown(self) -> List[str]:\n        \"\"\"Refresh tranche dropdown options in UI\"\"\"\n        \n    def validate_tranche_selection(self, tranche_name: str) -> bool:\n        \"\"\"Validate that selected tranche configuration exists and is valid\"\"\"\n        \n    def get_tranche_config(self, tranche_name: str) -> Dict[str, Any]:\n        \"\"\"Retrieve configuration for selected tranche\"\"\"\n```\n\n### 10. Publisher's Note Generator\n\n```python\nclass PublishersNoteGenerator:\n    \"\"\"Generate structured Publisher's Notes with specific formatting requirements\"\"\"\n    \n    def __init__(self, llm_caller: EnhancedLLMCaller):\n        self.llm_caller = llm_caller\n        self.max_paragraph_length = 600\n        self.required_paragraphs = 3\n    \n    def generate_publishers_note(self, book_metadata: Dict[str, Any]) -> str:\n        \"\"\"Generate 3-paragraph Publisher's Note with pilsa book explanation\"\"\"\n        \n    def validate_paragraph_length(self, paragraph: str) -> bool:\n        \"\"\"Validate paragraph does not exceed 600 character limit\"\"\"\n        \n    def ensure_pilsa_explanation(self, content: str) -> str:\n        \"\"\"Ensure pilsa book explanation is included exactly once\"\"\"\n        \n    def add_current_events_context(self, content: str, book_topic: str) -> str:\n        \"\"\"Add relevant current events without date-specific references\"\"\"\n```\n\n### 11. Mnemonics JSON Processor\n\n```python\nclass MnemonicsJSONProcessor:\n    \"\"\"Process mnemonics generation with proper JSON structure validation\"\"\"\n    \n    def __init__(self, llm_caller: EnhancedLLMCaller):\n        self.llm_caller = llm_caller\n        self.required_keys = ['mnemonics_data']\n    \n    def generate_mnemonics_with_validation(self, book_content: str) -> Dict[str, Any]:\n        \"\"\"Generate mnemonics ensuring proper JSON structure with required keys\"\"\"\n        \n    def validate_mnemonics_response(self, response: Dict[str, Any]) -> bool:\n        \"\"\"Validate that response contains expected mnemonics_data key\"\"\"\n        \n    def create_mnemonics_tex(self, mnemonics_data: Dict[str, Any]) -> str:\n        \"\"\"Create mnemonics.tex file from validated JSON data\"\"\"\n        \n    def handle_missing_keys_error(self, response: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Handle responses missing expected keys with fallback behavior\"\"\"\n```\n\n### 12. BISAC Category Analyzer\n\n```python\nclass BISACCategoryAnalyzer:\n    \"\"\"Analyze book content to generate specific, relevant BISAC categories\"\"\"\n    \n    def __init__(self, llm_caller: EnhancedLLMCaller):\n        self.llm_caller = llm_caller\n        self.generic_categories = ['Business>General', 'Self-Help>General']\n    \n    def analyze_content_for_categories(self, book_content: str) -> List[str]:\n        \"\"\"Analyze book content to determine specific BISAC categories\"\"\"\n        \n    def validate_category_specificity(self, categories: List[str]) -> List[str]:\n        \"\"\"Replace generic categories with more specific ones based on content\"\"\"\n        \n    def get_technical_categories(self, content_analysis: Dict[str, Any]) -> List[str]:\n        \"\"\"Generate technical BISAC categories based on content analysis\"\"\"\n        \n    def ensure_category_relevance(self, categories: List[str], book_metadata: Dict) -> List[str]:\n        \"\"\"Ensure all categories are relevant to the book's actual content\"\"\"\n```\n\n### 13. Pilsa Book Content Processor\n\n```python\nclass PilsaBookContentProcessor:\n    \"\"\"Ensure all book content properly identifies and describes pilsa format\"\"\"\n    \n    def __init__(self):\n        self.pilsa_identifiers = [\n            'pilsa book',\n            'transcriptive meditation handbook',\n            '90 quotations',\n            '90 facing pages for journaling'\n        ]\n    \n    def add_pilsa_identification(self, content: str, content_type: str) -> str:\n        \"\"\"Add pilsa book identification to various content types\"\"\"\n        \n    def validate_pilsa_description(self, content: str) -> bool:\n        \"\"\"Validate that content properly describes pilsa book format\"\"\"\n        \n    def enhance_back_cover_text(self, back_text: str) -> str:\n        \"\"\"Enhance back cover text with pilsa book description\"\"\"\n        \n    def update_marketing_copy(self, marketing_text: str) -> str:\n        \"\"\"Update marketing copy to emphasize meditative and journaling aspects\"\"\"\n```\n\n### 14. Last Verso Page Manager\n\n```python\nclass LastVersoPageManager:\n    \"\"\"Manage last verso page formatting with Notes heading when blank\"\"\"\n    \n    def __init__(self, typography_manager: TypographyManager):\n        self.typography_manager = typography_manager\n    \n    def check_last_verso_blank(self, book_content: str) -> bool:\n        \"\"\"Check if the last verso page is blank\"\"\"\n        \n    def add_notes_heading(self, page_content: str) -> str:\n        \"\"\"Add 'Notes' chapter heading to blank last verso page\"\"\"\n        \n    def format_notes_page(self) -> str:\n        \"\"\"Format Notes page with proper chapter heading styling\"\"\"\n        \n    def validate_notes_page_position(self, book_structure: Dict) -> bool:\n        \"\"\"Validate Notes page is properly positioned as final verso\"\"\"\n```\n\n### 15. ISBN Barcode Generator\n\n```python\nclass ISBNBarcodeGenerator:\n    \"\"\"Generate UPC-A barcodes for ISBN-13 with proper formatting\"\"\"\n    \n    def __init__(self, barcode_config: Dict[str, Any]):\n        self.barcode_settings = barcode_config\n        self.upc_format = \"UPC-A\"\n    \n    def generate_upc_barcode(self, isbn13: str) -> bytes:\n        \"\"\"Generate UPC-A barcode from ISBN-13\"\"\"\n        \n    def format_barcode_numerals(self, isbn13: str) -> str:\n        \"\"\"Format bar-code-reader numerals for display\"\"\"\n        \n    def integrate_barcode_to_cover(self, cover_template: str, isbn13: str) -> str:\n        \"\"\"Integrate barcode into back cover design\"\"\"\n        \n    def validate_barcode_standards(self, barcode_data: bytes) -> bool:\n        \"\"\"Validate barcode meets industry standards for retail scanning\"\"\"\n```\n\n### 16. Storefront Metadata Manager\n\n```python\nclass StorefrontMetadataManager:\n    \"\"\"Manage storefront metadata with accurate author information from tranche config\"\"\"\n    \n    def __init__(self, config_loader: MultiLevelConfigLoader):\n        self.config_loader = config_loader\n    \n    def extract_author_from_tranche(self, tranche_name: str) -> Dict[str, str]:\n        \"\"\"Extract Contributor One name from tranche configuration\"\"\"\n        \n    def generate_storefront_metadata(self, book_metadata: Dict, tranche_config: Dict) -> Dict[str, Any]:\n        \"\"\"Generate complete storefront metadata with accurate author information\"\"\"\n        \n    def validate_author_consistency(self, lsi_data: Dict, storefront_data: Dict) -> bool:\n        \"\"\"Validate author consistency between LSI CSV and storefront data\"\"\"\n        \n    def prevent_author_interpolation(self, metadata: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Ensure author names are not interpolated by the model\"\"\"\n```\n\n## Data Models\n\n### Bibliography Entry Model\n\n```python\n@dataclass\nclass BibliographyEntry:\n    \"\"\"Bibliography entry with formatting metadata\"\"\"\n    author: str\n    title: str\n    publisher: str\n    year: str\n    isbn: Optional[str]\n    citation_style: str = \"chicago\"\n    hanging_indent: str = \"0.15in\"\n```\n\n### ISBN Cache Model\n\n```python\n@dataclass\nclass ISBNCacheEntry:\n    \"\"\"Cached ISBN lookup result\"\"\"\n    isbn: str\n    title: str\n    author: str\n    publisher: str\n    publication_date: str\n    lookup_timestamp: datetime\n    source: str  # API source used for lookup\n```\n\n### Reporting Statistics Model\n\n```python\n@dataclass\nclass ProcessingStatistics:\n    \"\"\"Comprehensive processing statistics\"\"\"\n    total_prompts: int\n    successful_prompts: int\n    failed_prompts: int\n    total_quotes_requested: int\n    total_quotes_retrieved: int\n    processing_time: float\n    error_details: List[str]\n```\n\n### Publisher's Note Model\n\n```python\n@dataclass\nclass PublishersNote:\n    \"\"\"Publisher's Note with formatting constraints\"\"\"\n    paragraphs: List[str]\n    max_paragraph_length: int = 600\n    required_paragraph_count: int = 3\n    pilsa_explanation_included: bool = False\n    current_events_context: Optional[str] = None\n```\n\n### Mnemonics Data Model\n\n```python\n@dataclass\nclass MnemonicsData:\n    \"\"\"Mnemonics data structure for JSON validation\"\"\"\n    mnemonics_data: List[Dict[str, Any]]\n    acronym: Optional[str] = None\n    expansion: Optional[str] = None\n    explanation: Optional[str] = None\n    validation_status: str = \"pending\"\n```\n\n### BISAC Category Model\n\n```python\n@dataclass\nclass BISACCategory:\n    \"\"\"BISAC category with specificity validation\"\"\"\n    category_code: str\n    category_name: str\n    specificity_level: str  # 'generic', 'specific', 'technical'\n    content_relevance_score: float\n    source: str  # 'content_analysis', 'manual', 'fallback'\n```\n\n### Storefront Metadata Model\n\n```python\n@dataclass\nclass StorefrontMetadata:\n    \"\"\"Storefront metadata with author validation\"\"\"\n    title: str\n    storefront_author_en: str\n    storefront_author_ko: str\n    contributor_one_name: str  # From tranche config\n    author_source: str  # 'tranche_config', 'interpolated', 'manual'\n    metadata_consistency_validated: bool = False\n```\n\n## Error Handling\n\n### Quote Verification Error Recovery\n\n```python\nclass QuoteVerificationErrorHandler:\n    def handle_invalid_response(self, response: Dict, book_context: Dict) -> Dict:\n        \"\"\"Handle invalid verifier model responses with fallback\"\"\"\n        try:\n            # Attempt to extract partial data\n            if 'verified_quotes' not in response:\n                self.logger.error(f\"Missing 'verified_quotes' in response: {response}\")\n                # Return empty verified quotes list to continue processing\n                return {'verified_quotes': [], 'verification_status': 'failed'}\n            return response\n        except Exception as e:\n            self.logger.error(f\"Quote verification recovery failed: {e}\")\n            return {'verified_quotes': [], 'verification_status': 'error'}\n```\n\n### Field Completion Error Recovery\n\n```python\nclass FieldCompletionErrorHandler:\n    def handle_missing_method_error(self, completer_obj: Any, method_name: str) -> Any:\n        \"\"\"Handle missing method errors in field completion\"\"\"\n        try:\n            if hasattr(completer_obj, 'complete_field_safe'):\n                return completer_obj.complete_field_safe\n            elif hasattr(completer_obj, 'complete_field_fallback'):\n                return completer_obj.complete_field_fallback\n            else:\n                self.logger.error(f\"No fallback method available for {method_name}\")\n                return lambda *args, **kwargs: None\n        except Exception as e:\n            self.logger.error(f\"Field completion error recovery failed: {e}\")\n            return lambda *args, **kwargs: None\n```\n\n## Testing Strategy\n\n### Unit Tests\n\n```python\nclass TestBibliographyFormatting:\n    def test_hanging_indent_application(self):\n        \"\"\"Test that 0.15 hanging indent is properly applied\"\"\"\n        \n    def test_memoir_citation_field_integration(self):\n        \"\"\"Test memoir citation field formatting\"\"\"\n\nclass TestISBNCaching:\n    def test_cache_persistence(self):\n        \"\"\"Test that ISBN cache persists across sessions\"\"\"\n        \n    def test_duplicate_scan_prevention(self):\n        \"\"\"Test that documents are not scanned multiple times\"\"\"\n\nclass TestReportingAccuracy:\n    def test_quote_count_accuracy(self):\n        \"\"\"Test that reported quote counts match actual retrieval\"\"\"\n        \n    def test_prompt_success_rate_calculation(self):\n        \"\"\"Test accurate prompt success rate calculation\"\"\"\n```\n\n### Integration Tests\n\n```python\nclass TestBookProductionPipeline:\n    def test_end_to_end_bibliography_generation(self):\n        \"\"\"Test complete bibliography generation with proper formatting\"\"\"\n        \n    def test_isbn_lookup_and_caching_workflow(self):\n        \"\"\"Test ISBN lookup with caching in production workflow\"\"\"\n        \n    def test_accurate_reporting_integration(self):\n        \"\"\"Test that reporting accurately reflects pipeline execution\"\"\"\n```\n\n## Implementation Plan\n\n### Phase 1: Core Fixes\n1. Fix bibliography formatting with memoir citation fields and hanging indent\n2. Implement ISBN caching system to prevent duplicate lookups\n3. Fix reporting accuracy for prompt success and quote retrieval\n4. Enhance error handling for quote verification and field completion\n\n### Phase 2: Typography Enhancements\n1. Implement mnemonics page formatting with Adobe Caslon\n2. Add Korean character support with Apple Myungjo font\n3. Implement instruction page placement on every 8th recto\n4. Adjust chapter heading leading to 36 points\n\n### Phase 3: Layout and Configuration\n1. Implement 2-column glossary layout with term stacking\n2. Create writing style configuration system\n3. Implement quote assembly optimization for author distribution\n4. Add ISBN barcode generation with UPC-A format\n\n### Phase 4: Integration and Testing\n1. Integrate all components into existing pipeline\n2. Comprehensive testing of all fixes and enhancements\n3. Performance optimization and error handling validation\n4. Documentation and user guide updates\n\n## Technical Considerations\n\n### LaTeX Integration\n- Use memoir class citation fields for bibliography formatting\n- Implement proper font loading for Korean characters\n- Ensure column layout compatibility with existing templates\n\n### Performance Optimization\n- Implement efficient ISBN caching to reduce API calls\n- Optimize quote reordering algorithms for large datasets\n- Cache writing style configurations to avoid repeated file reads\n\n### Error Recovery\n- Implement graceful fallbacks for all critical operations\n- Ensure pipeline continues processing even when individual components fail\n- Provide detailed logging for debugging and monitoring\n\n### Backward Compatibility\n- Maintain compatibility with existing book templates\n- Ensure new features don't break existing workflows\n- Provide migration paths for existing configurations",
      "size": 22675,
      "modified": "2025-08-04T01:52:00.568115",
      "spec_directory": "book-production-fixes",
      "directory_modified": "2025-08-04T02:16:41.765212"
    },
    {
      "file": "design.md",
      "content": "# Design Document\n\n## Overview\n\nThe ISBN Schedule Assignment system is designed as a comprehensive solution for managing ISBN allocation across publishing schedules. The system follows a modular architecture with clear separation between data management, business logic, user interfaces, and external integrations. The design emphasizes data integrity, scalability, and ease of use while providing both programmatic and interactive access methods.\n\n## Architecture\n\n### System Components\n\n```mermaid\n",
      "full_content": "# Design Document\n\n## Overview\n\nThe ISBN Schedule Assignment system is designed as a comprehensive solution for managing ISBN allocation across publishing schedules. The system follows a modular architecture with clear separation between data management, business logic, user interfaces, and external integrations. The design emphasizes data integrity, scalability, and ease of use while providing both programmatic and interactive access methods.\n\n## Architecture\n\n### System Components\n\n```mermaid\ngraph TB\n    UI[User Interfaces] --> Core[Core ISBN Scheduler]\n    CLI[Command Line Interface] --> Core\n    API[REST API] --> Core\n    Core --> Storage[Data Storage Layer]\n    Core --> Validation[Validation Engine]\n    Core --> Reporting[Reporting Engine]\n    Storage --> JSON[JSON Files]\n    Storage --> CSV[CSV Export/Import]\n    Validation --> ISBN[ISBN Validation]\n    Validation --> Business[Business Rules]\n    Reporting --> Analytics[Usage Analytics]\n    Reporting --> Export[Export Formats]\n```\n\n### Data Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant UI\n    participant Scheduler\n    participant Storage\n    participant Validator\n    \n    User->>UI: Schedule ISBN Assignment\n    UI->>Validator: Validate Input Data\n    Validator-->>UI: Validation Result\n    UI->>Scheduler: Create Assignment\n    Scheduler->>Scheduler: Find Available ISBN\n    Scheduler->>Storage: Save Assignment\n    Storage-->>Scheduler: Confirmation\n    Scheduler-->>UI: Assignment Created\n    UI-->>User: Success/Error Message\n```\n\n## Components and Interfaces\n\n### Core ISBN Scheduler (`isbn_scheduler.py`)\n\nThe central component that manages all ISBN operations and maintains system state.\n\n**Key Classes:**\n- `ISBNScheduler`: Main orchestrator class\n- `ISBNAssignment`: Data model for individual assignments\n- `ISBNBlock`: Data model for ISBN blocks/ranges\n- `ISBNStatus`: Enumeration of assignment statuses\n\n**Primary Methods:**\n```python\nclass ISBNScheduler:\n    def add_isbn_block(prefix: str, start: int, end: int, publisher_code: str) -> str\n    def schedule_isbn_assignment(title: str, book_id: str, date: str, **kwargs) -> Optional[str]\n    def assign_isbn_now(isbn: str) -> bool\n    def reserve_isbn(isbn: str, reason: str) -> bool\n    def get_scheduled_assignments(start_date: str, end_date: str) -> List[ISBNAssignment]\n    def get_isbn_availability_report() -> Dict[str, Any]\n    def bulk_schedule_from_csv(csv_file: str) -> int\n```\n\n### Streamlit User Interface (`12_ISBN_Schedule_Manager.py`)\n\nInteractive web-based interface providing comprehensive ISBN management capabilities.\n\n**Page Structure:**\n- **Dashboard**: Overview metrics and upcoming assignments\n- **Schedule Assignment**: Form-based ISBN scheduling\n- **View Assignments**: Filterable assignment list with management actions\n- **Manage ISBN Blocks**: Block creation and utilization monitoring\n- **Reports**: Analytics and export functionality\n- **Bulk Operations**: CSV upload and batch processing\n\n**Key Features:**\n- Real-time data updates with session state management\n- Interactive filtering and search capabilities\n- Visual charts and metrics display\n- Export functionality for reports and data\n- Form validation and error handling\n\n### Command Line Interface (`isbn_schedule_cli.py`)\n\nComprehensive CLI tool for automation and scripting integration.\n\n**Available Commands:**\n```bash\n# Block management\npython isbn_schedule_cli.py add-block --prefix 978 --publisher-code 123456 --start 1000 --end 1999\n\n# Assignment scheduling\npython isbn_schedule_cli.py schedule --title \"My Book\" --book-id book_1 --date 2024-12-01\n\n# Assignment management\npython isbn_schedule_cli.py assign --isbn 9781234567890\npython isbn_schedule_cli.py reserve --isbn 9781234567890 --reason \"Special project\"\n\n# Reporting and listing\npython isbn_schedule_cli.py list --upcoming 30\npython isbn_schedule_cli.py report --format json\n\n# Bulk operations\npython isbn_schedule_cli.py bulk --csv-file assignments.csv\n```\n\n### Data Storage Layer\n\n**Primary Storage Format**: JSON files for structured data persistence\n- `configs/isbn_schedule.json`: Main schedule data file\n- Atomic write operations to prevent data corruption\n- Automatic backup creation before modifications\n- Schema versioning for future compatibility\n\n**Data Structure:**\n```json\n{\n  \"assignments\": {\n    \"isbn\": {\n      \"isbn\": \"string\",\n      \"book_title\": \"string\",\n      \"book_id\": \"string\",\n      \"scheduled_date\": \"YYYY-MM-DD\",\n      \"status\": \"scheduled|assigned|reserved|published\",\n      \"assigned_date\": \"ISO datetime\",\n      \"imprint\": \"string\",\n      \"publisher\": \"string\",\n      \"format\": \"paperback|hardcover|ebook\",\n      \"priority\": 1,\n      \"notes\": \"string\"\n    }\n  },\n  \"isbn_blocks\": {\n    \"block_id\": {\n      \"prefix\": \"978\",\n      \"start_number\": 1000,\n      \"end_number\": 1999,\n      \"publisher_code\": \"123456\",\n      \"imprint_code\": \"optional\",\n      \"total_count\": 1000,\n      \"used_count\": 0,\n      \"reserved_count\": 0\n    }\n  },\n  \"last_updated\": \"ISO datetime\",\n  \"version\": \"1.0\"\n}\n```\n\n## Data Models\n\n### ISBNAssignment Model\n\n```python\n@dataclass\nclass ISBNAssignment:\n    isbn: str                    # The assigned ISBN-13\n    book_title: str             # Book title\n    book_id: str                # Unique book identifier\n    scheduled_date: str         # YYYY-MM-DD format\n    assigned_date: Optional[str] # ISO datetime when assigned\n    status: str                 # Current assignment status\n    imprint: str                # Publishing imprint\n    publisher: str              # Publisher name\n    format: str                 # Book format (paperback/hardcover/ebook)\n    notes: str                  # Additional notes\n    priority: int               # Priority level (1=high, 2=medium, 3=low)\n```\n\n### ISBNBlock Model\n\n```python\n@dataclass\nclass ISBNBlock:\n    prefix: str          # ISBN prefix (usually 978 or 979)\n    start_number: int    # Starting number in range\n    end_number: int      # Ending number in range\n    publisher_code: str  # Publisher identifier code\n    imprint_code: str    # Optional imprint identifier\n    total_count: int     # Total ISBNs in block\n    used_count: int      # Number of ISBNs assigned\n    reserved_count: int  # Number of ISBNs reserved\n```\n\n### Status Enumeration\n\n```python\nclass ISBNStatus(Enum):\n    AVAILABLE = \"available\"    # ISBN exists but not scheduled\n    SCHEDULED = \"scheduled\"    # ISBN scheduled for future assignment\n    ASSIGNED = \"assigned\"      # ISBN assigned and ready for use\n    PUBLISHED = \"published\"    # ISBN used in published book\n    RESERVED = \"reserved\"      # ISBN reserved for special use\n```\n\n## Error Handling\n\n### Validation Strategy\n\n**Input Validation:**\n- ISBN format validation with check digit verification\n- Date format validation (YYYY-MM-DD)\n- Required field validation\n- Business rule validation (e.g., end_number > start_number)\n\n**Data Integrity:**\n- Atomic operations for critical data modifications\n- Transaction-like behavior for multi-step operations\n- Rollback capability for failed operations\n- Duplicate prevention mechanisms\n\n**Error Recovery:**\n- Graceful degradation when data files are corrupted\n- Automatic backup restoration options\n- Clear error messages with suggested remediation\n- Logging of all error conditions for debugging\n\n### Exception Hierarchy\n\n```python\nclass ISBNSchedulerError(Exception):\n    \"\"\"Base exception for ISBN scheduler operations\"\"\"\n    pass\n\nclass ISBNNotFoundError(ISBNSchedulerError):\n    \"\"\"Raised when requested ISBN doesn't exist\"\"\"\n    pass\n\nclass ISBNUnavailableError(ISBNSchedulerError):\n    \"\"\"Raised when no ISBNs are available for assignment\"\"\"\n    pass\n\nclass InvalidISBNFormatError(ISBNSchedulerError):\n    \"\"\"Raised when ISBN format is invalid\"\"\"\n    pass\n\nclass DataCorruptionError(ISBNSchedulerError):\n    \"\"\"Raised when data file corruption is detected\"\"\"\n    pass\n```\n\n## Testing Strategy\n\n### Unit Testing Approach\n\n**Core Functionality Tests:**\n- ISBN block creation and management\n- Assignment scheduling and status transitions\n- Data persistence and loading\n- ISBN formatting and validation\n- Error handling and edge cases\n\n**Test Coverage Requirements:**\n- Minimum 90% code coverage for core scheduler\n- 100% coverage for critical data operations\n- Comprehensive edge case testing\n- Performance testing for large datasets\n\n**Test Data Management:**\n- Isolated test environments with temporary files\n- Comprehensive test fixtures for various scenarios\n- Mock external dependencies\n- Automated test data cleanup\n\n### Integration Testing\n\n**Component Integration:**\n- Streamlit UI with core scheduler\n- CLI tool with core scheduler\n- Data persistence across sessions\n- Bulk operations with large datasets\n\n**End-to-End Workflows:**\n- Complete assignment lifecycle (schedule  assign  publish)\n- Multi-user scenarios with concurrent access\n- Error recovery and data consistency\n- Export/import functionality\n\n### Performance Testing\n\n**Scalability Targets:**\n- Support for 10,000+ ISBN assignments\n- Sub-second response times for common operations\n- Efficient memory usage for large datasets\n- Acceptable performance with 100+ concurrent users\n\n## Security Considerations\n\n### Data Protection\n\n**File System Security:**\n- Appropriate file permissions for data files\n- Protection against unauthorized access\n- Secure temporary file handling\n- Data sanitization for exports\n\n**Input Validation:**\n- SQL injection prevention (though not using SQL)\n- Path traversal prevention for file operations\n- Input sanitization for all user data\n- Rate limiting for bulk operations\n\n### Access Control\n\n**User Authentication:**\n- Integration with existing Codexes Factory auth system\n- Role-based access control (admin, editor, viewer)\n- Session management for web interface\n- API key authentication for programmatic access\n\n## Integration Points\n\n### Codexes Factory Integration\n\n**Metadata Integration:**\n- Automatic ISBN assignment during book pipeline\n- Integration with `CodexMetadata` objects\n- LSI CSV generation with assigned ISBNs\n- Cover generation with ISBN barcodes\n\n**Configuration Integration:**\n- Use existing multi-level configuration system\n- Imprint-specific ISBN block management\n- Publisher-specific assignment rules\n- Integration with existing logging infrastructure\n\n### External System Integration\n\n**ISBN Registration Authorities:**\n- Support for different ISBN agency formats\n- Validation against official ISBN databases\n- Integration with ISBN purchase workflows\n- Compliance with international ISBN standards\n\n**Publishing Platforms:**\n- IngramSpark LSI integration\n- Amazon KDP integration\n- Direct publisher platform connections\n- Automated submission workflows\n\n## Deployment Considerations\n\n### File Structure\n\n```\nsrc/codexes/modules/distribution/\n isbn_scheduler.py              # Core scheduler implementation\n isbn_validator.py              # ISBN validation utilities\n isbn_barcode_generator.py      # Barcode generation (existing)\n\nsrc/codexes/pages/\n 12_ISBN_Schedule_Manager.py    # Streamlit interface\n\ntools/\n isbn_schedule_cli.py           # Command line interface\n\ntests/\n test_isbn_scheduler.py         # Unit tests\n test_isbn_cli.py              # CLI tests\n test_isbn_ui.py               # UI tests\n\nconfigs/\n isbn_schedule.json            # Data storage file\n```\n\n### Configuration Requirements\n\n**Environment Variables:**\n- `ISBN_SCHEDULE_FILE`: Path to schedule data file\n- `ISBN_BACKUP_DIR`: Directory for automatic backups\n- `ISBN_LOG_LEVEL`: Logging verbosity level\n\n**Dependencies:**\n- No additional external dependencies required\n- Uses existing Codexes Factory infrastructure\n- Compatible with current Python 3.12+ requirements\n\n### Monitoring and Maintenance\n\n**Operational Monitoring:**\n- ISBN utilization tracking\n- Assignment success/failure rates\n- System performance metrics\n- Data file size and growth monitoring\n\n**Maintenance Tasks:**\n- Regular data backup verification\n- ISBN block utilization analysis\n- Performance optimization reviews\n- Security audit compliance",
      "size": 12055,
      "modified": "2025-08-05T01:29:34.611292",
      "spec_directory": "isbn-schedule-assignment",
      "directory_modified": "2025-08-05T02:05:32.467808"
    },
    {
      "file": "design.md",
      "content": "# Pipeline Configuration Fixes - Design Document\n\n## Overview\n\nThis design addresses critical configuration issues in the book production pipeline, ensuring consistent defaults, proper LLM configuration propagation, and accurate font rendering.\n\n## Architecture\n\n### Configuration Flow\n\n```mermaid\ngraph TD\n    A[Book Pipeline UI] --> B[Configuration Validation]\n    B --> C[Default Value Injection]\n    C --> D[Configuration Propagation]\n    D --> E[Stage 3 Processing]\n    E --> F[BackmatterProcess",
      "full_content": "# Pipeline Configuration Fixes - Design Document\n\n## Overview\n\nThis design addresses critical configuration issues in the book production pipeline, ensuring consistent defaults, proper LLM configuration propagation, and accurate font rendering.\n\n## Architecture\n\n### Configuration Flow\n\n```mermaid\ngraph TD\n    A[Book Pipeline UI] --> B[Configuration Validation]\n    B --> C[Default Value Injection]\n    C --> D[Configuration Propagation]\n    D --> E[Stage 3 Processing]\n    E --> F[BackmatterProcessor]\n    E --> G[Template Processing]\n    F --> H[LLM Calls]\n    G --> I[Font Substitution]\n```\n\n### Component Modifications\n\n#### 1. Book Pipeline UI (10_Book_Pipeline.py)\n- **Default Values**: Update default configuration dictionary\n- **Validation**: Add validation for required fields\n- **UI Display**: Ensure defaults are visible in form fields\n\n#### 2. BackmatterProcessor (backmatter_processor.py)\n- **Configuration Injection**: Accept and use pipeline LLM configuration\n- **Default Handling**: Use sensible defaults when no config provided\n- **Logging**: Log model usage for transparency\n\n#### 3. Prepress Processing (xynapse_traces/prepress.py)\n- **Configuration Propagation**: Pass LLM config to BackmatterProcessor\n- **Template Variables**: Inject font configuration into templates\n- **Error Handling**: Provide clear errors for configuration issues\n\n#### 4. LaTeX Template (xynapse_traces/template.tex)\n- **Font Variables**: Replace hardcoded fonts with template variables\n- **Fallback Handling**: Provide sensible defaults for missing fonts\n\n## Implementation Details\n\n### Default Configuration Values\n\n```python\nDEFAULT_CONFIG = {\n    'lightning_source_account': '6024045',\n    'language_code': 'eng',\n    'field_reports': 'HTML',\n    'llm_config': {\n        'model': 'gpt-4o-mini',\n        'temperature': 0.7,\n        'max_tokens': 2000\n    }\n}\n```\n\n### Font Configuration Variables\n\n```python\nFONT_TEMPLATE_VARS = {\n    'korean_font': 'Apple Myungjo',\n    'body_font': 'Adobe Caslon Pro', \n    'heading_font': 'Adobe Caslon Pro',\n    'quotations_font': 'Adobe Caslon Pro',\n    'mnemonics_font': 'Adobe Caslon Pro'\n}\n```\n\n## Components and Interfaces\n\n### 1. Configuration Manager Interface\n\n```python\nclass ConfigurationManager:\n    def apply_defaults(self, config: Dict) -> Dict\n    def validate_config(self, config: Dict) -> ValidationResult\n    def propagate_config(self, config: Dict) -> Dict\n```\n\n### 2. Template Processor Interface\n\n```python\nclass TemplateProcessor:\n    def inject_font_variables(self, template_vars: Dict, config: Dict) -> Dict\n    def process_template(self, template_path: str, variables: Dict) -> str\n    def validate_template_variables(self, variables: Dict) -> ValidationResult\n```\n\n## Data Models\n\n### Configuration Schema\n\n```python\n@dataclass\nclass PipelineConfig:\n    lightning_source_account: str = \"6024045\"\n    language_code: str = \"eng\"\n    field_reports: str = \"HTML\"\n    llm_config: Dict[str, Any] = field(default_factory=lambda: {\n        'model': 'gpt-4o-mini',\n        'temperature': 0.7,\n        'max_tokens': 2000\n    })\n    fonts: Dict[str, str] = field(default_factory=lambda: {\n        'korean': 'Apple Myungjo',\n        'body': 'Adobe Caslon Pro',\n        'heading': 'Adobe Caslon Pro',\n        'quotations': 'Adobe Caslon Pro',\n        'mnemonics': 'Adobe Caslon Pro'\n    })\n```\n\n## Error Handling\n\n### Configuration Errors\n\n1. **Missing Required Fields**: Provide clear error messages with suggested defaults\n2. **Invalid Values**: Show allowed values and current invalid value\n3. **Font Not Found**: Suggest alternative fonts or fallback to system defaults\n4. **LLM Configuration Issues**: Validate model availability and API keys\n\n## Testing Strategy\n\n### Unit Tests\n\n1. **Default Value Application**: Test that defaults are correctly applied\n2. **Configuration Validation**: Test validation rules and error messages\n3. **Template Variable Injection**: Test font variable substitution\n4. **LLM Configuration Propagation**: Test config passing to processors\n\n### Integration Tests\n\n1. **End-to-End Pipeline**: Test complete pipeline with various configurations\n2. **Font Rendering**: Test that configured fonts appear in generated PDFs\n3. **LLM Model Usage**: Test that correct models are used in processing\n4. **Error Recovery**: Test graceful handling of invalid configurations",
      "size": 4344,
      "modified": "2025-08-05T19:52:07.809950",
      "spec_directory": "pipeline-configuration-fixes",
      "directory_modified": "2025-08-05T20:20:01.076231"
    },
    {
      "file": "design.md",
      "content": "# Frontmatter and Backmatter Generation - Design Document\n\n## Overview\n\nThis design implements a robust system for frontmatter and backmatter generation with strict configuration hierarchy enforcement and regression prevention measures.\n\n## Architecture\n\n### Configuration Hierarchy System\n\n```mermaid\ngraph TD\n    A[Default Config] --> B[Publisher Config]\n    B --> C[Imprint Config] \n    C --> D[Tranche Config]\n    D --> E[Final Values]\n    F[Schedule.json] --> E\n    E --> G[Book Generation]\n```\n",
      "full_content": "# Frontmatter and Backmatter Generation - Design Document\n\n## Overview\n\nThis design implements a robust system for frontmatter and backmatter generation with strict configuration hierarchy enforcement and regression prevention measures.\n\n## Architecture\n\n### Configuration Hierarchy System\n\n```mermaid\ngraph TD\n    A[Default Config] --> B[Publisher Config]\n    B --> C[Imprint Config] \n    C --> D[Tranche Config]\n    D --> E[Final Values]\n    F[Schedule.json] --> E\n    E --> G[Book Generation]\n```\n\n### Section Classification\n\n**Frontmatter (Generated via Reprompting):**\n- Publisher's Note (`storefront_get_en_motivation`)\n- Foreword (new reprompt key needed)\n- Glossary (generated from pilsa_glossary.json)\n\n**Backmatter (Generated via BackmatterProcessor):**\n- Mnemonics (enhanced processing + fallback)\n- Bibliography (LOCKED - memoir class hangparas)\n\n## Components and Interfaces\n\n### 1. Configuration Hierarchy Enforcer\n\n```python\nclass ConfigurationHierarchyEnforcer:\n    def apply_hierarchy(self, default, publisher, imprint, tranche, schedule=None):\n        \"\"\"Apply strict hierarchy: default < publisher < imprint < tranche\"\"\"\n        \n    def enforce_schedule_overrides(self, config, schedule_data):\n        \"\"\"Schedule.json values always win\"\"\"\n        \n    def enforce_tranche_overrides(self, config, tranche_data):\n        \"\"\"Tranche author/imprint always win\"\"\"\n```\n\n### 2. Frontmatter Generator (Reprompting System)\n\n```python\nclass FrontmatterGenerator:\n    def generate_publishers_note(self, book_data, config):\n        \"\"\"100% LLM generated, no boilerplate\"\"\"\n        \n    def generate_foreword(self, book_data, config):\n        \"\"\"Clean Korean formatting, no markdown\"\"\"\n        \n    def generate_glossary(self, glossary_data, config):\n        \"\"\"Proper formatting, no numbering issues\"\"\"\n```\n\n### 3. Regression Prevention System\n\n```python\nclass RegressionPrevention:\n    def validate_bibliography_format(self, latex_output):\n        \"\"\"Ensure memoir class hangparas format is preserved\"\"\"\n        \n    def validate_configuration_hierarchy(self, final_config):\n        \"\"\"Ensure hierarchy is properly applied\"\"\"\n        \n    def validate_content_presence(self, build_dir):\n        \"\"\"Ensure all sections appear in final document\"\"\"\n```\n\n## Implementation Details\n\n### Configuration Hierarchy Enforcement\n\n```python\ndef apply_configuration_hierarchy(default, publisher, imprint, tranche, schedule=None):\n    \"\"\"\n    Apply strict hierarchy with tranche winning\n    \"\"\"\n    config = {}\n    \n    # Layer 1: Default\n    config.update(default)\n    \n    # Layer 2: Publisher  \n    config.update(publisher)\n    \n    # Layer 3: Imprint\n    config.update(imprint)\n    \n    # Layer 4: Tranche (wins all conflicts)\n    config.update(tranche)\n    \n    # Special overrides\n    if schedule:\n        if 'subtitle' in schedule:\n            config['subtitle'] = schedule['subtitle']  # Always wins\n    \n    if tranche:\n        if 'author' in tranche:\n            config['author'] = tranche['author']  # Always wins\n        if 'imprint' in tranche:\n            config['imprint'] = tranche['imprint']  # Always wins\n    \n    return config\n```\n\n### Foreword Prompt Improvement\n\n```python\nFOREWORD_PROMPT = \"\"\"\nWrite a scholarly foreword (300-400 words) about pilsa, the Korean tradition of mindful transcription.\n\nCRITICAL FORMATTING REQUIREMENTS:\n- Use proper LaTeX commands for Korean terms: \\\\korean{} not *pilsa*\n- No markdown syntax in output (no *italics* or **bold**)\n- Use \\\\textit{pilsa} for English italics\n- Use \\\\korean{} for Korean text\n\nCover:\n- Origins in Korean scholarly practices\n- Role in Buddhist and Confucian education  \n- Decline during modernization\n- Revival in digital age\n- Connection to modern reader experience\n\nReturn clean LaTeX-formatted text with proper Korean character handling.\n\"\"\"\n```\n\n### Glossary Formatting Fix\n\n```python\ndef format_glossary_chapter_header():\n    \"\"\"Fix glossary numbering issue\"\"\"\n    return \"\\\\chapter*{Glossary}\\n\\\\addcontentsline{toc}{chapter}{Glossary}\\n\"\n\ndef format_glossary_entry(korean, english, definition):\n    \"\"\"Proper leading and spacing\"\"\"\n    return f\"\"\"\\\\textbf{{\\\\korean{{{korean}}}}} \\\\textit{{{english}}}\n{definition}\n\n\\\\vspace{{6pt}}\n\"\"\"\n```\n\n### Publisher's Note Cleanup\n\n```python\ndef generate_publishers_note_clean(book_data, config):\n    \"\"\"\n    Generate 100% LLM content with no boilerplate\n    \"\"\"\n    # Remove any boilerplate attachment\n    # Use only LLM-generated content from storefront_get_en_motivation\n    return llm_generated_content  # No additional text\n```\n\n## Data Models\n\n### Configuration Schema\n\n```python\n@dataclass\nclass BookConfiguration:\n    # Hierarchy levels\n    default_config: Dict[str, Any]\n    publisher_config: Dict[str, Any] \n    imprint_config: Dict[str, Any]\n    tranche_config: Dict[str, Any]\n    schedule_overrides: Dict[str, Any]\n    \n    # Final resolved values\n    final_config: Dict[str, Any]\n    \n    # Validation\n    hierarchy_applied: bool\n    isbn_on_copyright: bool\n    logo_font_correct: bool\n```\n\n### Section Classification\n\n```python\n@dataclass\nclass DocumentSection:\n    name: str\n    section_type: str  # 'frontmatter' or 'backmatter'\n    generation_method: str  # 'reprompting' or 'processor'\n    prompt_key: Optional[str]\n    required: bool\n    locked: bool  # For regression prevention\n```\n\n## Error Handling\n\n### Configuration Conflicts\n- Log hierarchy application\n- Validate final values match expected hierarchy\n- Alert on unexpected overrides\n\n### Content Generation Failures\n- Fallback mechanisms for each section\n- Clear error messages with context\n- Graceful degradation\n\n### Regression Detection\n- Validate bibliography format hasn't changed\n- Check all required sections are present\n- Verify formatting quality\n\n## Testing Strategy\n\n### Configuration Hierarchy Tests\n- Test each hierarchy level override\n- Validate schedule.json subtitle override\n- Verify tranche author/imprint override\n- Check ISBN appears on copyright page\n\n### Content Generation Tests  \n- Test all frontmatter sections appear\n- Test all backmatter sections appear\n- Validate formatting quality\n- Check for markdown leakage\n\n### Regression Prevention Tests\n- Lock bibliography format validation\n- Test configuration hierarchy enforcement\n- Validate content presence in final document\n\n## Regression Prevention Strategy\n\n### 1. Component Locking\n- Bibliography formatting: LOCKED (memoir class)\n- Configuration hierarchy: ENFORCED\n- Section presence: VALIDATED\n\n### 2. Validation Gates\n- Pre-generation: validate configuration\n- Post-generation: validate content presence\n- Pre-compilation: validate formatting\n\n### 3. Stability Measures\n- Immutable components marked clearly\n- Change impact analysis required\n- Regression test suite mandatory\n\n## Migration Strategy\n\n### Phase 1: Lock Stable Components\n- Mark bibliography as DO NOT CHANGE\n- Implement configuration hierarchy enforcement\n- Add validation gates\n\n### Phase 2: Fix Regressions\n- Fix glossary numbering and overprinting\n- Clean up publisher's note boilerplate\n- Improve foreword Korean formatting\n- Ensure mnemonics appear\n\n### Phase 3: Prevent Future Regressions\n- Implement comprehensive validation\n- Add automated regression tests\n- Document locked components clearly",
      "size": 7252,
      "modified": "2025-08-05T23:08:50.579324",
      "spec_directory": "frontmatter-backmatter-fixes",
      "directory_modified": "2025-08-06T00:48:02.840139"
    }
  ],
  "requirements": [
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThe LSI Field Enhancement Phase 2 builds upon the existing LSI ACS Generator implementation to address specific issues identified during acceptance testing. While Phase 1 successfully implemented the core field mapping system, validation framework, and configuration management, Phase 2 focuses on improving the integration of LLM-generated content, enhancing field completion visibility, and ensuring proper storage of completion data.\n\n## Requirements\n\n###",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThe LSI Field Enhancement Phase 2 builds upon the existing LSI ACS Generator implementation to address specific issues identified during acceptance testing. While Phase 1 successfully implemented the core field mapping system, validation framework, and configuration management, Phase 2 focuses on improving the integration of LLM-generated content, enhancing field completion visibility, and ensuring proper storage of completion data.\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a publisher using the codexes system, I want LLM-generated field completions to be properly stored in a consistent directory structure, so that I can easily access and review them alongside other book artifacts.\n\n#### Acceptance Criteria\n\n1. WHEN saving LLM completions THEN the system SHALL store them in a metadata/ directory parallel to covers/ and interiors/ directories\n2. WHEN determining the output directory THEN the system SHALL first look for existing book directories by publisher_reference_id or ISBN\n3. WHEN saving completions THEN the system SHALL create the directory structure if it doesn't exist\n4. WHEN saving completions THEN the system SHALL use consistent file naming with timestamps and ISBN\n\n### Requirement 2\n\n**User Story:** As a publisher using the codexes system, I want LLM-generated field completions to be properly included in the final LSI CSV output, so that I don't need to manually add this information.\n\n#### Acceptance Criteria\n\n1. WHEN mapping fields to CSV THEN the system SHALL check for existing completions in the metadata.llm_completions dictionary\n2. WHEN a field has a corresponding LLM completion THEN the system SHALL use that value in the CSV output\n3. WHEN multiple LLM completions exist for a field THEN the system SHALL use a consistent priority order to select the appropriate value\n4. WHEN using LLM completions THEN the system SHALL log the source of each field value for traceability\n\n### Requirement 3\n\n**User Story:** As a quality assurance user, I want detailed field completion reports, so that I can understand which fields are being completed by which strategy and identify any issues.\n\n#### Acceptance Criteria\n\n1. WHEN generating LSI CSV files THEN the system SHALL generate detailed field completion reports\n2. WHEN creating field completion reports THEN the system SHALL include field name, strategy type, actual value, and source\n3. WHEN creating field completion reports THEN the system SHALL support multiple output formats (CSV, HTML, JSON)\n4. WHEN creating HTML reports THEN the system SHALL include statistics on field population rates and strategy usage\n5. WHEN creating reports THEN the system SHALL highlight empty fields and potential issues\n\n### Requirement 4\n\n**User Story:** As a system administrator, I want the field completion reporting to be integrated with the book pipeline, so that reports are automatically generated during batch processing.\n\n#### Acceptance Criteria\n\n1. WHEN running the book pipeline THEN the system SHALL generate field completion reports for each book\n2. WHEN generating reports THEN the system SHALL save them alongside the LSI CSV files\n3. WHEN generating reports THEN the system SHALL maintain backward compatibility with existing reporting\n4. WHEN generating reports THEN the system SHALL handle errors gracefully and continue processing",
      "size": 3382,
      "modified": "2025-07-18T21:02:00.255621",
      "spec_directory": "lsi-field-enhancement-phase2",
      "directory_modified": "2025-07-18T21:02:00.255804"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nPhase 3 of the LSI Field Enhancement project builds upon previous phases to improve the management of ISBNs, series metadata, and field completion for Lightning Source Inc. (LSI) distribution. This phase focuses on creating a database for ISBN management, implementing series metadata functionality, and enhancing field completion for various LSI metadata fields.\n\n## Requirements\n\n### Requirement 1: ISBN Database Management\n\n**User Story:** As a publisher,",
      "full_content": "# Requirements Document\n\n## Introduction\n\nPhase 3 of the LSI Field Enhancement project builds upon previous phases to improve the management of ISBNs, series metadata, and field completion for Lightning Source Inc. (LSI) distribution. This phase focuses on creating a database for ISBN management, implementing series metadata functionality, and enhancing field completion for various LSI metadata fields.\n\n## Requirements\n\n### Requirement 1: ISBN Database Management\n\n**User Story:** As a publisher, I want to manage my ISBN inventory efficiently, so that I can automatically assign ISBNs to new books and track their usage status.\n\n#### Acceptance Criteria\n\n1. WHEN a publisher uploads a Bowker spreadsheet THEN the system SHALL import all ISBNs owned by the publisher.\n2. WHEN the system imports ISBNs THEN it SHALL identify which ISBNs are available (never publicly assigned).\n3. WHEN a book requires an ISBN for LSI distribution THEN the system SHALL automatically pull the next available ISBN from the database.\n4. WHEN an ISBN is assigned to a book THEN the system SHALL mark it as \"privately assigned\" in the database.\n5. WHEN an ISBN is uploaded to LSI THEN the system SHALL mark it as \"publicly assigned\" in the database.\n6. WHEN an ISBN is marked as \"publicly assigned\" THEN the system SHALL prevent it from being used for any other purpose.\n7. WHEN a publisher requests to release a privately assigned ISBN THEN the system SHALL mark it as available again IF it has not yet been published.\n\n### Requirement 2: LSI Series Metadata Management\n\n**User Story:** As a publisher, I want to manage book series metadata for LSI distribution, so that I can maintain consistent series information and automatically assign series numbers.\n\n#### Acceptance Criteria\n\n1. WHEN a publisher creates a series THEN the system SHALL store the series name and generate a unique series ID.\n2. WHEN a book is added to a series THEN the system SHALL assign it a sequence number within that series.\n3. WHEN a user starts the Book Pipeline with a series name THEN the system SHALL automatically assign the appropriate series number.\n4. WHEN a publisher has multiple series THEN the system SHALL maintain separate tracking for each series.\n5. WHEN different publishers have series with the same name THEN the system SHALL treat them as separate series with different IDs.\n6. WHEN a series is designated as multi-publisher THEN the system SHALL allow multiple publishers to contribute to it.\n7. WHEN a series is not explicitly designated as multi-publisher THEN the system SHALL default it to single-publisher.\n8. WHEN a user performs CRUD operations on series titles THEN the system SHALL maintain the integrity of the series without renumbering.\n\n### Requirement 3: Enhanced Field Completion\n\n**User Story:** As a publisher, I want improved field completion for LSI metadata, so that I can ensure accurate and consistent information across all required fields.\n\n#### Acceptance Criteria\n\n1. WHEN a user selects a series in the UI THEN the system SHALL allow selection from registered series or creation of a new one.\n2. WHEN a title is assigned to a series THEN the system SHALL assign it a series ID number.\n3. WHEN generating Annotation/Summary fields THEN the system SHALL:\n   - Format content in simple HTML (max 4000 characters, no outbound links)\n   - Include a dramatic hook in bold italic as the first paragraph\n   - Include back_cover_text\n   - Include paragraphed strings from the publisher's shared dictionary\n4. WHEN generating BISAC Category fields THEN the system SHALL:\n   - Use llm_completion to suggest_bisac_codes\n   - Apply any user-specified BISAC category overrides for specific job types\n5. WHEN generating Thema Subject fields THEN the system SHALL assign based on results of suggest_thema_codes.\n6. WHEN generating Contributor info fields THEN the system SHALL assign based on results of extract_lsi_contributor_info.\n7. WHEN generating Illustrations and Illustration Note fields THEN the system SHALL assign based on results of gemini_get_basic_info.\n8. WHEN generating Table of Contents THEN the system SHALL assign based on results of create_simple_toc.\n9. WHEN generating LSI CSV THEN the system SHALL leave specified fields blank (Reserved*, SIBI*, Stamped*, LSI Flexfield*, Review Quotes).",
      "size": 4306,
      "modified": "2025-07-18T21:02:00.256487",
      "spec_directory": "lsi-field-enhancement-phase3",
      "directory_modified": "2025-07-18T21:02:00.256615"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThe LSI ACS Generator currently maps a subset of available LSI (Lightning Source Inc.) fields from CodexMetadata objects to CSV format. The LSI template contains over 100 fields including account information, submission methods, territorial rights, multiple pricing tiers, contributor details, and specialized distribution options. This enhancement will expand the generator to support comprehensive field mapping, provide configuration options for different",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThe LSI ACS Generator currently maps a subset of available LSI (Lightning Source Inc.) fields from CodexMetadata objects to CSV format. The LSI template contains over 100 fields including account information, submission methods, territorial rights, multiple pricing tiers, contributor details, and specialized distribution options. This enhancement will expand the generator to support comprehensive field mapping, provide configuration options for different publishing scenarios, and ensure data validation for LSI submission requirements.\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a publisher using the codexes system, I want the LSI ACS generator to support all available LSI fields, so that I can create complete submission files without manual editing.\n\n#### Acceptance Criteria\n\n1. WHEN the LSI ACS generator processes metadata THEN it SHALL map all 100+ LSI template fields to appropriate values\n2. WHEN a field has no corresponding metadata value THEN the system SHALL use appropriate default values or leave empty based on LSI requirements\n3. WHEN generating the CSV THEN the system SHALL maintain the exact field order and formatting required by LSI\n4. WHEN processing contributor information THEN the system SHALL support multiple contributors with roles, bios, affiliations, and locations\n\n### Requirement 2\n\n**User Story:** As a publisher with different distribution strategies, I want configurable LSI field mappings, so that I can customize output for different imprints, territories, and pricing models.\n\n#### Acceptance Criteria\n\n1. WHEN configuring the generator THEN the system SHALL allow custom field mapping overrides\n2. WHEN setting up different imprints THEN the system SHALL support imprint-specific default values\n3. WHEN handling territorial rights THEN the system SHALL support region-specific pricing and discount configurations\n4. WHEN processing series information THEN the system SHALL automatically populate series-related fields from metadata\n\n### Requirement 3\n\n**User Story:** As a system administrator, I want comprehensive validation of LSI field data, so that generated files meet LSI submission requirements and reduce rejection rates.\n\n#### Acceptance Criteria\n\n1. WHEN validating ISBN data THEN the system SHALL verify format and check-digit validity\n2. WHEN processing pricing information THEN the system SHALL validate currency formats and discount percentages\n3. WHEN handling dates THEN the system SHALL ensure proper date formatting for publication and street dates\n4. WHEN validating BISAC codes THEN the system SHALL verify against current BISAC standards\n5. WHEN checking physical specifications THEN the system SHALL validate trim sizes and page counts\n6. WHEN validating digital files THEN interior and cover files SHALL be available in ftp2lsi staging area with names matching ISBN_interior.pdf and ISBN_cover.pdf\n7. WHEN processing PDF files THEN interior and cover files SHALL pass verification as PDF X-1a format\n8. WHEN validating interior specifications THEN interior PDF SHALL match page count and trim sizes provided in LSI spreadsheet\n\n### Requirement 4\n\n**User Story:** As a content manager, I want enhanced metadata model support, so that all LSI-specific information can be stored and retrieved from the system.\n\n#### Acceptance Criteria\n\n1. WHEN extending the metadata model THEN the system SHALL add fields for all unmapped LSI requirements\n2. WHEN storing contributor information THEN the system SHALL support detailed contributor profiles with bios and affiliations\n3. WHEN handling territorial data THEN the system SHALL store region-specific pricing and availability information\n4. WHEN managing submission preferences THEN the system SHALL store cover and text submission method preferences\n\n### Requirement 5\n\n**User Story:** As a quality assurance user, I want detailed logging and error reporting, so that I can troubleshoot LSI submission issues and track field mapping accuracy.\n\n#### Acceptance Criteria\n\n1. WHEN generating LSI files THEN the system SHALL log all field mappings and transformations\n2. WHEN encountering validation errors THEN the system SHALL provide specific field-level error messages\n3. WHEN processing fails THEN the system SHALL generate detailed error reports with suggested corrections\n4. WHEN successful generation occurs THEN the system SHALL provide summary reports of populated vs empty fields\n\n### Requirement 6\n\n**User Story:** As the Nimble Books (host) website manager, I want to add this new functionality to the existing UI, codexes-factory-home-ui, as an optional checkbox or section under 10. Book Pipeline.\n\n#### Acceptance Criteria\n\n1. WHEN viewing the Book Pipeline page THEN LSI enhanced metadata SHALL be available as an option on the main form\n2. WHEN the LSI enhanced metadata option is selected THEN a valid, complete LSI ACS spreadsheet row SHALL be delivered for each book built",
      "size": 4957,
      "modified": "2025-07-16T11:34:39.338639",
      "spec_directory": "lsi-field-enhancement",
      "directory_modified": "2025-07-18T21:02:00.257098"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThis document outlines the requirements for Phase 4 of the LSI Field Enhancement project. The primary goal is to improve the field population rate in LSI CSV outputs, addressing the current issue where a significant percentage of fields remain empty. This phase will focus on enhancing field completion mechanisms, improving default values, and implementing better field mapping strategies.\n\n## Requirements\n\n1.  As a publisher rushing books to market, I nee",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThis document outlines the requirements for Phase 4 of the LSI Field Enhancement project. The primary goal is to improve the field population rate in LSI CSV outputs, addressing the current issue where a significant percentage of fields remain empty. This phase will focus on enhancing field completion mechanisms, improving default values, and implementing better field mapping strategies.\n\n## Requirements\n\n1.  As a publisher rushing books to market, I need 100% field population with valid fields, including null valids.\n\n2.  As a programmer, I need high transparency: what is happening at each step.\n\n3.  As a systems operator, I need high filterability: show me only warnings, errors, and major decisions.\n\n4.  Specific known issues: \n\nI only need to view the field report in one format (HTML). No need to create markdown and csv unless requested.\n- save llm completions to imprint/metadata/llm_completions BEFORE filtering via field mapping strategies.\n- only 2/12 llm completions are showing up in metadata at present. All should be.\n\n5. The top IMMEDiATE priority is that running the pipeline against rows 1-12 of xynapse_traces_schedule.json must create valid fully populated complete LSI csv for those titles.",
      "size": 1261,
      "modified": "2025-07-19T17:23:37.888292",
      "spec_directory": "lsi-field-enhancement-phase4",
      "directory_modified": "2025-07-19T17:23:37.888292"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThe Xynapse Tranche 1 project addresses 15 specific field completion and validation issues to ship a complete batch of 12 xynapse traces books with fully optimized LSI metadata. Each requirement corresponds to a specific item in the punch list that must be resolved for successful LSI distribution.\n\n## Requirements\n\n### Requirement 1: Account and Configuration Fields\n\n**User Story:** As a publisher, I want correct Lightning Source account information and ",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThe Xynapse Tranche 1 project addresses 15 specific field completion and validation issues to ship a complete batch of 12 xynapse traces books with fully optimized LSI metadata. Each requirement corresponds to a specific item in the punch list that must be resolved for successful LSI distribution.\n\n## Requirements\n\n### Requirement 1: Account and Configuration Fields\n\n**User Story:** As a publisher, I want correct Lightning Source account information and proper field exclusions, so that LSI files are properly configured for our account.\n\n#### Acceptance Criteria\n\n1. WHEN generating LSI files THEN the system SHALL set Lightning Source Account # to \"6024045\"\n2. WHEN creating CSV output THEN the system SHALL exclude \"Metadata Contact Dictionary\" field from CSV output\n3. WHEN processing Parent ISBN field THEN the system SHALL leave it empty for this tranche\n4. WHEN validating account fields THEN the system SHALL ensure Lightning Source Account # is always populated\n\n### Requirement 2: ISBN Management and Database\n\n**User Story:** As a production manager, I want proper ISBN assignment from a validated database, so that each book has a unique, valid ISBN.\n\n#### Acceptance Criteria\n\n1. WHEN initializing the system THEN the system SHALL load ISBN database with real data\n2. WHEN processing each book THEN the system SHALL assign a unique ISBN from the valid database\n3. WHEN assigning ISBNs THEN the system SHALL mark assigned ISBNs as unavailable\n4. WHEN ISBN assignment fails THEN the system SHALL provide clear error messages\n\n### Requirement 3: Tranche Configuration Management\n\n**User Story:** As a configuration manager, I want tranche-level configuration options, so that consistent settings can be applied to all books in a batch.\n\n#### Acceptance Criteria\n\n1. WHEN processing a tranche THEN the system SHALL apply tranche-level configuration to all books\n2. WHEN tranche config is set THEN the system SHALL override individual book settings with tranche settings\n3. WHEN validating configuration THEN the system SHALL ensure tranche settings are consistent across all books\n4. WHEN configuration conflicts occur THEN the system SHALL prioritize tranche-level settings\n\n### Requirement 4: Field Validation Against Valid Lists\n\n**User Story:** As a quality assurance specialist, I want validation against LSI valid value lists, so that all field values are accepted by LSI systems.\n\n#### Acceptance Criteria\n\n1. WHEN validating rendition booktype THEN the system SHALL check against lsi_valid_rendition_booktypes.txt\n2. WHEN validating Contributor Role One THEN the system SHALL check against lsi_valid_contributor_codes.csv\n3. WHEN validation fails THEN the system SHALL provide specific error messages with valid options\n4. WHEN valid values are updated THEN the system SHALL use the most current validation lists\n\n### Requirement 5: Smart Publication Date Assignment\n\n**User Story:** As a scheduling coordinator, I want intelligent publication date assignment, so that books are distributed evenly across available publication dates.\n\n#### Acceptance Criteria\n\n1. WHEN processing publication dates THEN the system SHALL extract month/year from schedule.json\n2. WHEN assigning dates THEN the system SHALL spread all titles across Tuesdays in the target month\n3. WHEN multiple books exist THEN the system SHALL distribute them evenly across available Tuesdays\n4. WHEN date assignment completes THEN the system SHALL ensure no date conflicts exist\n\n### Requirement 6: Enhanced Content Generation\n\n**User Story:** As a content manager, I want enhanced annotation/summary generation, so that book descriptions are comprehensive and consistent.\n\n#### Acceptance Criteria\n\n1. WHEN generating annotations THEN the system SHALL combine LLM completion results with boilerplate strings\n2. WHEN using boilerplate THEN the system SHALL source strings from configuration dictionary\n3. WHEN joining content THEN the system SHALL ensure proper formatting and flow\n4. WHEN content exceeds limits THEN the system SHALL truncate appropriately while maintaining readability\n\n### Requirement 7: BISAC Subject Handling\n\n**User Story:** As a metadata specialist, I want proper BISAC subject formatting and override capability, so that categorization is accurate and consistent.\n\n#### Acceptance Criteria\n\n1. WHEN setting BISAC Subject THEN the system SHALL use category name only (no code)\n2. WHEN setting BISAC Subject 2 and 3 THEN the system SHALL follow the same format as BISAC Subject\n3. WHEN tranche config specifies required BISAC Subject THEN the system SHALL override LLM completion\n4. WHEN BISAC validation occurs THEN the system SHALL ensure all subjects are valid category names\n\n### Requirement 8: Content Length Validation\n\n**User Story:** As a format compliance manager, I want proper content length validation, so that all text fields meet LSI requirements.\n\n#### Acceptance Criteria\n\n1. WHEN validating short description THEN the system SHALL ensure it is no more than 350 bytes\n2. WHEN content exceeds limits THEN the system SHALL truncate while preserving meaning\n3. WHEN truncation occurs THEN the system SHALL log the original and truncated content\n4. WHEN validation completes THEN the system SHALL report any length violations\n\n### Requirement 9: Thema Subject Correction\n\n**User Story:** As a subject classification specialist, I want proper Thema subject handling, so that subject codes are complete and accurate.\n\n#### Acceptance Criteria\n\n1. WHEN processing Thema subjects THEN the system SHALL use full multi-letter strings instead of single letters\n2. WHEN validating Thema codes THEN the system SHALL ensure codes are complete and valid\n3. WHEN Thema subjects are truncated THEN the system SHALL restore full codes\n4. WHEN multiple Thema subjects exist THEN the system SHALL handle all with consistent formatting\n\n### Requirement 10: GC Market Pricing\n\n**User Story:** As a pricing manager, I want consistent GC market pricing, so that country-specific markets have appropriate pricing.\n\n#### Acceptance Criteria\n\n1. WHEN setting GC market prices THEN the system SHALL use the same price as US List Price\n2. WHEN processing multiple GC markets THEN the system SHALL apply consistent pricing across all\n3. WHEN validating GC pricing THEN the system SHALL ensure all GC markets have identical pricing\n4. WHEN pricing updates occur THEN the system SHALL maintain consistency across GC markets",
      "size": 6459,
      "modified": "2025-07-19T15:13:36.866721",
      "spec_directory": "xynapse-tranche-1",
      "directory_modified": "2025-07-20T15:23:30.266950"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThis specification addresses a bug in the prepress.py file where the code incorrectly counts `\\textbf` LaTeX commands. The current implementation counts any `\\textbf` command that appears after a newline, but the requirement is to count only `\\textbf` commands that appear at the very beginning of a line (with no preceding whitespace or other characters).\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a book processing system, I want to accuratel",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThis specification addresses a bug in the prepress.py file where the code incorrectly counts `\\textbf` LaTeX commands. The current implementation counts any `\\textbf` command that appears after a newline, but the requirement is to count only `\\textbf` commands that appear at the very beginning of a line (with no preceding whitespace or other characters).\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a book processing system, I want to accurately count mnemonic entries that start with bold text, so that I can generate the correct number of practice pages.\n\n#### Acceptance Criteria\n\n1. WHEN processing mnemonics LaTeX content THEN the system SHALL count only `\\textbf` commands that appear at the beginning of a line\n2. WHEN a `\\textbf` command is preceded by whitespace or other characters on the same line THEN the system SHALL NOT count it as a line-beginning bold command\n3. WHEN a `\\textbf` command appears immediately after a newline character with no intervening characters THEN the system SHALL count it as a line-beginning bold command\n4. WHEN the mnemonics count is determined THEN the system SHALL use this count to generate the appropriate number of practice pages\n\n### Requirement 2\n\n**User Story:** As a developer, I want the regex pattern to be precise and maintainable, so that future modifications are clear and the logic is easy to understand.\n\n#### Acceptance Criteria\n\n1. WHEN implementing the fix THEN the system SHALL use a regex pattern that specifically matches `\\textbf` at line beginnings\n2. WHEN the code is reviewed THEN the regex pattern SHALL be documented with clear comments explaining its purpose\n3. WHEN testing the implementation THEN the system SHALL correctly handle edge cases like empty lines and lines with only whitespace",
      "size": 1819,
      "modified": "2025-07-20T18:36:10.752771",
      "spec_directory": "textbf-line-detection-fix",
      "directory_modified": "2025-07-20T19:51:45.228220"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThis feature enhances the book layout system to support a specialized mnemonic practice format where each mnemonic is placed on a verso (left-hand) page followed by a recto (right-hand) dot grid page for practice exercises. The dot grid pages will include numbered practice labels at the bottom to help readers track their progress through the mnemonic exercises.\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a book publisher, I want to generate b",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThis feature enhances the book layout system to support a specialized mnemonic practice format where each mnemonic is placed on a verso (left-hand) page followed by a recto (right-hand) dot grid page for practice exercises. The dot grid pages will include numbered practice labels at the bottom to help readers track their progress through the mnemonic exercises.\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a book publisher, I want to generate books with alternating mnemonic and practice pages, so that readers can study mnemonics and then practice them on dedicated dot grid pages.\n\n#### Acceptance Criteria\n\n1. WHEN a book contains mnemonic content THEN the system SHALL place each mnemonic on a verso (left-hand) page\n2. WHEN a mnemonic is placed on a verso page THEN the system SHALL generate a corresponding recto (right-hand) dot grid page immediately following it\n3. WHEN generating dot grid practice pages THEN the system SHALL include \"Mnemonic Practice {i}\" text at the bottom of each page where {i} is the sequential practice number\n\n### Requirement 2\n\n**User Story:** As a reader, I want clearly numbered practice pages, so that I can easily track my progress through the mnemonic exercises.\n\n#### Acceptance Criteria\n\n1. WHEN dot grid practice pages are generated THEN each page SHALL display \"Mnemonic Practice {i}\" at the bottom\n2. WHEN numbering practice pages THEN the system SHALL increment {i} sequentially starting from 1\n3. WHEN placing the practice label THEN the system SHALL position it in the instruction location at the bottom of the page\n\n### Requirement 3\n\n**User Story:** As a book designer, I want the mnemonic practice layout to integrate with existing LaTeX templates, so that the feature works seamlessly with current book generation workflows.  \n\n#### Acceptance Criteria\n\n1. WHEN generating mnemonic practice layouts THEN the system SHALL use existing dot grid generation capabilities\n2. WHEN applying the layout THEN the system SHALL maintain compatibility with current LaTeX template structure\n3. WHEN processing book content THEN the system SHALL detect mnemonic content and automatically apply the alternating layout\n4.  WHEN generating the final layout THEN the system SHALL place the Mnemonics section in the back matter after the Bibliography and before any other appendixes.\n\n### Requirement 4\n\n**User Story:** As a content creator, I want the system to handle variable numbers of mnemonics, so that books with different amounts of mnemonic content are properly formatted.\n\n#### Acceptance Criteria\n\n1. WHEN a book contains multiple mnemonics THEN the system SHALL create the appropriate number of verso/recto page pairs\n2. WHEN processing mnemonic content THEN the system SHALL maintain proper page sequencing regardless of the total number of mnemonics\n3. WHEN generating the final layout THEN the system SHALL ensure proper page alignment for printing requirements",
      "size": 2966,
      "modified": "2025-07-22T02:31:54.566794",
      "spec_directory": "mnemonic-practice-layout",
      "directory_modified": "2025-07-22T02:31:54.566794"
    },
    {
      "file": "requirements.md",
      "content": "# Cover Fixes Requirements Document\n\n## Introduction\n\nThis specification addresses two critical issues with the cover generation system that are preventing proper display of cover content.\n\n## Requirements\n\n### Requirement 1: Back Cover Text Variable Substitution\n\n**User Story:** As a publisher, I want the back cover text to properly substitute template variables like {stream}, {title}, {description}, and {quotes_per_book} with actual book data, so that the back cover displays meaningful content",
      "full_content": "# Cover Fixes Requirements Document\n\n## Introduction\n\nThis specification addresses two critical issues with the cover generation system that are preventing proper display of cover content.\n\n## Requirements\n\n### Requirement 1: Back Cover Text Variable Substitution\n\n**User Story:** As a publisher, I want the back cover text to properly substitute template variables like {stream}, {title}, {description}, and {quotes_per_book} with actual book data, so that the back cover displays meaningful content instead of placeholder text.\n\n#### Acceptance Criteria\n\n1. WHEN the cover generation system processes back_cover_text THEN it SHALL substitute {stream} with the book's subject/topic\n2. WHEN the cover generation system processes back_cover_text THEN it SHALL substitute {title} with the actual book title\n3. WHEN the cover generation system processes back_cover_text THEN it SHALL substitute {description} with the book description\n4. WHEN the cover generation system processes back_cover_text THEN it SHALL substitute {quotes_per_book} with the actual number of quotes\n5. WHEN all variables are substituted THEN the back cover SHALL display complete, meaningful text without any remaining placeholder variables\n\n### Requirement 2: Front Cover Korean Text Formatting\n\n**User Story:** As a publisher, I want the Korean characters () on the front cover to display properly without visible LaTeX commands, so that the cover looks professional and the Korean text renders correctly.\n\n#### Acceptance Criteria\n\n1. WHEN the cover generation system processes Korean text THEN it SHALL NOT display visible \\korean{} LaTeX commands on the rendered cover\n2. WHEN Korean text is processed THEN it SHALL use the proper Korean font (AppleMyungjo) for rendering\n3. WHEN the cover is generated THEN the Korean characters SHALL appear as clean, properly formatted text\n4. WHEN the LaTeX is compiled THEN the \\korean{} command SHALL be processed internally without appearing in the final output\n5. WHEN the cover template is populated THEN Korean text variables SHALL be properly escaped for LaTeX processing",
      "size": 2093,
      "modified": "2025-07-28T00:26:45.967314",
      "spec_directory": "cover-fixes",
      "directory_modified": "2025-07-28T00:27:39.502680"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThe current back cover text processing system uses basic string substitution to replace variables like `{stream}`, `{title}`, `{author}`, etc. in the back cover text template. This approach is fragile and doesn't handle complex variable resolution or produce natural, flowing text. The system needs to be enhanced to use LLM-based processing that takes the template text with variables and returns clean, final text ready for placement on the book cover.\n\n##",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThe current back cover text processing system uses basic string substitution to replace variables like `{stream}`, `{title}`, `{author}`, etc. in the back cover text template. This approach is fragile and doesn't handle complex variable resolution or produce natural, flowing text. The system needs to be enhanced to use LLM-based processing that takes the template text with variables and returns clean, final text ready for placement on the book cover.\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a book publisher, I want the back cover text to be processed through an LLM so that all variable substitutions are resolved intelligently and the final text flows naturally.\n\n#### Acceptance Criteria\n\n1. WHEN the cover generation process encounters back cover text with variables THEN the system SHALL send the template text to an LLM for processing\n2. WHEN the LLM processes the back cover text THEN it SHALL resolve all variable substitutions using the book metadata\n3. WHEN the LLM returns the processed text THEN it SHALL be clean, final text with no remaining variables or placeholders\n4. WHEN the processed text is returned THEN it SHALL be properly formatted for LaTeX placement on the cover\n\n### Requirement 2\n\n**User Story:** As a developer, I want the LLM back cover text processing to be integrated seamlessly into the existing cover generation pipeline without breaking current functionality.\n\n#### Acceptance Criteria\n\n1. WHEN the cover generator processes back cover text THEN it SHALL use the new LLM-based processor instead of basic string substitution\n2. WHEN the LLM processing fails THEN the system SHALL fall back to the current string substitution method\n3. WHEN the cover generation completes THEN all existing cover generation features SHALL continue to work as before\n4. WHEN the system processes back cover text THEN it SHALL log the LLM processing steps for debugging\n\n### Requirement 3\n\n**User Story:** As a content creator, I want the LLM to have access to all relevant book metadata so that it can create contextually appropriate back cover text.\n\n#### Acceptance Criteria\n\n1. WHEN the LLM processes back cover text THEN it SHALL have access to title, author, imprint, subject/stream, description, quotes_per_book, and other relevant metadata\n2. WHEN the LLM encounters a variable reference THEN it SHALL substitute it with the appropriate metadata value\n3. WHEN metadata is missing for a variable THEN the LLM SHALL use intelligent fallbacks or omit the reference gracefully\n4. WHEN the LLM generates text THEN it SHALL maintain the intended tone and style appropriate for the book's subject matter\n\n### Requirement 4\n\n**User Story:** As a system administrator, I want the LLM back cover text processing to be configurable and monitorable for cost and performance management.\n\n#### Acceptance Criteria\n\n1. WHEN the system processes back cover text THEN it SHALL use the configured LLM model and settings\n2. WHEN LLM processing occurs THEN it SHALL be logged with request/response details for monitoring\n3. WHEN the system encounters LLM errors THEN it SHALL retry with exponential backoff according to configuration\n4. WHEN processing completes THEN it SHALL track token usage and costs for the back cover text generation\n\n### Requirement 5\n\n**User Story:** As a book designer, I want the back cover narrative to be less than 100 words. All back text must fit in a safe area in top part of page.",
      "size": 3478,
      "modified": "2025-07-28T07:17:57.229868",
      "spec_directory": "llm-back-cover-text-processing",
      "directory_modified": "2025-07-28T07:17:57.229868"
    },
    {
      "file": "requirements.md",
      "content": "# LSI CSV Generator Project - Requirements\n\n## Introduction\n\nThis project focuses on creating a robust, standalone LSI (Lightning Source Inc.) CSV generation system that can transform book metadata into complete, validated LSI-compliant CSV files for print-on-demand distribution.\n\n## Requirements\n\n### Requirement 1: Core LSI CSV Generation\n\n**User Story:** As a publisher, I want to generate complete LSI CSV files from book metadata, so that I can submit books to IngramSpark's automated content s",
      "full_content": "# LSI CSV Generator Project - Requirements\n\n## Introduction\n\nThis project focuses on creating a robust, standalone LSI (Lightning Source Inc.) CSV generation system that can transform book metadata into complete, validated LSI-compliant CSV files for print-on-demand distribution.\n\n## Requirements\n\n### Requirement 1: Core LSI CSV Generation\n\n**User Story:** As a publisher, I want to generate complete LSI CSV files from book metadata, so that I can submit books to IngramSpark's automated content submission system.\n\n#### Acceptance Criteria\n\n1. WHEN provided with book metadata THEN the system SHALL generate a CSV file with all ~119 LSI-required fields\n2. WHEN generating CSV files THEN the system SHALL validate all field formats according to LSI specifications\n3. WHEN a required field is missing THEN the system SHALL provide intelligent fallbacks or flag the issue\n4. WHEN generating CSV files THEN the system SHALL ensure all text fields are properly encoded and escaped\n5. WHEN processing metadata THEN the system SHALL handle multiple book formats (paperback, hardcover, ebook)\n\n### Requirement 2: AI-Powered Field Completion\n\n**User Story:** As a publisher, I want the system to automatically fill missing metadata fields using AI, so that I can generate complete LSI submissions with minimal manual input.\n\n#### Acceptance Criteria\n\n1. WHEN metadata has missing fields THEN the system SHALL use LLM to generate appropriate content\n2. WHEN using AI completion THEN the system SHALL provide fallback values if AI generation fails\n3. WHEN generating AI content THEN the system SHALL ensure all generated text meets LSI field requirements\n4. WHEN completing fields THEN the system SHALL log all AI-generated content for review\n5. WHEN AI completion is disabled THEN the system SHALL still generate valid CSV files with available data\n\n### Requirement 3: Validation and Quality Assurance\n\n**User Story:** As a publisher, I want comprehensive validation of generated CSV files, so that I can be confident my submissions will be accepted by LSI.\n\n#### Acceptance Criteria\n\n1. WHEN generating CSV files THEN the system SHALL validate all field lengths against LSI limits\n2. WHEN validating content THEN the system SHALL check for required field completeness\n3. WHEN processing text THEN the system SHALL validate character encoding and special characters\n4. WHEN generating files THEN the system SHALL verify CSV format compliance\n5. WHEN validation fails THEN the system SHALL provide detailed error reports with correction suggestions\n\n### Requirement 4: Batch Processing\n\n**User Story:** As a publisher, I want to process multiple books simultaneously, so that I can efficiently generate LSI files for entire catalogs.\n\n#### Acceptance Criteria\n\n1. WHEN provided with multiple metadata files THEN the system SHALL process them in batch\n2. WHEN batch processing THEN the system SHALL generate individual CSV files for each book\n3. WHEN batch processing THEN the system SHALL create a combined catalog CSV file\n4. WHEN processing batches THEN the system SHALL provide progress reporting and error handling\n5. WHEN batch processing fails THEN the system SHALL continue with remaining files and report failures\n\n### Requirement 5: Configuration Management\n\n**User Story:** As a publisher, I want flexible configuration options, so that I can customize the LSI generation for different imprints and publishing workflows.\n\n#### Acceptance Criteria\n\n1. WHEN configuring the system THEN the system SHALL support publisher-specific settings\n2. WHEN using configurations THEN the system SHALL support imprint-specific customizations\n3. WHEN processing books THEN the system SHALL apply tranche-specific configurations\n4. WHEN configuring fields THEN the system SHALL allow field mapping and transformation rules\n5. WHEN updating configurations THEN the system SHALL validate configuration files before use\n\n### Requirement 6: Reporting and Analytics\n\n**User Story:** As a publisher, I want detailed reports on LSI generation results, so that I can monitor quality and identify areas for improvement.\n\n#### Acceptance Criteria\n\n1. WHEN generating CSV files THEN the system SHALL create completion reports showing field population rates\n2. WHEN processing multiple books THEN the system SHALL generate summary analytics\n3. WHEN AI completion is used THEN the system SHALL report on AI-generated content quality\n4. WHEN validation occurs THEN the system SHALL provide detailed validation reports\n5. WHEN generating reports THEN the system SHALL support multiple output formats (HTML, CSV, JSON)\n\n### Requirement 7: Integration and Extensibility\n\n**User Story:** As a developer, I want a modular system architecture, so that I can easily extend and integrate the LSI generator with other publishing tools.\n\n#### Acceptance Criteria\n\n1. WHEN designing the system THEN the system SHALL use modular, pluggable architecture\n2. WHEN extending functionality THEN the system SHALL support custom field processors\n3. WHEN integrating THEN the system SHALL provide clear API interfaces\n4. WHEN adding features THEN the system SHALL maintain backward compatibility\n5. WHEN deploying THEN the system SHALL support both standalone and integrated usage\n\n### Requirement 8: Performance and Scalability\n\n**User Story:** As a publisher, I want fast and reliable LSI generation, so that I can process large catalogs efficiently.\n\n#### Acceptance Criteria\n\n1. WHEN processing single books THEN the system SHALL complete generation within 30 seconds\n2. WHEN batch processing THEN the system SHALL handle 100+ books without memory issues\n3. WHEN using AI completion THEN the system SHALL implement efficient caching and rate limiting\n4. WHEN processing large datasets THEN the system SHALL provide progress indicators\n5. WHEN system resources are limited THEN the system SHALL gracefully handle resource constraints",
      "size": 5889,
      "modified": "2025-07-28T14:15:27.592743",
      "spec_directory": "lsi-csv-generator-project",
      "directory_modified": "2025-07-28T14:16:23.003675"
    },
    {
      "file": "requirements.md",
      "content": "# LSI CSV Bug Fixes Project - Requirements\n\n## Introduction\n\nThis project focuses on identifying and fixing bugs in the existing LSI CSV generation system within the Codexes Factory codebase. The goal is to improve reliability, accuracy, and completeness of LSI CSV file generation for IngramSpark submissions.\n\n## Requirements\n\n### Requirement 1: Fix JSON Parsing Errors in LLM Responses\n\n**User Story:** As a developer, I want LLM responses to consistently return valid JSON, so that the LSI field ",
      "full_content": "# LSI CSV Bug Fixes Project - Requirements\n\n## Introduction\n\nThis project focuses on identifying and fixing bugs in the existing LSI CSV generation system within the Codexes Factory codebase. The goal is to improve reliability, accuracy, and completeness of LSI CSV file generation for IngramSpark submissions.\n\n## Requirements\n\n### Requirement 1: Fix JSON Parsing Errors in LLM Responses\n\n**User Story:** As a developer, I want LLM responses to consistently return valid JSON, so that the LSI field completion process doesn't fail with parsing errors.\n\n#### Acceptance Criteria\n\n1. WHEN LLM prompts are executed THEN the system SHALL receive valid JSON responses\n2. WHEN JSON parsing fails THEN the system SHALL provide meaningful error messages\n3. WHEN prompts return conversational text THEN the system SHALL handle gracefully with fallbacks\n4. WHEN using modernized prompts THEN the system SHALL enforce JSON-only responses\n5. WHEN LLM responses are malformed THEN the system SHALL log the issue and continue processing\n\n### Requirement 2: Improve Field Completion Accuracy\n\n**User Story:** As a publisher, I want LSI fields to be populated with accurate, relevant content, so that my book submissions are accepted by IngramSpark.\n\n#### Acceptance Criteria\n\n1. WHEN generating BISAC codes THEN the system SHALL provide valid, current BISAC classifications\n2. WHEN creating descriptions THEN the system SHALL respect character limits and formatting requirements\n3. WHEN determining age ranges THEN the system SHALL provide appropriate values based on content\n4. WHEN generating keywords THEN the system SHALL create relevant, searchable terms\n5. WHEN completing contributor information THEN the system SHALL generate professional, accurate bios\n\n### Requirement 3: Fix Validation Logic Issues\n\n**User Story:** As a developer, I want the validation system to correctly identify and report LSI compliance issues, so that generated CSV files meet IngramSpark requirements.\n\n#### Acceptance Criteria\n\n1. WHEN validating field lengths THEN the system SHALL correctly enforce LSI character limits\n2. WHEN checking required fields THEN the system SHALL accurately identify missing mandatory data\n3. WHEN validating formats THEN the system SHALL properly check dates, ISBNs, and other structured fields\n4. WHEN validation fails THEN the system SHALL provide specific, actionable error messages\n5. WHEN validation passes THEN the system SHALL guarantee LSI compliance\n\n### Requirement 4: Resolve Configuration Loading Problems\n\n**User Story:** As a developer, I want configuration files to load correctly and apply settings properly, so that publisher and imprint-specific customizations work as expected.\n\n#### Acceptance Criteria\n\n1. WHEN loading multi-level configs THEN the system SHALL properly inherit and override settings\n2. WHEN configuration files are missing THEN the system SHALL use appropriate defaults\n3. WHEN config syntax is invalid THEN the system SHALL provide clear error messages\n4. WHEN applying tranche settings THEN the system SHALL correctly merge configurations\n5. WHEN config changes are made THEN the system SHALL reload without restart\n\n### Requirement 5: Fix Batch Processing Failures\n\n**User Story:** As a publisher, I want batch processing to handle errors gracefully and complete successfully, so that I can process multiple books without manual intervention.\n\n#### Acceptance Criteria\n\n1. WHEN one book fails in a batch THEN the system SHALL continue processing remaining books\n2. WHEN batch processing encounters errors THEN the system SHALL log detailed error information\n3. WHEN memory issues occur THEN the system SHALL handle large batches efficiently\n4. WHEN processing is interrupted THEN the system SHALL provide resume capability\n5. WHEN batch completes THEN the system SHALL generate comprehensive success/failure reports\n\n### Requirement 6: Improve Error Handling and Logging\n\n**User Story:** As a developer, I want comprehensive error handling and logging, so that I can quickly diagnose and fix issues in the LSI generation process.\n\n#### Acceptance Criteria\n\n1. WHEN errors occur THEN the system SHALL log detailed context and stack traces\n2. WHEN processing books THEN the system SHALL provide progress indicators and status updates\n3. WHEN LLM calls fail THEN the system SHALL implement proper retry logic with backoff\n4. WHEN validation fails THEN the system SHALL log specific field issues and suggested fixes\n5. WHEN debugging is needed THEN the system SHALL provide verbose logging options\n\n### Requirement 7: Fix Field Mapping and Transformation Issues\n\n**User Story:** As a publisher, I want book metadata to be correctly mapped to LSI fields, so that all relevant information appears in the proper CSV columns.\n\n#### Acceptance Criteria\n\n1. WHEN mapping metadata fields THEN the system SHALL correctly transform data types and formats\n2. WHEN handling missing source data THEN the system SHALL apply intelligent defaults or AI completion\n3. WHEN processing complex fields THEN the system SHALL properly parse and format structured data\n4. WHEN applying field rules THEN the system SHALL correctly implement publisher-specific mappings\n5. WHEN transforming content THEN the system SHALL preserve data integrity and meaning\n\n### Requirement 8: Resolve Performance and Memory Issues\n\n**User Story:** As a developer, I want the LSI generation system to run efficiently without memory leaks or performance degradation, so that it can handle production workloads.\n\n#### Acceptance Criteria\n\n1. WHEN processing large books THEN the system SHALL complete within reasonable time limits\n2. WHEN running batch jobs THEN the system SHALL maintain stable memory usage\n3. WHEN making LLM calls THEN the system SHALL implement efficient caching and rate limiting\n4. WHEN handling large datasets THEN the system SHALL use streaming and pagination appropriately\n5. WHEN system resources are constrained THEN the system SHALL gracefully handle limitations",
      "size": 5988,
      "modified": "2025-07-28T14:16:55.461347",
      "spec_directory": "lsi-csv-bug-fixes",
      "directory_modified": "2025-07-28T22:26:09.548904"
    },
    {
      "file": "requirements.md",
      "content": "# LSI Pricing System Fix Requirements\n\n## Introduction\n\nThe LSI CSV generation system has critical pricing regressions that must be fixed to ensure proper Lightning Source compatibility. The current system generates incorrect price formats and missing calculated prices.\n\n## Requirements\n\n### Requirement 1: Price Format Standardization\n\n**User Story:** As a Lightning Source user, I want all prices to be decimal numbers without currency symbols, so that the CSV can be properly processed by LSI sys",
      "full_content": "# LSI Pricing System Fix Requirements\n\n## Introduction\n\nThe LSI CSV generation system has critical pricing regressions that must be fixed to ensure proper Lightning Source compatibility. The current system generates incorrect price formats and missing calculated prices.\n\n## Requirements\n\n### Requirement 1: Price Format Standardization\n\n**User Story:** As a Lightning Source user, I want all prices to be decimal numbers without currency symbols, so that the CSV can be properly processed by LSI systems.\n\n#### Acceptance Criteria\n\n1. WHEN generating any price field THEN the system SHALL output decimal numbers with exactly 2 decimal places\n2. WHEN generating any price field THEN the system SHALL NOT include currency symbols ($, , , etc.)\n3. WHEN generating US Suggested List Price THEN the output SHALL be format \"19.95\" not \"$19.95\"\n4. WHEN generating UK Suggested List Price THEN the output SHALL be format \"19.99\" not \"19.99\"\n5. WHEN generating any territorial price THEN the output SHALL be numeric only\n\n### Requirement 2: Calculated Territorial Pricing\n\n**User Story:** As a publisher, I want EU, CA, and AU prices to be automatically calculated from the US base price, so that I have consistent multi-territorial pricing.\n\n#### Acceptance Criteria\n\n1. WHEN US Suggested List Price is provided THEN the system SHALL calculate EU Suggested List Price using current exchange rates\n2. WHEN US Suggested List Price is provided THEN the system SHALL calculate CA Suggested List Price using current exchange rates\n3. WHEN US Suggested List Price is provided THEN the system SHALL calculate AU Suggested List Price using current exchange rates\n4. WHEN calculating territorial prices THEN the system SHALL use cached exchange rates with fallback defaults\n5. WHEN territorial prices are calculated THEN they SHALL be formatted as decimal numbers without currency symbols\n\n### Requirement 3: Market Price Consistency\n\n**User Story:** As a Lightning Source user, I want GC and other specialty market prices to equal the US list price when not specifically set, so that pricing is consistent across all markets.\n\n#### Acceptance Criteria\n\n1. WHEN GC Suggested List Price is not specifically configured THEN it SHALL equal the US Suggested List Price\n2. WHEN USBR1 Suggested List Price is not configured THEN it SHALL equal the US Suggested List Price\n3. WHEN USDE1 Suggested List Price is not configured THEN it SHALL equal the US Suggested List Price\n4. WHEN any specialty market price is not configured THEN it SHALL default to the US Suggested List Price\n5. WHEN specialty market prices are set THEN they SHALL be formatted as decimal numbers without currency symbols\n\n### Requirement 4: Wholesale Discount Consistency\n\n**User Story:** As a publisher, I want wholesale discounts to be consistently applied across all markets, so that my distribution terms are uniform.\n\n#### Acceptance Criteria\n\n1. WHEN a wholesale discount is configured for US market THEN it SHALL be applied to all other markets unless specifically overridden\n2. WHEN no wholesale discount is specified for a market THEN it SHALL use the default wholesale discount percentage\n3. WHEN wholesale discounts are output THEN they SHALL be integer percentages without the % symbol\n4. WHEN wholesale discount is 40% THEN the output SHALL be \"40\" not \"40%\"\n5. WHEN wholesale discounts are calculated THEN they SHALL be consistent across all territorial markets\n\n### Requirement 5: Price Field Mapping Optimization\n\n**User Story:** As a system administrator, I want the field mapping system to be optimized with no duplicate or redundant strategies, so that the system is efficient and maintainable.\n\n#### Acceptance Criteria\n\n1. WHEN the system initializes THEN it SHALL register exactly one mapping strategy per LSI field\n2. WHEN field mapping strategies are registered THEN there SHALL be no duplicate registrations\n3. WHEN the system reports mapping strategies THEN the count SHALL not exceed the number of LSI template fields\n4. WHEN pricing strategies are registered THEN they SHALL be consolidated into a single pricing strategy per field\n5. WHEN the system processes pricing fields THEN it SHALL use optimized, non-redundant mapping logic",
      "size": 4212,
      "modified": "2025-07-29T00:17:17.656574",
      "spec_directory": "lsi-pricing-fixes",
      "directory_modified": "2025-07-29T00:53:34.598584"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThe BISAC category fields in the LSI CSV generation system are not working correctly. Currently, only one BISAC category field is being populated, and it contains a code instead of the full category name. The system needs to be fixed to properly populate all three BISAC category fields with full category names (not codes) using LLM assistance and validation against the current BISAC standards.\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a pub",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThe BISAC category fields in the LSI CSV generation system are not working correctly. Currently, only one BISAC category field is being populated, and it contains a code instead of the full category name. The system needs to be fixed to properly populate all three BISAC category fields with full category names (not codes) using LLM assistance and validation against the current BISAC standards.\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a publisher, I want all three BISAC category fields to be populated with appropriate categories, so that my books are properly classified for distribution.\n\n#### Acceptance Criteria\n\n1. WHEN the LSI CSV is generated THEN all three BISAC Category fields (BISAC Category, BISAC Category 2, BISAC Category 3) SHALL be populated\n2. WHEN a BISAC category is assigned THEN it SHALL contain the full category name without the code (e.g., \"BUSINESS & ECONOMICS / General\" not \"BUS000000\")\n3. WHEN multiple BISAC categories are needed THEN they SHALL be distinct and relevant to the book content\n4. IF fewer than three relevant categories exist THEN only the relevant categories SHALL be populated, leaving others empty\n\n### Requirement 2\n\n**User Story:** As a publisher, I want BISAC categories to be validated against current standards, so that my books meet distribution requirements.\n\n#### Acceptance Criteria\n\n1. WHEN BISAC categories are generated THEN they SHALL be validated against the current BISAC standards database\n2. WHEN an invalid BISAC category is detected THEN the system SHALL use a fallback strategy with valid alternatives\n3. WHEN BISAC categories are assigned THEN they SHALL use the most recent BISAC standards (2024)\n4. IF a generated category is not found in the standards THEN the system SHALL log a warning and use the closest valid match\n\n### Requirement 3\n\n**User Story:** As a publisher, I want BISAC categories to be intelligently generated based on book content, so that categorization is accurate and relevant.\n\n#### Acceptance Criteria\n\n1. WHEN generating BISAC categories THEN the system SHALL use LLM assistance to analyze book metadata\n2. WHEN the LLM generates categories THEN they SHALL be ranked by relevance to the book content\n3. WHEN book metadata contains existing BISAC information THEN it SHALL be used as the primary category\n4. IF the primary category is invalid THEN the system SHALL generate alternatives using LLM assistance\n\n### Requirement 4\n\n**User Story:** As a publisher, I want tranche configuration to override BISAC categories for specific book series, so that all books in a series (like PILSA) can be consistently categorized.\n\n#### Acceptance Criteria\n\n1. WHEN a tranche config specifies a BISAC category override THEN it SHALL be used as the primary category\n2. WHEN tranche override is applied THEN the remaining categories SHALL be generated to complement the override\n3. WHEN tranche override is \"Self-Help / Journaling\" THEN all PILSA books SHALL use this as their primary category\n4. IF tranche override is invalid THEN the system SHALL log a warning and use the override anyway if it's a valid category name\n\n### Requirement 5\n\n**User Story:** As a publisher, I want BISAC categories to come from different top-level categories when possible, so that books have broader discoverability.\n\n#### Acceptance Criteria\n\n1. WHEN generating multiple BISAC categories THEN at least 2 categories SHOULD come from different top-level categories (e.g., BUS vs SEL vs COM)\n2. WHEN selecting categories THEN the system SHALL prioritize diversity across top-level categories\n3. WHEN only one top-level category is relevant THEN the system SHALL use the most specific subcategories within that top-level\n4. IF diverse top-level categories cannot be found THEN the system SHALL log this and use the most relevant categories available\n\n### Requirement 6\n\n**User Story:** As a developer, I want the BISAC category system to have proper error handling and logging, so that issues can be diagnosed and resolved.\n\n#### Acceptance Criteria\n\n1. WHEN BISAC category generation fails THEN the system SHALL log detailed error information\n2. WHEN fallback strategies are used THEN the system SHALL log the reason and chosen alternatives\n3. WHEN validation fails THEN the system SHALL provide specific error messages with suggested corrections\n4. IF all generation attempts fail THEN the system SHALL use safe fallback categories and log the failure",
      "size": 4475,
      "modified": "2025-07-29T02:01:55.204578",
      "spec_directory": "bisac-category-fixes",
      "directory_modified": "2025-07-29T02:25:21.321841"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThis specification addresses six specific field mapping corrections needed in the LSI CSV generation system. These corrections focus on proper tranche value precedence, data extraction from JSON metadata, series-aware descriptions, pricing field management, and file path handling to ensure accurate and compliant LSI CSV output.\n\n## Requirements\n\n### Requirement 1: Tranche Value Override Priority\n\n**User Story:** As a publisher, I want tranche configurati",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThis specification addresses six specific field mapping corrections needed in the LSI CSV generation system. These corrections focus on proper tranche value precedence, data extraction from JSON metadata, series-aware descriptions, pricing field management, and file path handling to ensure accurate and compliant LSI CSV output.\n\n## Requirements\n\n### Requirement 1: Tranche Value Override Priority\n\n**User Story:** As a publisher, I want tranche configuration values to override LLM-generated values (except for specific append cases), so that my publishing-specific settings take precedence over AI-generated content.\n\n#### Acceptance Criteria\n\n1. WHEN a field has both tranche configuration and LLM-generated values THEN the system SHALL use the tranche value as the final output\n2. WHEN the field is \"annotation_boilerplate\" or similar append-type fields THEN the system SHALL append tranche values to LLM-generated content\n3. WHEN a tranche value is explicitly set to empty/null THEN the system SHALL respect that override and not fall back to LLM values\n4. WHEN no tranche value exists THEN the system SHALL use LLM-generated values as fallback\n\n### Requirement 2: Thema Subject Extraction and Mapping\n\n**User Story:** As a metadata manager, I want Thema subject codes extracted from JSON metadata and properly mapped to LSI columns, so that subject classification is accurate and complete.\n\n#### Acceptance Criteria\n\n1. WHEN JSON metadata contains \"thema\" field with array values THEN the system SHALL extract the first three values\n2. WHEN thema values are extracted THEN the system SHALL map them to \"Thema Subject 1\", \"Thema Subject 2\", and \"Thema Subject 3\" columns respectively\n3. WHEN fewer than 3 thema values exist THEN the system SHALL leave remaining Thema Subject columns empty\n4. WHEN no thema values exist in JSON THEN the system SHALL leave all Thema Subject columns empty\n5. WHEN thema values are malformed or invalid THEN the system SHALL log warnings and skip invalid entries\n\n### Requirement 3: Age Range Extraction and Formatting\n\n**User Story:** As a content categorizer, I want minimum and maximum age values extracted from JSON metadata and formatted as integers, so that age targeting is properly specified in the LSI submission.\n\n#### Acceptance Criteria\n\n1. WHEN JSON metadata contains \"min_age\" field THEN the system SHALL extract and convert it to integer format for \"Min Age\" column\n2. WHEN JSON metadata contains \"max_age\" field THEN the system SHALL extract and convert it to integer format for \"Max Age\" column\n3. WHEN age values are non-numeric THEN the system SHALL log warnings and leave age columns empty\n4. WHEN age values are negative or unrealistic (>150) THEN the system SHALL validate and reject invalid values\n5. WHEN no age values exist in JSON THEN the system SHALL leave age columns empty\n\n### Requirement 4: Series-Aware Short Description\n\n**User Story:** As a series publisher, I want short descriptions to reference the series name when applicable, so that readers understand the book's context within the series.\n\n#### Acceptance Criteria\n\n1. WHEN a book has a series name AND short description contains \"This book\" THEN the system SHALL replace it with \"This book in the {series_name} series\"\n2. WHEN a book has no series name THEN the system SHALL leave \"This book\" unchanged\n3. WHEN short description doesn't contain \"This book\" THEN the system SHALL leave the description unchanged\n4. WHEN series name is empty or null THEN the system SHALL treat it as no series\n\n### Requirement 5: Ingram Pricing Field Management\n\n**User Story:** As an LSI administrator, I want specific Ingram pricing fields to remain blank, so that the CSV complies with current LSI requirements and avoids submission errors.\n\n#### Acceptance Criteria\n\n1. WHEN generating LSI CSV THEN the system SHALL ensure \"US-Ingram-Only* Suggested List Price (mode 2)\" is blank\n2. WHEN generating LSI CSV THEN the system SHALL ensure \"US-Ingram-Only* Wholesale Discount % (Mode 2)\" is blank\n3. WHEN generating LSI CSV THEN the system SHALL ensure \"US - Ingram - GAP * Suggested List Price (mode 2)\" is blank\n4. WHEN generating LSI CSV THEN the system SHALL ensure \"US - Ingram - GAP * Wholesale Discount % (Mode 2)\" is blank\n5. WHEN generating LSI CSV THEN the system SHALL ensure \"SIBI - EDUC - US * Suggested List Price (mode 2)\" is blank\n6. WHEN generating LSI CSV THEN the system SHALL ensure \"SIBI - EDUC - US * Wholesale Discount % (Mode 2)\" is blank\n\n### Requirement 6: File Path Tranche Compliance\n\n**User Story:** As a file manager, I want LSI CSV file paths to honor tranche definitions, so that the correct file locations are specified for each publishing configuration.\n\n#### Acceptance Criteria\n\n1. WHEN tranche configuration specifies file path patterns THEN the system SHALL use those patterns for \"Interior Path / Filename\" and \"Cover Path / Filename\" columns\n2. WHEN tranche defines custom path templates THEN the system SHALL apply book-specific variables (title, ISBN, etc.) to generate actual paths\n3. WHEN no tranche file path is defined THEN the system SHALL use default path generation logic\n4. WHEN file paths are generated THEN the system SHALL validate they follow LSI naming conventions\n5. WHEN file paths contain invalid characters THEN the system SHALL sanitize them according to LSI requirements",
      "size": 5385,
      "modified": "2025-07-31T17:58:25.153296",
      "spec_directory": "lsi-field-mapping-corrections",
      "directory_modified": "2025-07-31T18:33:35.171851"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThe Streamlit UI needs to be enhanced to accept and manage all current parameters and configuration files in the Codexes Factory system. The current UI has basic parameter support but lacks comprehensive integration with the multi-level configuration system (default  publisher  imprint  tranche) and many of the advanced LSI and distribution parameters that have been implemented.\n\n## Requirements\n\n### Requirement 1: Multi-Level Configuration Management",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThe Streamlit UI needs to be enhanced to accept and manage all current parameters and configuration files in the Codexes Factory system. The current UI has basic parameter support but lacks comprehensive integration with the multi-level configuration system (default  publisher  imprint  tranche) and many of the advanced LSI and distribution parameters that have been implemented.\n\n## Requirements\n\n### Requirement 1: Multi-Level Configuration Management\n\n**User Story:** As a publisher, I want to select and manage configurations at different levels (publisher, imprint, tranche), so that I can apply consistent settings across my publishing workflow.\n\n#### Acceptance Criteria\n\n1. WHEN accessing the UI THEN the system SHALL provide dropdowns for selecting publisher, imprint, and tranche configurations\n2. WHEN a configuration level is selected THEN the system SHALL load and display the relevant configuration parameters\n3. WHEN multiple configuration levels are selected THEN the system SHALL show the inheritance hierarchy (default  publisher  imprint  tranche)\n4. WHEN configuration conflicts exist THEN the system SHALL clearly indicate which values take precedence\n\n### Requirement 2: Comprehensive LSI Parameter Support\n\n**User Story:** As a metadata specialist, I want access to all LSI parameters through the UI, so that I can configure complete LSI submissions without manual file editing.\n\n#### Acceptance Criteria\n\n1. WHEN configuring LSI settings THEN the system SHALL provide UI controls for all LSI fields including territorial pricing, physical specifications, and metadata defaults\n2. WHEN LSI parameters are modified THEN the system SHALL validate values against LSI requirements\n3. WHEN territorial pricing is configured THEN the system SHALL support all territorial markets with proper currency and pricing multipliers\n4. WHEN field overrides are specified THEN the system SHALL allow tranche-level field overrides and exclusions\n\n### Requirement 3: Dynamic Configuration Loading\n\n**User Story:** As a production manager, I want the UI to dynamically load available configurations, so that I can work with any publisher, imprint, or tranche without hardcoded limitations.\n\n#### Acceptance Criteria\n\n1. WHEN the UI loads THEN the system SHALL scan configuration directories and populate dropdown options\n2. WHEN new configuration files are added THEN the system SHALL detect them without requiring UI restart\n3. WHEN configuration files are invalid THEN the system SHALL provide clear error messages and fallback options\n4. WHEN configurations are updated THEN the system SHALL refresh the UI to reflect changes\n\n### Requirement 4: Advanced Parameter Organization\n\n**User Story:** As a user, I want parameters organized in logical groups with expandable sections, so that I can efficiently navigate and configure complex settings.\n\n#### Acceptance Criteria\n\n1. WHEN viewing parameters THEN the system SHALL organize them into logical groups (Core Settings, LSI Configuration, Territorial Pricing, etc.)\n2. WHEN parameter groups are displayed THEN the system SHALL use expandable sections to manage screen space\n3. WHEN parameters have dependencies THEN the system SHALL show/hide related fields dynamically\n4. WHEN parameters have help text THEN the system SHALL provide tooltips or help sections\n\n### Requirement 5: Configuration Validation and Preview\n\n**User Story:** As a quality assurance specialist, I want to validate configurations before execution, so that I can catch errors early in the process.\n\n#### Acceptance Criteria\n\n1. WHEN configurations are modified THEN the system SHALL validate parameters in real-time\n2. WHEN validation errors occur THEN the system SHALL highlight problematic fields with specific error messages\n3. WHEN configurations are complete THEN the system SHALL provide a preview of the final merged configuration\n4. WHEN submitting the pipeline THEN the system SHALL perform final validation before execution\n\n### Requirement 6: Enhanced File and Template Management\n\n**User Story:** As a template manager, I want to upload and manage configuration files through the UI, so that I can maintain configurations without direct file system access.\n\n#### Acceptance Criteria\n\n1. WHEN managing configurations THEN the system SHALL allow uploading new publisher, imprint, and tranche configuration files\n2. WHEN configuration files are uploaded THEN the system SHALL validate JSON structure and required fields\n3. WHEN templates are needed THEN the system SHALL provide downloadable templates for each configuration type\n4. WHEN configurations are exported THEN the system SHALL allow downloading current configurations as JSON files\n\n### Requirement 7: Pipeline Integration Enhancement\n\n**User Story:** As a pipeline operator, I want seamless integration between UI configuration and pipeline execution, so that all parameters are properly passed to the book generation process.\n\n#### Acceptance Criteria\n\n1. WHEN pipeline is executed THEN the system SHALL pass all UI-configured parameters to the run_book_pipeline.py script\n2. WHEN complex configurations are used THEN the system SHALL handle parameter serialization and command-line argument construction\n3. WHEN pipeline execution begins THEN the system SHALL display the complete configuration being used\n4. WHEN pipeline completes THEN the system SHALL save the configuration used for audit purposes\n\n### Requirement 8: User Experience Improvements\n\n**User Story:** As a user, I want an intuitive and responsive interface, so that I can efficiently configure and execute book production workflows.\n\n#### Acceptance Criteria\n\n1. WHEN using the interface THEN the system SHALL provide responsive design that works on different screen sizes\n2. WHEN configurations are complex THEN the system SHALL provide search and filter capabilities for parameters\n3. WHEN working with multiple books THEN the system SHALL support batch configuration and execution\n4. WHEN errors occur THEN the system SHALL provide clear, actionable error messages with suggested solutions\n\n### Requirement 9: Configuration History and Audit\n\n**User Story:** As an administrator, I want to track configuration changes and pipeline executions, so that I can maintain audit trails and troubleshoot issues.\n\n#### Acceptance Criteria\n\n1. WHEN configurations are modified THEN the system SHALL log changes with timestamps and user information\n2. WHEN pipelines are executed THEN the system SHALL save complete configuration snapshots\n3. WHEN reviewing history THEN the system SHALL provide a configuration history viewer\n4. WHEN troubleshooting THEN the system SHALL allow comparing configurations between different executions\n\n### Requirement 10: Advanced Features Integration\n\n**User Story:** As a power user, I want access to advanced features like LLM configuration, field mapping strategies, and custom validation rules, so that I can fully utilize the system's capabilities.\n\n#### Acceptance Criteria\n\n1. WHEN configuring LLM settings THEN the system SHALL provide UI controls for model selection, retry parameters, and monitoring settings\n2. WHEN field mapping is needed THEN the system SHALL allow configuration of custom field mapping strategies\n3. WHEN validation rules are required THEN the system SHALL support custom validation rule configuration\n4. WHEN debugging is needed THEN the system SHALL provide debug modes and detailed logging options",
      "size": 7476,
      "modified": "2025-08-02T15:19:50.598788",
      "spec_directory": "streamlit-ui-config-enhancement",
      "directory_modified": "2025-08-02T15:42:18.959595"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nFix the Streamlit form error caused by using `st.button()` inside an `st.form()` in the Book Pipeline page. The error occurs because the ConfigurationUI component has a \"Load Config\" button inside the form, which violates Streamlit's form constraints.\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a user, I want the configuration to load automatically when I select publisher/imprint/tranche options, so that I don't need to manually click a load ",
      "full_content": "# Requirements Document\n\n## Introduction\n\nFix the Streamlit form error caused by using `st.button()` inside an `st.form()` in the Book Pipeline page. The error occurs because the ConfigurationUI component has a \"Load Config\" button inside the form, which violates Streamlit's form constraints.\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a user, I want the configuration to load automatically when I select publisher/imprint/tranche options, so that I don't need to manually click a load button.\n\n#### Acceptance Criteria\n\n1. WHEN I select a publisher, imprint, or tranche THEN the system SHALL automatically load and merge the configurations\n2. WHEN configurations are loaded automatically THEN the system SHALL update the session state with the merged configuration\n3. WHEN configuration loading fails THEN the system SHALL display an error message to the user\n4. WHEN no configurations are selected THEN the system SHALL use default values\n\n### Requirement 2\n\n**User Story:** As a user, I want to be able to change my configuration selections if I realize I picked the wrong ones, so that I can correct my choices without errors.\n\n#### Acceptance Criteria\n\n1. WHEN I change any configuration selection THEN the system SHALL immediately reload the configuration\n2. WHEN I change from a valid configuration to an invalid one THEN the system SHALL show validation warnings\n3. WHEN I change configurations THEN the system SHALL preserve any manual parameter overrides where possible\n4. WHEN configuration changes occur THEN the system SHALL provide visual feedback about the loading process\n\n### Requirement 3\n\n**User Story:** As a developer, I want the form to comply with Streamlit's constraints, so that the application runs without errors.\n\n#### Acceptance Criteria\n\n1. WHEN the form is rendered THEN it SHALL NOT contain any `st.button()` calls\n2. WHEN the form is rendered THEN it SHALL only use `st.form_submit_button()` for form submission\n3. WHEN the configuration selector is used THEN it SHALL work both inside and outside of forms\n4. WHEN the UI components are loaded THEN they SHALL not cause Streamlit API exceptions\n\n### Requirement 4\n\n**User Story:** As a user, I want clear visual feedback about configuration loading status, so that I understand what's happening when I make selections.\n\n#### Acceptance Criteria\n\n1. WHEN configurations are being loaded THEN the system SHALL show a loading indicator\n2. WHEN configuration loading completes successfully THEN the system SHALL show a success message\n3. WHEN configuration inheritance is active THEN the system SHALL display the inheritance chain\n4. WHEN validation occurs THEN the system SHALL show validation status in real-time",
      "size": 2707,
      "modified": "2025-08-02T19:42:22.260631",
      "spec_directory": "streamlit-form-button-fix",
      "directory_modified": "2025-08-02T20:13:10.265817"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThis specification addresses critical UI interaction issues in the Streamlit application that are causing runaway loops and preventing proper dropdown refreshes. The current implementation has two major problems: the imprint dropdown doesn't refresh when the publisher changes, and the validation button triggers infinite page rerun loops.\n\n## Requirements\n\n### Requirement 1: Fix Imprint Dropdown Refresh\n\n**User Story:** As a user, I want the imprint dropd",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThis specification addresses critical UI interaction issues in the Streamlit application that are causing runaway loops and preventing proper dropdown refreshes. The current implementation has two major problems: the imprint dropdown doesn't refresh when the publisher changes, and the validation button triggers infinite page rerun loops.\n\n## Requirements\n\n### Requirement 1: Fix Imprint Dropdown Refresh\n\n**User Story:** As a user, I want the imprint dropdown to automatically refresh and show available imprints when I select a publisher, so that I can properly configure the pipeline without manual page refreshes.\n\n#### Acceptance Criteria\n\n1. WHEN a user selects \"nimble_books\" as publisher THEN the imprint dropdown SHALL immediately show \"xynapse_traces\" as an option\n2. WHEN a user changes from one publisher to another THEN the imprint dropdown SHALL clear and repopulate with the new publisher's imprints\n3. WHEN the publisher selection changes THEN the system SHALL NOT trigger infinite rerun loops\n4. WHEN the imprint dropdown refreshes THEN the tranche dropdown SHALL also refresh to show relevant tranches\n5. WHEN no publisher is selected THEN the imprint dropdown SHALL show only an empty option\n\n### Requirement 2: Fix Validation Button Runaway Loop\n\n**User Story:** As a user, I want to click the \"Validate Only\" button to check my configuration without causing the application to enter an infinite refresh loop, so that I can validate my settings safely.\n\n#### Acceptance Criteria\n\n1. WHEN a user clicks the \"Validate Only\" button THEN the system SHALL validate the configuration exactly once\n2. WHEN validation completes THEN the system SHALL display validation results without triggering additional reruns\n3. WHEN validation results are shown THEN the page SHALL remain stable and responsive\n4. WHEN validation finds errors THEN the system SHALL display them clearly without causing UI instability\n5. WHEN validation is successful THEN the system SHALL show success status without page refresh loops\n\n### Requirement 3: Prevent Rerun Loop Cascades\n\n**User Story:** As a user, I want the UI to respond to my interactions smoothly without causing cascading page refreshes that make the application unusable, so that I can work efficiently with the configuration interface.\n\n#### Acceptance Criteria\n\n1. WHEN any form element changes THEN the system SHALL update only the necessary dependent elements\n2. WHEN session state is updated THEN the system SHALL NOT trigger unnecessary reruns\n3. WHEN dropdown dependencies change THEN the system SHALL use controlled refresh mechanisms\n4. WHEN validation is triggered THEN the system SHALL prevent rerun loops during the validation process\n5. WHEN configuration loading occurs THEN the system SHALL debounce rapid successive changes\n\n### Requirement 4: Improve Session State Management\n\n**User Story:** As a user, I want the application to maintain consistent state across interactions without losing my selections or causing unexpected behavior, so that my workflow is predictable and reliable.\n\n#### Acceptance Criteria\n\n1. WHEN dropdown selections change THEN the system SHALL update session state atomically\n2. WHEN publisher changes THEN the system SHALL preserve valid dependent selections where possible\n3. WHEN configuration loads THEN the system SHALL maintain UI state consistency\n4. WHEN validation occurs THEN the system SHALL preserve form data and user selections\n5. WHEN errors occur THEN the system SHALL maintain stable session state without corruption",
      "size": 3573,
      "modified": "2025-08-02T22:29:45.749195",
      "spec_directory": "streamlit-ui-runaway-fixes",
      "directory_modified": "2025-08-02T22:40:42.397820"
    },
    {
      "file": "requirements.md",
      "content": "# Configuration Synchronization Fix Requirements\n\n## Introduction\n\nThe Book Pipeline interface has a disconnect between the Configuration Selection (outside the form) and the Core Settings (inside the form). Users select a publisher and imprint in the configuration section, but these values don't populate the corresponding fields in the core settings, causing validation errors for required fields that should be automatically filled.\n\n## Requirements\n\n### Requirement 1: Configuration to Core Sett",
      "full_content": "# Configuration Synchronization Fix Requirements\n\n## Introduction\n\nThe Book Pipeline interface has a disconnect between the Configuration Selection (outside the form) and the Core Settings (inside the form). Users select a publisher and imprint in the configuration section, but these values don't populate the corresponding fields in the core settings, causing validation errors for required fields that should be automatically filled.\n\n## Requirements\n\n### Requirement 1: Configuration to Core Settings Synchronization\n\n**User Story:** As a user, I want the publisher and imprint I select in the Configuration Selection to automatically populate the corresponding fields in Core Settings, so that I don't get validation errors for fields that should be pre-filled.\n\n#### Acceptance Criteria\n\n1. WHEN a user selects a publisher in the Configuration Selection THEN the publisher field in Core Settings SHALL be automatically populated with the same value\n2. WHEN a user selects an imprint in the Configuration Selection THEN the imprint field in Core Settings SHALL be automatically populated with the same value\n3. WHEN a user clicks the refresh button in Configuration Selection THEN any updated publisher/imprint values SHALL be synchronized to Core Settings\n4. WHEN the Configuration Selection values are synchronized to Core Settings THEN the validation SHALL pass for these required fields\n5. WHEN a user manually changes the publisher or imprint in Core Settings THEN it SHALL override the Configuration Selection values for that session\n\n### Requirement 2: Default Value Population\n\n**User Story:** As a user, I want the Core Settings to show meaningful default values based on my configuration selection, so that I can see what values will be used without having to guess.\n\n#### Acceptance Criteria\n\n1. WHEN the page loads with no configuration selected THEN the Core Settings publisher and imprint fields SHALL show placeholder text indicating they will be populated from configuration\n2. WHEN a valid publisher and imprint are selected in Configuration Selection THEN the Core Settings SHALL immediately reflect these values\n3. WHEN the configuration selection is cleared or reset THEN the Core Settings SHALL revert to empty/placeholder state\n4. WHEN configuration values are populated THEN they SHALL be visually distinct from user-entered values (e.g., with helper text indicating \"from configuration\")\n\n### Requirement 3: Validation Logic Enhancement\n\n**User Story:** As a user, I want the validation to recognize when required fields are populated from configuration selection, so that I don't see false validation errors.\n\n#### Acceptance Criteria\n\n1. WHEN publisher and imprint are populated from Configuration Selection THEN validation SHALL treat these as valid required field values\n2. WHEN validation runs THEN it SHALL check both the Core Settings form values AND the Configuration Selection values for required fields\n3. WHEN a required field is empty in Core Settings but populated in Configuration Selection THEN validation SHALL pass for that field\n4. WHEN validation displays results THEN it SHALL clearly indicate which values came from configuration vs. user input\n5. WHEN validation fails THEN error messages SHALL provide clear guidance on whether to fix the issue in Configuration Selection or Core Settings\n\n### Requirement 4: User Experience Consistency\n\n**User Story:** As a user, I want a clear understanding of how Configuration Selection and Core Settings work together, so that I can efficiently configure the pipeline without confusion.\n\n#### Acceptance Criteria\n\n1. WHEN I view the interface THEN there SHALL be clear visual indicators showing which Core Settings values are populated from Configuration Selection\n2. WHEN I change a value in Configuration Selection THEN any affected Core Settings SHALL update immediately with visual feedback\n3. WHEN I manually override a Core Settings value THEN there SHALL be a clear indication that this value is now independent of Configuration Selection\n4. WHEN I hover over auto-populated fields THEN tooltips SHALL explain that these values come from Configuration Selection\n5. WHEN validation runs THEN the results SHALL clearly distinguish between configuration-derived and user-entered values\n\n### Requirement 5: State Management\n\n**User Story:** As a developer, I want proper state synchronization between Configuration Selection and Core Settings, so that the interface behaves predictably and maintains data consistency.\n\n#### Acceptance Criteria\n\n1. WHEN Configuration Selection values change THEN the session state SHALL be updated to reflect the new values\n2. WHEN Core Settings form is rendered THEN it SHALL pull default values from the current Configuration Selection state\n3. WHEN a user manually overrides a Core Settings value THEN the override state SHALL be tracked separately from configuration defaults\n4. WHEN the page refreshes or reloads THEN the synchronization between Configuration Selection and Core Settings SHALL be restored\n5. WHEN multiple users access the system THEN each user's configuration state SHALL be maintained independently\n\n## Success Criteria\n\n- Users can select publisher/imprint in Configuration Selection and see these values automatically appear in Core Settings\n- Validation passes when required fields are populated from Configuration Selection\n- Clear visual distinction between configuration-derived and user-entered values\n- No false validation errors for fields that should be auto-populated\n- Smooth, intuitive user experience with immediate feedback on configuration changes\n\n## Technical Considerations\n\n- Configuration Selection is outside the main form, Core Settings are inside the form\n- Session state management needs to bridge the gap between these two sections\n- Validation logic needs to consider both configuration and form values\n- Real-time synchronization without causing form submission or page refresh\n- Backward compatibility with existing configuration workflows",
      "size": 6019,
      "modified": "2025-08-02T23:28:22.855433",
      "spec_directory": "config-sync-fix",
      "directory_modified": "2025-08-02T23:36:25.671807"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a publisher, I want all three BISAC category fields to be populated with appropriate categories, so that my books are properly classified for distribution.\n\n#### Acceptance Criteria\n\n1. WHEN the LSI CSV is generated THEN all three BISAC Category fields (BISAC Category, BISAC Category 2, BISAC Category 3) SHALL be populated",
      "full_content": "# Requirements Document\n\n## Introduction\n\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a publisher, I want all three BISAC category fields to be populated with appropriate categories, so that my books are properly classified for distribution.\n\n#### Acceptance Criteria\n\n1. WHEN the LSI CSV is generated THEN all three BISAC Category fields (BISAC Category, BISAC Category 2, BISAC Category 3) SHALL be populated",
      "size": 422,
      "modified": "2025-08-03T23:01:59.678353",
      "spec_directory": "imprint_builder",
      "directory_modified": "2025-08-03T23:01:59.678441"
    },
    {
      "file": "requirements.md",
      "content": "# Book Production Fixes and Enhancements Requirements\n\n## Introduction\n\nThis specification addresses critical production issues and enhancements needed for the book generation pipeline, covering typography, formatting, error handling, reporting accuracy, and new features for professional book production. The system currently has several issues affecting the quality and reliability of generated books, including bibliography formatting problems, inefficient ISBN lookups, inaccurate reporting, and ",
      "full_content": "# Book Production Fixes and Enhancements Requirements\n\n## Introduction\n\nThis specification addresses critical production issues and enhancements needed for the book generation pipeline, covering typography, formatting, error handling, reporting accuracy, and new features for professional book production. The system currently has several issues affecting the quality and reliability of generated books, including bibliography formatting problems, inefficient ISBN lookups, inaccurate reporting, and inconsistent typography that fails to meet professional publishing standards.\n\n## Requirements\n\n### Requirement 1: Bibliography Formatting Enhancement\n\n**User Story:** As a publisher and editor, I want bibliographies to use proper hanging indent formatting with memoir citation fields, so that the bibliography appears professionally formatted according to publishing standards.\n\n#### Acceptance Criteria\n\n1. WHEN a bibliography is generated THEN it SHALL use memoir citation field formatting\n2. WHEN bibliography entries span multiple lines THEN the second and subsequent lines SHALL have a 0.15 hanging indent\n3. WHEN bibliography entries are processed THEN they SHALL maintain consistent formatting across all entries\n4. WHEN the LaTeX template is applied THEN bibliography formatting SHALL be automatically applied without manual intervention\n5. WHEN multiple bibliography entries exist THEN each SHALL be properly separated and formatted independently\n\n### Requirement 2: ISBN Lookup and Caching System\n\n**User Story:** As a system administrator, I want ISBN lookups to be cached in processed JSON files and avoid duplicate scans, so that the system is efficient and doesn't repeat expensive API calls.\n\n#### Acceptance Criteria\n\n1. WHEN an ISBN is encountered in a document THEN the system SHALL check if it has already been processed in the cached JSON\n2. WHEN an ISBN lookup is performed THEN the results SHALL be stored in the processed JSON file for future reference\n3. WHEN a document is scanned for ISBNs THEN the system SHALL not repeat the scan if it has already been completed\n4. WHEN cached ISBN data exists THEN it SHALL be used instead of making new API calls\n5. WHEN ISBN lookup fails THEN the system SHALL log the failure and continue processing without blocking the pipeline\n\n### Requirement 3: Accurate Prompt and Quote Reporting\n\n**User Story:** As a content manager, I want accurate reporting of prompt success and quote retrieval statistics, so that I can monitor the effectiveness of the content generation process.\n\n#### Acceptance Criteria\n\n1. WHEN 90 quotes are successfully retrieved THEN the report SHALL accurately reflect this count, not show 0\n2. WHEN prompts execute successfully THEN the success rate SHALL be accurately calculated and reported\n3. WHEN quote retrieval completes THEN the system SHALL provide detailed statistics on success/failure rates\n4. WHEN reporting is generated THEN it SHALL align with actual test results and pipeline execution\n5. WHEN multiple prompts are processed THEN each prompt's individual success rate SHALL be tracked and reported\n\n### Requirement 4: Error Handling and Logging Improvements\n\n**User Story:** As a developer, I want comprehensive error handling and logging for quote verification and field completion failures, so that I can diagnose and fix issues quickly.\n\n#### Acceptance Criteria\n\n1. WHEN quote verification fails THEN the system SHALL log detailed error information including the response received\n2. WHEN the verifier model returns invalid data THEN the system SHALL handle the error gracefully and continue processing\n3. WHEN field completion fails due to missing methods THEN the system SHALL provide clear error messages and fallback behavior\n4. WHEN validation reports 0 fields checked THEN the system SHALL investigate and report why no fields were processed\n5. WHEN runtime errors occur THEN they SHALL be logged with sufficient context for debugging and resolution\n\n### Requirement 5: Book Pipeline UI Must Display and Honor Available Tranche Configs\n\n**User Story:** As a web admin and system operator, I need the tranche config to be available on the dropdowns in the Book Pipeline, so that users can select appropriate tranche configurations for book processing.\n\n#### Acceptance Criteria\n\n1. WHEN the Book Pipeline UI loads THEN the tranche dropdown SHALL display all available tranche configurations\n2. WHEN tranche configurations are updated THEN the dropdown SHALL refresh to show current options\n3. WHEN a user selects a tranche THEN the system SHALL load the corresponding configuration\n4. WHEN no tranche configurations exist THEN the system SHALL display an appropriate message\n5. WHEN tranche selection is made THEN it SHALL be properly passed to the book processing pipeline\n\n### Requirement 6: Typography and Formatting Enhancements\n\n**User Story:** As a book designer, I want consistent typography and formatting across all book elements, so that the final product meets professional publishing standards.\n\n#### Acceptance Criteria\n\n1. WHEN mnemonics pages are generated THEN they SHALL use the same format as quotations with Adobe Caslon font\n2. WHEN acronym mnemonics are displayed THEN each letter of the expanded acronym SHALL have its own bullet point\n3. WHEN Korean characters appear on title pages THEN they SHALL use Apple Myungjo font\n4. WHEN instructions are placed THEN they SHALL appear on every 8th recto page at the bottom\n5. WHEN chapter headings are formatted THEN the leading underneath SHALL be reduced to approximately 36 points\n6. WHEN LaTeX commands are used THEN they SHALL be properly escaped so that they are not visible. For example, the string \"\\textit\" should never be visible in the compiled PDF.\n\n### Requirement 7: Glossary Layout and Formatting\n\n**User Story:** As a content formatter, I want glossaries to be properly formatted in 2 columns within the page text area, so that Korean and English terms are clearly presented.\n\n#### Acceptance Criteria\n\n1. WHEN a glossary is generated THEN it SHALL be formatted in exactly 2 columns\n2. WHEN glossary content is laid out THEN it SHALL fit within the defined page text area\n3. WHEN Korean and English terms are displayed THEN they SHALL be stacked on top of each other in the left-hand cells\n4. WHEN glossary formatting is applied THEN it SHALL maintain consistent spacing and alignment\n5. WHEN multiple glossary entries exist THEN they SHALL be distributed evenly across both columns\n\n### Requirement 8: Publisher's Note Enhancement\n\n**User Story:** As an editor, I want the Publisher's Note to be concise, engaging, and properly formatted, so that it effectively motivates both publishers and readers.\n\n#### Acceptance Criteria\n\n1. WHEN the Publisher's Note is generated THEN it SHALL consist of exactly 3 medium-length paragraphs\n2. WHEN each paragraph is created THEN it SHALL have a maximum of 600 characters\n3. WHEN the content is written THEN it SHALL explain once that this is a pilsa book\n4. WHEN current events are referenced THEN they SHALL be included without being date-specific\n5. WHEN the note is finalized THEN it SHALL focus on motivating both publisher and reader engagement\n\n### Requirement 9: Mnemonics JSON properly created\n\n**User Story:** As an editor, I want a properly created Mnemonics appendix for each book, so that the mnemonics.tex file is generated successfully without JSON parsing errors.\n\n#### Acceptance Criteria\n\n1. WHEN mnemonics are generated THEN the LLM response SHALL include the expected 'mnemonics_data' key\n2. WHEN the mnemonics prompt is executed THEN it SHALL be reformulated to require the expected JSON structure\n3. WHEN JSON parsing fails THEN the system SHALL provide clear error messages and fallback behavior\n4. WHEN mnemonics content is missing THEN the system SHALL log the issue and continue processing\n5. WHEN mnemonics.tex is created THEN it SHALL contain properly formatted mnemonic content\n\n### Requirement 10: Back Text should include mention that book is pilsa book\n\n**User Story:** As a reader and publisher, I want the book description to clearly identify this as a pilsa book, so that readers understand the unique nature and purpose of the publication.\n\n#### Acceptance Criteria\n\n1. WHEN descriptive content is generated THEN it SHALL clearly identify the book as a pilsa or transcriptive meditation handbook\n2. WHEN book descriptions are created THEN they SHALL mention the 90 quotations and 90 facing pages for journaling\n3. WHEN back cover text is generated THEN it SHALL include the pilsa book designation\n4. WHEN marketing copy is written THEN it SHALL emphasize the unique meditative and journaling aspects\n5. WHEN book metadata is created THEN it SHALL consistently reference the pilsa format\n\n### Requirement 11: BISAC Categories 2 and 3 are too vague\n\n**User Story:** As a publisher and bookseller, I want BISAC categories to accurately reflect the specific technical content of each book, so that readers can find books through appropriate subject classifications.\n\n#### Acceptance Criteria\n\n1. WHEN BISAC categories are generated THEN they SHALL reflect the specific technical content discussed in the book\n2. WHEN generic categories like \"Business>General\" are detected THEN they SHALL be replaced with more specific classifications\n3. WHEN technical content is analyzed THEN appropriate categories like \"Science > Planetary Exploration\" SHALL be selected\n4. WHEN multiple BISAC categories are assigned THEN each SHALL be relevant to the book's actual content\n5. WHEN category validation occurs THEN it SHALL ensure categories match the book's subject matter\n\n### Requirement 12: Writing Style Configuration System\n\n**User Story:** As a publisher, I want configurable writing styles at tranche, imprint, and publisher levels, so that I can maintain consistent voice and tone across publications.\n\n#### Acceptance Criteria\n\n1. WHEN writing style configuration is needed THEN it SHALL be available at tranche, imprint, and publisher levels\n2. WHEN style configuration is defined THEN it SHALL be stored in a writing_style.json file in the appropriate directory\n3. WHEN multiple text values exist in the JSON THEN they SHALL be constructed into a single prompt to append to the original prompt\n4. WHEN style configuration is applied THEN it SHALL override lower-level configurations (tranche > imprint > publisher)\n5. WHEN no style configuration exists THEN the system SHALL use default styling without errors\n\n### Requirement 13: Quote Assembly and Author Distribution\n\n**User Story:** As a content curator, I want quote assembly to avoid repetitive author citations, so that the quotations section has better variety and readability.\n\n#### Acceptance Criteria\n\n1. WHEN quotations are assembled THEN no single author SHALL be quoted more than three times in a row\n2. WHEN author repetition is detected THEN the system SHALL reorder quotes to improve distribution\n3. WHEN quote ordering is applied THEN it SHALL maintain thematic coherence while improving author variety\n4. WHEN insufficient quotes exist from different authors THEN the system SHALL work with available content while minimizing repetition\n5. WHEN quote assembly is complete THEN the final arrangement SHALL be reviewed for author distribution balance\n\n### Requirement 14: Last Verso Page\n\n**User Story:** As a book designer, I want blank last verso pages to have a \"Notes\" heading, so that the page serves a functional purpose for readers.\n\n#### Acceptance Criteria\n\n1. WHEN the last verso page is blank THEN it SHALL have a chapter heading reading \"Notes\" at the top of the page\n2. WHEN the Notes heading is added THEN it SHALL follow the same formatting as other chapter headings\n3. WHEN the last verso page has content THEN no Notes heading SHALL be added\n4. WHEN the Notes page is created THEN it SHALL provide space for reader annotations\n5. WHEN the book is compiled THEN the Notes page SHALL be properly positioned as the final verso page\n\n### Requirement 15: ISBN and Barcode Integration\n\n**User Story:** As a publisher, I want automatic UPC-A barcode generation with ISBN-13 when ISBNs are assigned, so that books are ready for retail distribution.\n\n#### Acceptance Criteria\n\n1. WHEN an ISBN-13 is assigned to a book THEN a UPC-A barcode SHALL be automatically generated\n2. WHEN the barcode is created THEN it SHALL include bar-code-reader numerals for retail scanning\n3. WHEN the barcode is integrated THEN it SHALL be properly positioned on the back cover\n4. WHEN barcode generation occurs THEN it SHALL meet industry standards for retail distribution\n5. WHEN ISBN assignment is complete THEN the book SHALL be ready for retail with proper barcode integration\n\n### Requirement 16: Storefront Catalog metadata\n\n**User Story:** As the operator of a online book storefront, I want all metadata for each title to be complete and accurate.\n\n#### Acceptance Criteria\n\n1. WHEN storefront metadata is generated THEN the author name SHALL match the Contributor One name from the Tranche config\n2. WHEN author fields are populated THEN they SHALL not be interpolated or generated by the model\n3. WHEN storefront_author_en and _ko fields are created THEN they SHALL use the exact name from the configuration\n4. WHEN metadata is validated THEN it SHALL ensure consistency between LSI CSV and storefront data\n5. WHEN tranche configuration is missing author data THEN the system SHALL provide clear error messages\n\n\n\n## Success Criteria\n\n- Bibliography entries display with proper hanging indents using memoir citation fields\n- ISBN lookups are cached and not repeated, improving system efficiency\n- Reporting accurately reflects actual prompt success and quote retrieval statistics\n- Error handling provides clear, actionable information for debugging\n- Typography and formatting meet professional publishing standards\n- Glossaries are properly formatted in 2 columns with correct term stacking\n- Publisher's Notes are concise, engaging, and properly structured\n- Writing style configuration works across all organizational levels\n- Quote assembly avoids excessive author repetition\n- ISBN assignment automatically generates proper UPC-A barcodes\n- Storefront metadata uses accurate author names from tranche configuration\n\n## Technical Considerations\n\n- LaTeX memoir class integration for bibliography formatting\n- JSON caching system for ISBN lookup results\n- Enhanced logging and error reporting mechanisms\n- Font management for Korean characters (Apple Myungjo)\n- Column layout algorithms for glossary formatting\n- Hierarchical configuration loading system\n- Quote reordering algorithms for author distribution\n- Barcode generation libraries and positioning systems\n- Tranche configuration integration for accurate metadata sourcing",
      "size": 14766,
      "modified": "2025-08-04T01:48:57.958477",
      "spec_directory": "book-production-fixes",
      "directory_modified": "2025-08-04T02:16:41.765212"
    },
    {
      "file": "requirements.md",
      "content": "# Requirements Document\n\n## Introduction\n\nThe ISBN Schedule Assignment feature provides a comprehensive system for managing ISBN assignments across publishing schedules. This feature allows publishers to efficiently allocate ISBNs to books, track their usage, and manage ISBN blocks from registration authorities. The system supports both manual and bulk operations, provides detailed reporting, and integrates with the existing Codexes Factory publishing pipeline.\n\n## Requirements\n\n### Requirement ",
      "full_content": "# Requirements Document\n\n## Introduction\n\nThe ISBN Schedule Assignment feature provides a comprehensive system for managing ISBN assignments across publishing schedules. This feature allows publishers to efficiently allocate ISBNs to books, track their usage, and manage ISBN blocks from registration authorities. The system supports both manual and bulk operations, provides detailed reporting, and integrates with the existing Codexes Factory publishing pipeline.\n\n## Requirements\n\n### Requirement 1\n\n**User Story:** As a publisher, I want to manage ISBN blocks and ranges, so that I can efficiently allocate ISBNs from my registered blocks to upcoming book publications.\n\n#### Acceptance Criteria\n\n1. WHEN I add a new ISBN block THEN the system SHALL store the prefix, publisher code, imprint code, start number, end number, and total count\n2. WHEN I view ISBN blocks THEN the system SHALL display utilization statistics including used, reserved, and available ISBNs\n3. WHEN I add an ISBN block with invalid parameters THEN the system SHALL reject the block and provide clear error messages\n4. IF an ISBN block overlaps with existing blocks THEN the system SHALL warn about potential conflicts\n\n### Requirement 2\n\n**User Story:** As a production manager, I want to schedule ISBN assignments for upcoming books, so that I can plan publication schedules and ensure ISBNs are available when needed.\n\n#### Acceptance Criteria\n\n1. WHEN I schedule an ISBN assignment THEN the system SHALL automatically assign the next available ISBN from appropriate blocks\n2. WHEN I schedule an assignment THEN the system SHALL store book title, book ID, scheduled date, imprint, publisher, format, priority, and notes\n3. WHEN no ISBNs are available THEN the system SHALL reject the assignment and notify me of the shortage\n4. WHEN I schedule an assignment THEN the system SHALL update block utilization counts automatically\n5. IF I specify an imprint or publisher THEN the system SHALL prioritize matching ISBN blocks\n\n### Requirement 3\n\n**User Story:** As a production coordinator, I want to view and manage scheduled ISBN assignments, so that I can track upcoming publications and make adjustments as needed.\n\n#### Acceptance Criteria\n\n1. WHEN I view assignments THEN the system SHALL display ISBN, title, book ID, scheduled date, status, and other metadata\n2. WHEN I filter assignments by status THEN the system SHALL show only assignments matching the selected status\n3. WHEN I filter assignments by date range THEN the system SHALL show only assignments within the specified dates\n4. WHEN I search assignments THEN the system SHALL find matches in title, ISBN, or book ID fields\n5. WHEN I select an assignment THEN the system SHALL allow me to assign, reserve, edit, or view details\n\n### Requirement 4\n\n**User Story:** As a production manager, I want to assign ISBNs immediately when books are ready for publication, so that I can move books from scheduled to active status.\n\n#### Acceptance Criteria\n\n1. WHEN I assign an ISBN THEN the system SHALL change status from scheduled to assigned\n2. WHEN I assign an ISBN THEN the system SHALL record the assignment date and time\n3. WHEN I try to assign a non-existent ISBN THEN the system SHALL reject the operation with an error message\n4. WHEN I assign an ISBN THEN the system SHALL update all related reports and statistics\n\n### Requirement 5\n\n**User Story:** As a publisher, I want to reserve ISBNs for special projects, so that I can prevent them from being automatically assigned to regular publications.\n\n#### Acceptance Criteria\n\n1. WHEN I reserve an ISBN THEN the system SHALL change its status to reserved\n2. WHEN I reserve an ISBN THEN the system SHALL require and store a reservation reason\n3. WHEN I reserve an ISBN THEN the system SHALL exclude it from automatic assignment pools\n4. WHEN I view reserved ISBNs THEN the system SHALL display the reservation reason and date\n\n### Requirement 6\n\n**User Story:** As a production manager, I want to update existing ISBN assignments, so that I can correct information or adjust schedules as publication plans change.\n\n#### Acceptance Criteria\n\n1. WHEN I update an assignment THEN the system SHALL allow changes to title, book ID, scheduled date, priority, and notes\n2. WHEN I update an assignment THEN the system SHALL validate all changes before saving\n3. WHEN I try to update a non-existent assignment THEN the system SHALL reject the operation with an error message\n4. WHEN I update an assignment THEN the system SHALL maintain an audit trail of changes\n\n### Requirement 7\n\n**User Story:** As a publisher, I want to generate comprehensive reports on ISBN usage and availability, so that I can make informed decisions about ordering new ISBN blocks.\n\n#### Acceptance Criteria\n\n1. WHEN I generate an availability report THEN the system SHALL show total, used, reserved, and available ISBNs\n2. WHEN I generate a report THEN the system SHALL display assignments grouped by status\n3. WHEN I generate a report THEN the system SHALL show block utilization percentages\n4. WHEN I generate a report THEN the system SHALL provide export options in JSON and CSV formats\n5. WHEN I view upcoming assignments THEN the system SHALL show books scheduled within a specified timeframe\n\n### Requirement 8\n\n**User Story:** As a production coordinator, I want to perform bulk operations on ISBN assignments, so that I can efficiently manage large publication schedules.\n\n#### Acceptance Criteria\n\n1. WHEN I upload a CSV file THEN the system SHALL validate the format and required columns\n2. WHEN I process bulk assignments THEN the system SHALL schedule ISBNs for all valid entries\n3. WHEN bulk processing encounters errors THEN the system SHALL report which entries failed and why\n4. WHEN I perform bulk status updates THEN the system SHALL allow assigning all ISBNs scheduled for a specific date\n5. WHEN I perform bulk operations THEN the system SHALL provide progress feedback and final summary\n\n### Requirement 9\n\n**User Story:** As a system administrator, I want the ISBN schedule to persist across sessions, so that all assignment data is safely stored and recoverable.\n\n#### Acceptance Criteria\n\n1. WHEN I make changes to assignments or blocks THEN the system SHALL automatically save to persistent storage\n2. WHEN the system restarts THEN the system SHALL load all existing assignments and blocks\n3. WHEN save operations fail THEN the system SHALL notify users and attempt recovery\n4. WHEN loading data THEN the system SHALL validate file integrity and handle corruption gracefully\n\n### Requirement 10\n\n**User Story:** As a developer, I want a command-line interface for ISBN management, so that I can integrate ISBN operations with automated publishing workflows.\n\n#### Acceptance Criteria\n\n1. WHEN I use the CLI THEN the system SHALL support all major operations including add-block, schedule, assign, and report\n2. WHEN I use CLI commands THEN the system SHALL provide clear help documentation and examples\n3. WHEN CLI operations fail THEN the system SHALL return appropriate exit codes and error messages\n4. WHEN I use the CLI THEN the system SHALL support different output formats including table, JSON, and CSV\n\n### Requirement 11\n\n**User Story:** As a quality assurance manager, I want comprehensive validation and error handling, so that the system maintains data integrity and provides clear feedback on issues.\n\n#### Acceptance Criteria\n\n1. WHEN invalid data is entered THEN the system SHALL reject it with specific error messages\n2. WHEN ISBN formatting is required THEN the system SHALL generate valid ISBN-13 numbers with correct check digits\n3. WHEN system errors occur THEN the system SHALL log detailed information for debugging\n4. WHEN data conflicts arise THEN the system SHALL prevent corruption and guide users to resolution",
      "size": 7822,
      "modified": "2025-08-05T01:26:52.237426",
      "spec_directory": "isbn-schedule-assignment",
      "directory_modified": "2025-08-05T02:05:32.467808"
    },
    {
      "file": "requirements.md",
      "content": "# Pipeline Configuration Fixes - Requirements Document\n\n## Introduction\n\nThis specification addresses critical configuration issues discovered during the production of the GLOBAL RENEWABLES book. The fixes ensure proper default values, LLM configuration respect, font configuration consistency, and proper generation of all backmatter components including foreword, publisher's note, and glossary.\n\n## Requirements\n\n### Requirement 1: Required Field Defaults\n\n**User Story:** As a publisher, I want r",
      "full_content": "# Pipeline Configuration Fixes - Requirements Document\n\n## Introduction\n\nThis specification addresses critical configuration issues discovered during the production of the GLOBAL RENEWABLES book. The fixes ensure proper default values, LLM configuration respect, font configuration consistency, and proper generation of all backmatter components including foreword, publisher's note, and glossary.\n\n## Requirements\n\n### Requirement 1: Required Field Defaults\n\n**User Story:** As a publisher, I want required fields to have sensible defaults, so that I don't have to manually enter common values every time.\n\n#### Acceptance Criteria\n\n1. WHEN using simple or advanced display modes THEN lightning_source_account SHALL default to \"6024045\"\n2. WHEN configuring language settings THEN language_code SHALL default to \"eng\"\n3. WHEN setting up field reports THEN field_reports SHALL default to \"HTML\"\n4. WHEN these defaults are set THEN they SHALL be visible in the UI and used in processing\n\n### Requirement 2: LLM Configuration Consistency\n\n**User Story:** As a publisher, I want all LLM calls to respect the pipeline configuration, so that I have consistent model usage and cost control.\n\n#### Acceptance Criteria\n\n1. WHEN I configure LLM settings in the pipeline THEN all stage 3 processing SHALL use those settings\n2. WHEN BackmatterProcessor is instantiated THEN it SHALL receive the pipeline LLM configuration\n3. WHEN foreword generation occurs THEN it SHALL use the pipeline LLM configuration (not hardcoded gemini-2.5-pro)\n4. WHEN publisher's note generation occurs THEN it SHALL use the pipeline LLM configuration\n5. WHEN no LLM configuration is provided THEN it SHALL use sensible defaults (not hardcoded gemini models)\n6. WHEN LLM calls are made THEN they SHALL log the model being used for transparency\n\n### Requirement 3: Font Configuration Consistency\n\n**User Story:** As a publisher, I want font configurations to be consistently applied, so that the compiled PDF matches my configuration settings.\n\n#### Acceptance Criteria\n\n1. WHEN Korean font is configured in tranche settings THEN the LaTeX template SHALL use that font\n2. WHEN template processing occurs THEN font variables SHALL be substituted from configuration\n3. WHEN fonts are not specified THEN sensible defaults SHALL be used\n4. WHEN font substitution fails THEN clear error messages SHALL be provided\n\n### Requirement 4: Backmatter Generation Completeness\n\n**User Story:** As a publisher, I want all backmatter components to be generated properly, so that the final interior contains complete foreword, publisher's note, and glossary sections.\n\n#### Acceptance Criteria\n\n1. WHEN book processing occurs THEN a foreword SHALL be generated using LLM about pilsa tradition\n2. WHEN book processing occurs THEN a publisher's note SHALL be generated with 3 structured paragraphs\n3. WHEN book processing occurs THEN a glossary SHALL be generated in 2-column format\n4. WHEN glossary is generated THEN Korean terms SHALL appear at top of left-hand cells with English equivalents below\n5. WHEN glossary is generated THEN it SHALL fit within page margins using proper column layout\n6. WHEN any backmatter generation fails THEN clear error messages SHALL be provided\n\n### Requirement 5: LaTeX Formatting and Escaping\n\n**User Story:** As a publisher, I want clean LaTeX output without broken commands, so that the compiled PDF displays properly formatted text.\n\n#### Acceptance Criteria\n\n1. WHEN LaTeX content is generated THEN broken LaTeX commands like \"extit{\" SHALL be properly escaped or removed\n2. WHEN foreword is generated THEN it SHALL contain no stray LaTeX commands in the final output\n3. WHEN any text processing occurs THEN LaTeX special characters SHALL be properly escaped\n4. WHEN LaTeX compilation occurs THEN no LaTeX syntax errors SHALL be present\n\n### Requirement 6: Bibliography Formatting\n\n**User Story:** As a publisher, I want properly formatted bibliography citations, so that the compiled PDF displays professional hanging indents.\n\n#### Acceptance Criteria\n\n1. WHEN bibliography is generated THEN citations SHALL have first line flush left\n2. WHEN bibliography is generated THEN second and following lines SHALL be indented 0.15 inches\n3. WHEN bibliography appears in compiled PDF THEN hanging indent formatting SHALL be visible\n4. WHEN bibliography formatting fails THEN clear error messages SHALL be provided\n\n### Requirement 7: Configuration Validation\n\n**User Story:** As a publisher, I want configuration validation, so that I can catch issues before processing begins.\n\n#### Acceptance Criteria\n\n1. WHEN required fields are missing THEN validation SHALL fail with clear messages\n2. WHEN LLM configuration is invalid THEN validation SHALL provide helpful guidance\n3. WHEN font configuration is invalid THEN validation SHALL suggest alternatives\n4. WHEN validation passes THEN processing SHALL proceed with confidence\n\n## Technical Requirements\n\n### TR1: Default Value Management\n- Lightning Source Account: \"6024045\"\n- Language Code: \"eng\"\n- Field Reports: \"HTML\"\n- LLM Model: \"gpt-4o-mini\" (not gemini models)\n\n### TR2: Configuration Propagation\n- Pipeline configuration must flow to all processing modules\n- LLM configuration must be passed to BackmatterProcessor\n- LLM configuration must be passed to foreword generation\n- LLM configuration must be passed to publisher's note generation\n- Font configuration must be available to template processing\n\n### TR3: Template Variable System\n- Font variables must be injected into LaTeX templates\n- Configuration values must override hardcoded values\n- Template processing must be robust and provide clear errors\n\n### TR4: Backmatter Generation Requirements\n- Foreword: 300-400 words about pilsa tradition and practice\n- Publisher's Note: 3 paragraphs with pilsa book explanation\n- Glossary: 2-column layout with Korean/English term stacking\n- All components must use pipeline LLM configuration\n\n### TR5: LaTeX Processing and Formatting\n- LaTeX escaping must be robust and prevent broken commands\n- Bibliography must use hanging indent with 0.15in indentation\n- All LaTeX special characters must be properly escaped\n- LaTeX compilation must be error-free\n\n### TR6: Logging and Transparency\n- LLM model usage must be logged for all backmatter generation\n- Configuration values must be logged at startup\n- Template substitution must be logged for debugging\n- Backmatter generation success/failure must be logged\n- LaTeX processing errors must be logged with context",
      "size": 6497,
      "modified": "2025-08-05T20:00:00.068199",
      "spec_directory": "pipeline-configuration-fixes",
      "directory_modified": "2025-08-05T20:20:01.076231"
    },
    {
      "file": "requirements.md",
      "content": "# Frontmatter and Backmatter Generation - Requirements Document\n\n## Introduction\n\nThis specification addresses critical issues with frontmatter and backmatter generation, configuration hierarchy enforcement, and prevents regressions in the book production pipeline.\n\n## Requirements\n\n### Requirement 1: Configuration Hierarchy Enforcement\n\n**User Story:** As a publisher, I want configuration values to follow a strict hierarchy, so that tranche settings always override other configurations.\n\n#### A",
      "full_content": "# Frontmatter and Backmatter Generation - Requirements Document\n\n## Introduction\n\nThis specification addresses critical issues with frontmatter and backmatter generation, configuration hierarchy enforcement, and prevents regressions in the book production pipeline.\n\n## Requirements\n\n### Requirement 1: Configuration Hierarchy Enforcement\n\n**User Story:** As a publisher, I want configuration values to follow a strict hierarchy, so that tranche settings always override other configurations.\n\n#### Acceptance Criteria\n\n1. WHEN configuration values conflict THEN hierarchy SHALL be: default < publisher < imprint < tranche (tranche wins)\n2. WHEN schedule.json contains subtitle THEN it SHALL always trump machine-generated alternatives\n3. WHEN tranche.json contains author and imprint THEN they SHALL always trump LLM generated values\n4. WHEN ISBN is assigned THEN it SHALL appear on the copyright page\n5. WHEN logo font is specified in imprint config THEN it SHALL be used (Zapfino for xynapse_traces)\n\n### Requirement 2: Bibliography Formatting (DO NOT CHANGE)\n\n**User Story:** As a publisher, I want bibliography formatting to remain stable, so that hanging indents work correctly.\n\n#### Acceptance Criteria\n\n1. WHEN bibliography is generated THEN it SHALL use memoir class `\\begin{hangparas}{0.15in}{1}` format\n2. WHEN bibliography formatting works correctly THEN it SHALL be marked as \"do not change\"\n3. WHEN bibliography is compiled THEN first line SHALL be flush left, subsequent lines indented 0.15in\n\n### Requirement 3: Publisher's Note Generation\n\n**User Story:** As a publisher, I want publisher's note to be 100% LLM generated, so that no boilerplate text is attached.\n\n#### Acceptance Criteria\n\n1. WHEN publisher's note is generated THEN it SHALL be 100% LLM generated content\n2. WHEN publisher's note is created THEN no boilerplate paragraphs SHALL be attached\n3. WHEN publisher's note uses reprompting THEN it SHALL use `storefront_get_en_motivation` prompt\n\n### Requirement 4: Foreword Generation\n\n**User Story:** As a publisher, I want foreword generation to produce clean text, so that Korean characters and markdown are properly formatted.\n\n#### Acceptance Criteria\n\n1. WHEN foreword is generated THEN Korean characters SHALL be properly formatted (no visible markdown)\n2. WHEN foreword contains Korean terms THEN they SHALL use proper LaTeX commands\n3. WHEN foreword is generated THEN no visible markdown syntax SHALL appear (no `*pilsa*`)\n4. WHEN foreword uses reprompting THEN it SHALL follow same sequence as main model calls\n\n### Requirement 5: Glossary Formatting\n\n**User Story:** As a publisher, I want glossary to display correctly, so that it has proper formatting and no numbering issues.\n\n#### Acceptance Criteria\n\n1. WHEN glossary is generated THEN it SHALL NOT have numeral \"2\" at the beginning\n2. WHEN glossary text is displayed THEN it SHALL NOT be overprinted on itself\n3. WHEN glossary is formatted THEN it SHALL have proper leading for typographic best practices\n4. WHEN glossary uses chapter heading THEN it SHALL use `\\chapter*{Glossary}` (unnumbered)\n\n### Requirement 6: Mnemonics Generation\n\n**User Story:** As a publisher, I want mnemonics section to appear in the final document, so that all backmatter is complete.\n\n#### Acceptance Criteria\n\n1. WHEN book is processed THEN mnemonics section SHALL appear in final document\n2. WHEN mnemonics processing fails THEN fallback methods SHALL be used\n3. WHEN mnemonics are generated THEN they SHALL use proper LLM configuration\n4. WHEN mnemonics file is created THEN it SHALL be `mnemonics.tex` in build directory\n\n### Requirement 7: Frontmatter vs Backmatter Classification\n\n**User Story:** As a publisher, I want proper classification of document sections, so that frontmatter and backmatter are correctly organized.\n\n#### Acceptance Criteria\n\n1. WHEN sections are classified THEN foreword SHALL be frontmatter\n2. WHEN sections are classified THEN publisher's note SHALL be frontmatter  \n3. WHEN sections are classified THEN glossary SHALL be frontmatter\n4. WHEN sections are classified THEN mnemonics SHALL be backmatter\n5. WHEN sections are classified THEN bibliography SHALL be backmatter\n6. WHEN frontmatter sections use reprompting THEN they SHALL follow reprompt sequence\n\n## Technical Requirements\n\n### TR1: Configuration Hierarchy Implementation\n- Hierarchy: default < publisher < imprint < tranche\n- Schedule.json subtitle: always wins\n- Tranche author/imprint: always wins\n- ISBN: must appear on copyright page\n- Logo font: use imprint specification (Zapfino for xynapse_traces)\n\n### TR2: Bibliography Stability (LOCKED)\n- Format: `\\begin{hangparas}{0.15in}{1}` ... `\\end{hangparas}`\n- Status: DO NOT CHANGE\n- Indentation: 0.15in hanging indent\n\n### TR3: Content Generation Requirements\n- Publisher's note: 100% LLM, no boilerplate\n- Foreword: clean Korean formatting, no visible markdown\n- Glossary: `\\chapter*{Glossary}`, proper leading, no overprinting\n- Mnemonics: must appear, proper fallback handling\n\n### TR4: Reprompting Integration\n- Frontmatter sections: use reprompting system\n- Sequence: same as main model calls\n- Prompts: use existing prompt keys where available\n\n### TR5: Regression Prevention\n- Lock stable components (bibliography)\n- Validate configuration hierarchy\n- Test all sections appear in final document\n- Prevent markdown/formatting leakage\n\n## Anti-Regression Measures\n\n### Configuration Validation\n- Test hierarchy enforcement at each level\n- Validate ISBN appears on copyright page\n- Verify logo font matches imprint specification\n\n### Content Validation  \n- Check all sections appear in final document\n- Validate formatting quality (no overprinting, proper leading)\n- Ensure no boilerplate text in LLM-generated content\n\n### Stability Locks\n- Bibliography formatting: LOCKED (memoir class hangparas)\n- Configuration hierarchy: ENFORCED\n- Section classification: frontmatter vs backmatter\n\n## Success Criteria\n\n1.  All sections appear in final document\n2.  Configuration hierarchy strictly enforced\n3.  No visible markdown or formatting errors\n4.  Bibliography hanging indents work correctly (LOCKED)\n5.  No regressions in working components\n6.  Clean, professional typographic output",
      "size": 6217,
      "modified": "2025-08-05T23:08:14.318468",
      "spec_directory": "frontmatter-backmatter-fixes",
      "directory_modified": "2025-08-06T00:48:02.840139"
    }
  ]
}